{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ],
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "  \"conf\": {\n",
        "    \"spark.notebook.parameters\": \"{}\"\n",
        "  },\n",
        "  \"defaultLakehouse\": {\n",
        "    \"name\": \"MetricsLakehouse\"\n",
        "  }\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ“Š 5. Visualize Metrics (data export only)\n",
        "\n",
        "This notebook gathers metrics from parameter_set workspaces into the central\n",
        "`MetricsLakehouse.metrics` table and exports a cleaned metrics file for\n",
        "downstream visualization/analysis. Charts and plotting have been removed so\n",
        "the notebook focuses on producing reliable, reproducible data artifacts.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "merge_metrics_from_workspaces"
      },
      "source": [
        "# Merge metrics by reading each workspace's BenchmarkLakehouse Files/metrics ABFSS directory\n",
        "# Uses mssparkutils.fs.ls to probe ABFSS paths and reads delta tables safely.\n",
        "\n",
        "import json\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "abfss_account = \"onelake.dfs.fabric.microsoft.com\"\n",
        "source_lakehouse_name = \"BenchmarkLakehouse\"\n",
        "controller_table = \"MetricsLakehouse.metrics\"\n",
        "\n",
        "# canonical metrics schema (used only to create empty table if missing)\n",
        "metrics_schema = StructType([\n",
        "    StructField(\"test_case_id\", StringType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"source\", StringType(), True),\n",
        "    StructField(\"format\", StringType(), True),\n",
        "    StructField(\"rows\", IntegerType(), True),\n",
        "    StructField(\"update_strategy\", StringType(), True),\n",
        "    StructField(\"action\", StringType(), True),\n",
        "    StructField(\"ingest_time_s\", FloatType(), True),\n",
        "    StructField(\"spinup_time_s\", FloatType(), True),\n",
        "    StructField(\"query_type\", StringType(), True),\n",
        "    StructField(\"query_time_s\", FloatType(), True),\n",
        "    StructField(\"notes\", StringType(), True)\n",
        "])\n",
        "\n",
        "# parse runs (safe default)\n",
        "conf_key = \"spark.notebook.parameters\"\n",
        "runs = json.loads(spark.conf.get(conf_key, \"{}\")).get(\"runs\", [])\n",
        "workspace_names = [r.get(\"name\") for r in runs if r.get(\"name\")]\n",
        "print(\"Workspaces to inspect (from runs):\", workspace_names)\n",
        "\n",
        "# ensure controller table exists\n",
        "if not spark.catalog.tableExists(controller_table):\n",
        "    empty = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema=metrics_schema)\n",
        "    empty.write.mode(\"overwrite\").saveAsTable(controller_table)\n",
        "\n",
        "dest_df = spark.table(controller_table)\n",
        "before_total = dest_df.count()\n",
        "print(f\"Controller metrics table found: {controller_table} rows={before_total}\")\n",
        "\n",
        "# prefer mssparkutils for path probing in Fabric notebooks\n",
        "try:\n",
        "    from notebookutils import mssparkutils\n",
        "except Exception:\n",
        "    try:\n",
        "        import mssparkutils\n",
        "    except Exception:\n",
        "        mssparkutils = None\n",
        "\n",
        "total_inserted = 0\n",
        "\n",
        "for ws in workspace_names:\n",
        "    if not ws:\n",
        "        continue\n",
        "\n",
        "    container = str(ws)\n",
        "    source_table_root = f\"abfss://{container}@{abfss_account}/{source_lakehouse_name}.lakehouse/Tables/metrics\"\n",
        "    print(f\"\\nWorkspace '{ws}': probing {source_table_root}\")\n",
        "\n",
        "    path_exists = False\n",
        "    if mssparkutils:\n",
        "        try:\n",
        "            ls_entries = mssparkutils.fs.ls(source_table_root)\n",
        "            path_exists = bool(ls_entries)\n",
        "        except Exception:\n",
        "            path_exists = False\n",
        "    else:\n",
        "        # Fallback: try a guarded read\n",
        "        try:\n",
        "            tmp = spark.read.format(\"delta\").load(source_table_root)\n",
        "            path_exists = True\n",
        "        except Exception:\n",
        "            path_exists = False\n",
        "\n",
        "    print(f\"  path_exists={path_exists}\")\n",
        "\n",
        "    if not path_exists:\n",
        "        print(f\"  Path not present or inaccessible for workspace '{ws}'; skipping.\")\n",
        "        continue\n",
        "\n",
        "    # read the delta table (delta reader handles _delta_log correctly)\n",
        "    src_df = spark.read.format(\"delta\").load(source_table_root)\n",
        "    src_count = src_df.count()\n",
        "    print(f\"  Read {src_count} rows from delta table at {source_table_root}\")\n",
        "\n",
        "    # Deduplicate exact rows against destination by using common columns\n",
        "    dest_cols = dest_df.columns\n",
        "    common_cols = [c for c in dest_cols if c in src_df.columns]\n",
        "    if not common_cols:\n",
        "        print(f\"  No common columns between source ({source_table_root}) and destination ({controller_table}); skipping\")\n",
        "        continue\n",
        "\n",
        "    src_sel = src_df.select(*common_cols)\n",
        "    dest_sel = dest_df.select(*common_cols)\n",
        "\n",
        "    # left_anti finds rows in src_sel that are not present in dest_sel\n",
        "    new_rows = src_sel.join(dest_sel, on=common_cols, how=\"left_anti\")\n",
        "    insert_count = new_rows.count()\n",
        "    print(f\"  New unique rows to insert from '{ws}': {insert_count}\")\n",
        "\n",
        "    if insert_count > 0:\n",
        "        new_rows.write.mode(\"append\").saveAsTable(controller_table)\n",
        "        dest_df = spark.table(controller_table)\n",
        "        total_inserted += insert_count\n",
        "        print(f\"  Appended {insert_count} rows from '{ws}' into {controller_table}\")\n",
        "\n",
        "print(f\"\\nFinished: before_total={before_total}, total_inserted={total_inserted}\")\n",
        "after_total = spark.table(controller_table).count()\n",
        "print(f\"After total rows in {controller_table}: {after_total}\")\n",
        "\n",
        "metrics_df = spark.table(controller_table).toPandas()\n",
        "print(\"Loaded merged metrics into pandas DataFrame with rows:\", len(metrics_df))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Show all metrics as interactive table\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "# Expose the two proxy columns for clarity in the table view\n",
        "display(metrics_df.assign(storage_rows_proxy=metrics_df['storage_rows_proxy'], cu_used_int=metrics_df['cu_used_int']))\n",
        "print('Displayed metrics table')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Completion\")\n",
        "print(\"Metrics merge and export complete. Charts removed; outputs are in .state/\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}

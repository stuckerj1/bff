{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# ðŸ““ 4. Run Queries\n",
    "## Query Performance Benchmarking for TC.11.xâ€“TC.13.x\n",
    "\n",
    "### ðŸ”— Ensure `BenchmarkLakehouse` is connected as a data source before running.\n",
    "\n",
    "Assumes synthetic data, initial load, and updates have been completed.\n",
    "All queries are run against the target table in BenchmarkLakehouse and BenchmarkWarehouse.\n",
    "Join queries are excluded (single table per location).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "setup",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "id": "params",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Paths and table names\n",
    "target_lakehouse = \"BenchmarkLakehouse\"\n",
    "target_warehouse = \"BenchmarkWarehouse\"\n",
    "row_count = 10000  # Update as appropriate\n",
    "\n",
    "delta_tables = {\n",
    "    \"refresh\": \"delta_refresh_load\",\n",
    "    \"compare\": \"delta_compare_load\",\n",
    "    \"increment\": \"delta_increment_load\"\n",
    "}\n",
    "warehouse_tables = {\n",
    "    \"refresh\": \"wh_table_refresh_load\",\n",
    "    \"compare\": \"wh_table_compare_load\",\n",
    "    \"increment\": \"wh_table_increment_load\"\n",
    "}\n",
    "\n",
    "# Choose table to query (usually 'increment' for event log)\n",
    "lakehouse_table = f\"{target_lakehouse}.{delta_tables['increment']}\"\n",
    "warehouse_table = f\"{target_warehouse}.dbo.{warehouse_tables['increment']}\""
   ]
  },
  {
   "cell_type": "code",
   "id": "metrics_schema",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
    "from datetime import datetime\n",
    "\n",
    "# NOTE: storage_size_mb and cu_used are INTEGER row-count proxies in this pipeline.\n",
    "metrics_schema = StructType([\n",
    "    StructField(\"test_case_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"format\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"rows\", IntegerType(), True),\n",
    "    StructField(\"update_strategy\", StringType(), True),\n",
    "    StructField(\"ingest_time_s\", FloatType(), True),\n",
    "    StructField(\"spinup_time_s\", FloatType(), True),\n",
    "    StructField(\"storage_size_mb\", IntegerType(), True),  # row-count proxy\n",
    "    StructField(\"query_type\", StringType(), True),\n",
    "    StructField(\"query_time_s\", FloatType(), True),\n",
    "    StructField(\"cu_used\", IntegerType(), True),          # rows-processed proxy\n",
    "    StructField(\"notes\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "id": "read_lakehouse",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read Delta table from Lakehouse\n",
    "df_lakehouse = spark.read.table(lakehouse_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query_intro",
   "metadata": {},
   "source": [
    "## Query Types\n",
    "- **Filter Query**: Select rows with a specific category value.\n",
    "- **Aggregate Query**: Group by category, aggregate numeric columns.\n",
    "- **Batch Query**: Select update events in a specific time window.\n",
    "- **Top-N Query**: Retrieve top N rows by a numeric column.\n",
    "\n",
    "*(Join queries are excludedâ€”only one table in each target location.)*"
   ]
  },
  {
   "cell_type": "code",
   "id": "query_funcs",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define query functions\n",
    "def run_filter_query(df):\n",
    "    # Example: Filter by category\n",
    "    return df.filter(col(\"cat_1\") == \"A\").count()\n",
    "\n",
    "def run_aggregate_query(df):\n",
    "    # Example: Group by category, aggregate numerics\n",
    "    return df.groupBy(\"cat_1\").agg({\"num_1\": \"avg\", \"num_2\": \"max\"}).count()\n",
    "\n",
    "def run_batch_query(df):\n",
    "    # Example: Select events by update_type and time window\n",
    "    return df.filter((col(\"update_type\") == \"update\") & (col(\"ts_1\") > \"2025-01-01\")).count()\n",
    "\n",
    "def run_topn_query(df, n=10):\n",
    "    # Example: Top-N by numeric value\n",
    "    return df.orderBy(col(\"num_1\").desc()).limit(n).count()"
   ]
  },
  {
   "cell_type": "code",
   "id": "logging_util",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Performance logging utility\n",
    "def log_query_perf(query_func, df, description):\n",
    "    start = time.time()\n",
    "    result = query_func(df)\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"{description}: {elapsed:.3f}s (Rows: {result})\")\n",
    "    return {\"query\": description, \"rows\": result, \"time_s\": elapsed}"
   ]
  },
  {
   "cell_type": "code",
   "id": "lakehouse_queries",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run queries on Lakehouse Delta table\n",
    "lakehouse_metrics = []\n",
    "lakehouse_metrics.append(log_query_perf(run_filter_query, df_lakehouse, \"Lakehouse Filter cat_1 == 'A'\"))\n",
    "lakehouse_metrics.append(log_query_perf(run_aggregate_query, df_lakehouse, \"Lakehouse Aggregate by cat_1\"))\n",
    "lakehouse_metrics.append(log_query_perf(run_batch_query, df_lakehouse, \"Lakehouse Batch update_type == update, ts_1 > '2025-01-01'\"))\n",
    "lakehouse_metrics.append(log_query_perf(run_topn_query, df_lakehouse, \"Lakehouse Top 10 num_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "id": "read_warehouse",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read Warehouse table (for Spark SQL endpoint, not as DataFrame)\n",
    "from com.microsoft.spark.fabric import Constants\n",
    "df_warehouse = spark.read.synapsesql(warehouse_table)"
   ]
  },
  {
   "cell_type": "code",
   "id": "warehouse_queries",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run queries on Warehouse table\n",
    "warehouse_metrics = []\n",
    "warehouse_metrics.append(log_query_perf(run_filter_query, df_warehouse, \"Warehouse Filter cat_1 == 'A'\"))\n",
    "warehouse_metrics.append(log_query_perf(run_aggregate_query, df_warehouse, \"Warehouse Aggregate by cat_1\"))\n",
    "warehouse_metrics.append(log_query_perf(run_batch_query, df_warehouse, \"Warehouse Batch update_type == update, ts_1 > '2025-01-01'\"))\n",
    "warehouse_metrics.append(log_query_perf(run_topn_query, df_warehouse, \"Warehouse Top 10 num_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "id": "visualize",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display metrics as table\n",
    "import pandas as pd\n",
    "\n",
    "all_metrics = lakehouse_metrics + warehouse_metrics\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "display(metrics_df)\n",
    "\n",
    "# Print completion message\n",
    "print(\"Query performance benchmarking complete. Metrics above can be visualized in the next step.\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "log_to_metrics_table",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Log metrics to metrics table in BenchmarkLakehouse\n",
    "def log_query_to_metrics(test_case_id, format, location, rows, query_type, query_time_s, notes=\"\"):\n",
    "    # Use None for fields that are not applicable. storage_size_mb and cu_used are INTEGER row-count proxies elsewhere\n",
    "    metrics_row = [(\n",
    "        test_case_id,\n",
    "        datetime.now(),\n",
    "        format,\n",
    "        location,\n",
    "        rows,\n",
    "        \"\",                  # update_strategy (N/A for queries)\n",
    "        None,                 # ingest_time_s (N/A for queries)\n",
    "        None,                 # spinup_time_s (N/A)\n",
    "        None,                 # storage_size_mb (N/A for queries)\n",
    "        query_type,\n",
    "        float(query_time_s),\n",
    "        None,                 # cu_used (N/A for queries)\n",
    "        notes\n",
    "    )]\n",
    "    spark.createDataFrame(metrics_row, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')"
   ]
  },
  {
   "cell_type": "code",
   "id": "log_all_metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Log all Lakehouse query metrics\n",
    "for metric in lakehouse_metrics:\n",
    "    log_query_to_metrics(\n",
    "        test_case_id=\"TC.11.x\",\n",
    "        format=\"Delta\",\n",
    "        location=\"Tables\",\n",
    "        rows=metric['rows'],\n",
    "        query_type=metric['query'],\n",
    "        query_time_s=metric['time_s'],\n",
    "        notes=\"Lakehouse query performance\"\n",
    "    )\n",
    "\n",
    "# Log all Warehouse query metrics\n",
    "for metric in warehouse_metrics:\n",
    "    log_query_to_metrics(\n",
    "        test_case_id=\"TC.12.x\",\n",
    "        format=\"Warehouse\",\n",
    "        location=\"Tables\",\n",
    "        rows=metric['rows'],\n",
    "        query_type=metric['query'],\n",
    "        query_time_s=metric['time_s'],\n",
    "        notes=\"Warehouse query performance\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz_intro",
   "metadata": {},
   "source": [
    "# ðŸ“Š Visualize Metrics\n",
    "### ðŸ”— Ensure `BenchmarkLakehouse` is connected as a data source before running.\n",
    "\n",
    "This section appends the new visualizations to the notebook. It reads the metrics table and creates a 5Ã—6 matrix of small charts comparing formats and strategies, plus the standalone visuals for[...]\n",
    "Notes:\n",
    "- The notebook expects storage_size_mb and cu_used to be integer row-count proxies (per prior changes).\n",
    "- The visualizations below prefer recorded metrics from the lakehouse metrics table and fall back to a local metrics.csv for development.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "viz_load_metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load metrics table from Lakehouse (or local CSV for dev) and normalize proxy fields\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "try:\n",
    "    metrics_df = spark.read.table(\"BenchmarkLakehouse.metrics\").toPandas()\n",
    "except Exception:\n",
    "    metrics_df = pd.read_csv(\"metrics.csv\")\n",
    "\n",
    "# Normalize numeric proxy fields (storage_size_mb and cu_used may be stored as floats in the metrics table)\n",
    "metrics_df['storage_size_mb'] = pd.to_numeric(metrics_df.get('storage_size_mb', pd.Series(np.nan)), errors='coerce')\n",
    "metrics_df['cu_used'] = pd.to_numeric(metrics_df.get('cu_used', pd.Series(np.nan)), errors='coerce')\n",
    "\n",
    "# Create integer proxy columns for display and plotting (None for missing)\n",
    "def to_int_proxy(x):\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return int(float(x))\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "metrics_df['storage_rows_proxy'] = metrics_df['storage_size_mb'].apply(to_int_proxy)\n",
    "metrics_df['cu_used_int'] = metrics_df['cu_used'].apply(to_int_proxy)\n",
    "\n",
    "metrics_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "viz_storage_calc",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate storage_size_mb if missing (best effort)\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_storage_for_table(table_path):\n",
    "    try:\n",
    "        import mssparkutils\n",
    "        files = mssparkutils.fs.ls(table_path)\n",
    "        size_mb = sum(f.size for f in files) / (1024 * 1024)\n",
    "        return size_mb\n",
    "    except Exception:\n",
    "        return float('nan')\n",
    "\n",
    "# Identify unique targets and fill missing storage_size_mb when possible\n",
    "for idx, row in metrics_df.iterrows():\n",
    "    if ('storage_size_mb' in row and (pd.isna(row['storage_size_mb']) or math.isnan(row['storage_size_mb']))) and row['update_strategy']:\n",
    "        # crude extraction: use test_case_id to guess table name (customize if needed)\n",
    "        tc = row['test_case_id']\n",
    "        table_map = {\n",
    "            'TC.01.x': '/lakehouse/BenchmarkLakehouse/Tables/delta_refresh_load',\n",
    "            'TC.02.x': '/lakehouse/BenchmarkLakehouse/Tables/wh_table_refresh_load',\n",
    "            'TC.03.x': '/lakehouse/BenchmarkLakehouse/Tables/delta_refresh_load',\n",
    "            'TC.04.x': '/lakehouse/BenchmarkLakehouse/Tables/wh_table_refresh_load',\n",
    "            'TC.05.x': '/lakehouse/BenchmarkLakehouse/Tables/delta_compare_load',\n",
    "            'TC.06.x': '/lakehouse/BenchmarkLakehouse/Tables/wh_table_compare_load',\n",
    "            'TC.07.x': '/lakehouse/BenchmarkLakehouse/Tables/delta_increment_load',\n",
    "            'TC.08.x': '/lakehouse/BenchmarkLakehouse/Tables/wh_table_increment_load',\n",
    "        }\n",
    "        table_path = table_map.get(tc, None)\n",
    "        if table_path:\n",
    "            metrics_df.at[idx, 'storage_size_mb'] = calculate_storage_for_table(table_path)\n",
    "            # refresh proxy column when we populate storage_size_mb\n",
    "            try:\n",
    "                metrics_df.at[idx, 'storage_rows_proxy'] = int(metrics_df.at[idx, 'storage_size_mb'])\n",
    "            except Exception:\n",
    "                metrics_df.at[idx, 'storage_rows_proxy'] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "viz_ingest_vis",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initial ingestion performance\n",
    "import matplotlib.pyplot as plt\n",
    "ingest_df = metrics_df[metrics_df['update_strategy'] == 'Full Refresh']\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(ingest_df['format'], ingest_df['ingest_time_s'], color=['skyblue', 'orange'])\n",
    "plt.title(\"Initial Ingestion Time by Format\")\n",
    "plt.xlabel(\"Format\")\n",
    "plt.ylabel(\"Ingestion Time (s)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "viz_update_vis",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Update performance comparison\n",
    "update_df = metrics_df[metrics_df['update_strategy'].isin(['Full Compare', 'Incremental'])]\n",
    "plt.figure(figsize=(8,4))\n",
    "for strategy in update_df['update_strategy'].unique():\n",
    "    strat_df = update_df[update_df['update_strategy'] == strategy]\n",
    "    plt.bar(strat_df['format'] + \" \" + strat_df['update_strategy'], strat_df['ingest_time_s'], label=strategy)\n",
    "plt.title(\"Update Time by Strategy and Format\")\n",
    "plt.xlabel(\"Strategy\")\n",
    "plt.ylabel(\"Update Time (s)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "viz_query_vis",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Query performance comparison\n",
    "query_df = metrics_df[metrics_df['query_type'].notna()]\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(query_df['query_type'], query_df['query_time_s'], color='seagreen')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Query Performance Comparison\")\n",
    "plt.xlabel(\"Query Type\")\n",
    "plt.ylabel(\"Query Time (s)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "viz_storage_vis",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Storage cost comparison (use integer proxy column created above)\n",
    "storage_df = metrics_df.dropna(subset=['storage_rows_proxy'])\n",
    "storage_summary = storage_df.groupby(['format', 'update_strategy'])['storage_rows_proxy'].mean().reset_index()\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(storage_summary['format'] + \" \" + storage_summary['update_strategy'], storage_summary['storage_rows_proxy'], color='orchid')\n",
    "plt.title(\"Storage Size by Target Table\")\n",
    "plt.xlabel(\"Target Table\")\n",
    "plt.ylabel(\"Storage Size (rows) â€” row-count proxy\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "viz_summary_table",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show all metrics as interactive table\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "display(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "viz_complete",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Visualization complete. Review charts above for performance and storage comparisons across ingestion, update, and query activities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matrix_intro",
   "metadata": {},
   "source": [
    "# Visualize Metrics Matrix (Compact 5Ã—6)\n",
    "\n",
    "This compact matrix provides the small-multiples view we discussed: six columns (strategies) Ã— five rows (metrics). It uses the same metrics table and the integer row-count proxies for storage_size_mb and cu_used."
   ]
  },
  {
   "cell_type": "code",
   "id": "matrix_plot",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build and display the 5x6 metrics matrix\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "\n",
    "try:\n",
    "    metrics_df = spark.read.table(\"BenchmarkLakehouse.metrics\").toPandas()\n",
    "except Exception:\n",
    "    metrics_df = pd.read_csv(\"metrics.csv\")\n",
    "\n",
    "if 'timestamp' in metrics_df.columns:\n",
    "    try:\n",
    "        metrics_df['timestamp'] = pd.to_datetime(metrics_df['timestamp'])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "cols = [\n",
    "    (\"Warehouse\", \"Full Refresh\", \"TC.04.x\"),\n",
    "    (\"Warehouse\", \"Full Compare\", \"TC.06.x\"),\n",
    "    (\"Warehouse\", \"Incremental\", \"TC.08.x\"),\n",
    "    (\"Delta\",     \"Full Refresh\", \"TC.03.x\"),\n",
    "    (\"Delta\",     \"Full Compare\", \"TC.05.x\"),\n",
    "    (\"Delta\",     \"Incremental\", \"TC.07.x\"),\n",
    "]\n",
    "ingest_tc_by_format = {\"Delta\": \"TC.01.x\", \"Warehouse\": \"TC.02.x\"}\n",
    "\n",
    "def latest_row_for_testcase(df, tc):\n",
    "    if tc not in df['test_case_id'].values:\n",
    "        return None\n",
    "    sub = df[df['test_case_id'] == tc]\n",
    "    if 'timestamp' in sub.columns:\n",
    "        sub = sub.sort_values('timestamp')\n",
    "    return sub.iloc[-1]\n",
    "\n",
    "def sget_int(s, colname, default=0):\n",
    "    if s is None or colname not in s.index:\n",
    "        return int(default)\n",
    "    val = s[colname]\n",
    "    if pd.isna(val):\n",
    "        return int(default)\n",
    "    try:\n",
    "        return int(val)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return int(float(val))\n",
    "        except Exception:\n",
    "            return int(default)\n",
    "\n",
    "def sget_float(s, colname, default=0.0):\n",
    "    if s is None or colname not in s.index:\n",
    "        return float(default)\n",
    "    val = s[colname]\n",
    "    if pd.isna(val):\n",
    "        return float(default)\n",
    "    try:\n",
    "        return float(val)\n",
    "    except Exception:\n",
    "        return float(default)\n",
    "\n",
    "ingest_rows = []\n",
    "ingest_time_s = []\n",
    "update_rows = []\n",
    "update_time_s = []\n",
    "storage_rows = []\n",
    "col_labels = []\n",
    "\n",
    "for fmt, strat, update_tc in cols:\n",
    "    col_labels.append(f\"{fmt}\\n{strat}\")\n",
    "    ingest_tc = ingest_tc_by_format.get(fmt)\n",
    "    ingest_row = latest_row_for_testcase(metrics_df, ingest_tc)\n",
    "    ingest_rows.append(sget_int(ingest_row, 'rows', default=0))\n",
    "    ingest_time_s.append(sget_float(ingest_row, 'ingest_time_s', default=0.0))\n",
    "\n",
    "    update_row = latest_row_for_testcase(metrics_df, update_tc)\n",
    "    ur = 0\n",
    "    if update_row is not None:\n",
    "        if 'cu_used' in update_row.index and (not pd.isna(update_row['cu_used'])):\n",
    "            try:\n",
    "                ur = int(update_row['cu_used'])\n",
    "            except Exception:\n",
    "                ur = sget_int(update_row, 'rows', default=0)\n",
    "        else:\n",
    "            ur = sget_int(update_row, 'rows', default=0)\n",
    "    update_rows.append(int(ur))\n",
    "    update_time_s.append(sget_float(update_row, 'ingest_time_s', default=0.0))\n",
    "\n",
    "    sr = None\n",
    "    if update_row is not None and 'storage_size_mb' in update_row.index and (not pd.isna(update_row['storage_size_mb'])):\n",
    "        try:\n",
    "            sr = int(update_row['storage_size_mb'])\n",
    "        except Exception:\n",
    "            sr = None\n",
    "    if sr is None and ingest_row is not None and 'storage_size_mb' in ingest_row.index and (not pd.isna(ingest_row['storage_size_mb'])):\n",
    "        try:\n",
    "            sr = int(ingest_row['storage_size_mb'])\n",
    "        except Exception:\n",
    "            sr = None\n",
    "    if sr is None:\n",
    "        sr = 0\n",
    "    storage_rows.append(int(sr))\n",
    "\n",
    "matrix_df = pd.DataFrame(\n",
    "    data=[ingest_rows, ingest_time_s, update_rows, update_time_s, storage_rows],\n",
    "    index=[\"Ingestion rows\", \"Ingestion time_s\", \"Update rows\", \"Update time_s\", \"Storage rows\"],\n",
    "    columns=col_labels\n",
    ")\n",
    "\n",
    "print(\"Metrics matrix (values):\")\n",
    "display(matrix_df)\n",
    "\n",
    "n_rows, n_cols = matrix_df.shape\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(3 * n_cols, 2.2 * n_rows), squeeze=False)\n",
    "plt.subplots_adjust(hspace=0.6, wspace=0.6, top=0.93)\n",
    "fig.suptitle(\"Metrics matrix (rows = metrics, cols = strategies)\", fontsize=16)\n",
    "\n",
    "colors = [\"#2a9d8f\"] * n_cols\n",
    "\n",
    "for r_i, metric in enumerate(matrix_df.index):\n",
    "    vals = matrix_df.loc[metric].astype(float).values\n",
    "    vmin = 0.0\n",
    "    vmax = float(np.nanmax(vals)) if len(vals) > 0 else 1.0\n",
    "    if math.isclose(vmax, 0.0, abs_tol=1e-12):\n",
    "        vmax = 1.0\n",
    "    pad = vmax * 0.06\n",
    "    y_min, y_max = vmin, vmax + pad\n",
    "\n",
    "    for c_i, col_label in enumerate(matrix_df.columns):\n",
    "        ax = axes[r_i][c_i]\n",
    "        val = matrix_df.at[metric, col_label]\n",
    "        ax.bar([0], [val], width=0.6, color=colors[c_i])\n",
    "        ax.set_xlim(-0.8, 0.8)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        ax.set_xticks([])\n",
    "        ax.yaxis.set_major_locator(ticker.MaxNLocator(3))\n",
    "\n",
    "        if r_i == 0:\n",
    "            ax.set_title(col_label, fontsize=9)\n",
    "\n",
    "        if c_i == 0:\n",
    "            ax.set_ylabel(metric, fontsize=10)\n",
    "\n",
    "        try:\n",
    "            label_text = f\"{int(val):,}\"\n",
    "        except Exception:\n",
    "            label_text = f\"{val}\"\n",
    "        ax.text(0, y_min - (y_max - y_min) * 0.14, label_text, ha='center', va='top', fontsize=10)\n",
    "        for spine in ['top', 'right', 'left']:\n",
    "            ax.spines[spine].set_visible(False)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "summary = matrix_df.T.reset_index().rename(columns={'index': 'strategy'})\n",
    "print('\\nSummary (columns = metrics):')\n",
    "display(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "synapse_pyspark",
   "display_name": "Synapse PySpark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

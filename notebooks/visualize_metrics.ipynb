{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ],
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "  \"conf\": {\n",
        "    \"spark.notebook.parameters\": \"{}\"\n",
        "  },\n",
        "  \"defaultLakehouse\": {\n",
        "    \"name\": \"MetricsLakehouse\"\n",
        "  }\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ“Š 5. Visualize Metrics\n",
        "### ðŸ”— `MetricsLakehouse` is the data source.\n",
        "\n",
        "This notebook compares performance and storage across different ingestion, update, and query strategies.\n",
        "\n",
        "Activities visualized:\n",
        "- Initial ingestion\n",
        "- Update\n",
        "- Query\n",
        "- Storage cost for each target\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "merge_metrics_from_workspaces"
      },
      "source": [
        "# Merge metrics by reading each workspace's BenchmarkLakehouse Files/metrics ABFSS directory\n",
        "# Revised: use workspace display name verbatim for the ABFSS container (no .replace)\n",
        "# Uses mssparkutils.fs.ls to probe ABFSS paths and reads delta tables safely.\n",
        "\n",
        "import json\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "abfss_account = \"onelake.dfs.fabric.microsoft.com\"\n",
        "source_lakehouse_name = \"BenchmarkLakehouse\"\n",
        "controller_table = \"MetricsLakehouse.metrics\"\n",
        "\n",
        "# canonical metrics schema (used only to create empty table if missing)\n",
        "metrics_schema = StructType([\n",
        "    StructField(\"test_case_id\", StringType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"source\", StringType(), True),\n",
        "    StructField(\"format\", StringType(), True),\n",
        "    StructField(\"rows\", IntegerType(), True),\n",
        "    StructField(\"update_strategy\", StringType(), True),\n",
        "    StructField(\"action\", StringType(), True),\n",
        "    StructField(\"ingest_time_s\", FloatType(), True),\n",
        "    StructField(\"spinup_time_s\", FloatType(), True),\n",
        "    StructField(\"query_type\", StringType(), True),\n",
        "    StructField(\"query_time_s\", FloatType(), True),\n",
        "    StructField(\"notes\", StringType(), True)\n",
        "])\n",
        "\n",
        "# parse runs (safe default)\n",
        "conf_key = \"spark.notebook.parameters\"\n",
        "runs = json.loads(spark.conf.get(conf_key, \"{}\")).get(\"runs\", [])\n",
        "workspace_names = [r.get(\"name\") for r in runs if r.get(\"name\")]\n",
        "print(\"Workspaces to inspect (from runs):\", workspace_names)\n",
        "\n",
        "# ensure controller table exists\n",
        "if not spark.catalog.tableExists(controller_table):\n",
        "    empty = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema=metrics_schema)\n",
        "    empty.write.mode(\"overwrite\").saveAsTable(controller_table)\n",
        "\n",
        "dest_df = spark.table(controller_table)\n",
        "before_total = dest_df.count()\n",
        "print(f\"Controller metrics table found: {controller_table} rows={before_total}\")\n",
        "\n",
        "# prefer mssparkutils for path probing in Fabric notebooks\n",
        "try:\n",
        "    from notebookutils import mssparkutils\n",
        "except Exception:\n",
        "    try:\n",
        "        import mssparkutils\n",
        "    except Exception:\n",
        "        mssparkutils = None\n",
        "\n",
        "total_inserted = 0\n",
        "\n",
        "for ws in workspace_names:\n",
        "    if not ws:\n",
        "        continue\n",
        "\n",
        "    # Use workspace display name verbatim as the ABFSS container (test_case names are already safe)\n",
        "    container = str(ws)\n",
        "    # point to the delta table folder (avoid pointing into _delta_log)\n",
        "    source_table_root = f\"abfss://{container}@{abfss_account}/{source_lakehouse_name}.lakehouse/Tables/metrics\"\n",
        "    print(f\"\\nWorkspace '{ws}': probing {source_table_root}\")\n",
        "\n",
        "    path_exists = False\n",
        "    # Prefer mssparkutils.fs.ls which is safe in Fabric notebooks\n",
        "    if mssparkutils:\n",
        "        try:\n",
        "            ls_entries = mssparkutils.fs.ls(source_table_root)\n",
        "            # ls returns entries for files/dirs; non-empty list means the path is present\n",
        "            path_exists = bool(ls_entries)\n",
        "        except Exception:\n",
        "            # treat any exception as \"path does not exist or not accessible\"\n",
        "            path_exists = False\n",
        "    else:\n",
        "        # As a fallback, attempt a guarded delta load to detect presence; catch errors locally\n",
        "        try:\n",
        "            tmp = spark.read.format(\"delta\").load(source_table_root)\n",
        "            path_exists = True\n",
        "        except Exception:\n",
        "            path_exists = False\n",
        "\n",
        "    print(f\"  path_exists={path_exists}\")\n",
        "\n",
        "    if not path_exists:\n",
        "        print(f\"  Path not present or inaccessible for workspace '{ws}'; skipping.\")\n",
        "        continue\n",
        "\n",
        "    # read the delta table (delta reader handles _delta_log correctly)\n",
        "    src_df = spark.read.format(\"delta\").load(source_table_root)\n",
        "    src_count = src_df.count()\n",
        "    print(f\"  Read {src_count} rows from delta table at {source_table_root}\")\n",
        "\n",
        "    # Deduplicate exact rows against destination by using common columns\n",
        "    dest_cols = dest_df.columns\n",
        "    common_cols = [c for c in dest_cols if c in src_df.columns]\n",
        "    if not common_cols:\n",
        "        print(f\"  No common columns between source ({source_table_root}) and destination ({controller_table}); skipping\")\n",
        "        continue\n",
        "\n",
        "    src_sel = src_df.select(*common_cols)\n",
        "    dest_sel = dest_df.select(*common_cols)\n",
        "\n",
        "    new_rows = src_sel.join(dest_sel, on=common_cols, how=\"left_anti\")\n",
        "    insert_count = new_rows.count()\n",
        "    print(f\"  New unique rows to insert from '{ws}': {insert_count}\")\n",
        "\n",
        "    if insert_count > 0:\n",
        "        new_rows.write.mode(\"append\").saveAsTable(controller_table)\n",
        "        dest_df = spark.table(controller_table)\n",
        "        total_inserted += insert_count\n",
        "        print(f\"  Appended {insert_count} rows from '{ws}' into {controller_table}\")\n",
        "\n",
        "print(f\"\\nFinished: before_total={before_total}, total_inserted={total_inserted}\")\n",
        "after_total = spark.table(controller_table).count()\n",
        "print(f\"After total rows in {controller_table}: {after_total}\")\n",
        "\n",
        "metrics_df = spark.table(controller_table).toPandas()\n",
        "print(\"Loaded merged metrics into pandas DataFrame with rows:\", len(metrics_df))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Normalize numeric proxy fields (storage_size_mb and cu_used may be stored as floats in the metrics table)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "metrics_df['storage_size_mb'] = pd.to_numeric(metrics_df.get('storage_size_mb', pd.Series(np.nan)), errors='coerce')\n",
        "metrics_df['cu_used'] = pd.to_numeric(metrics_df.get('cu_used', pd.Series(np.nan)), errors='coerce')\n",
        "\n",
        "# Create integer proxy columns for display and plotting (None for missing)\n",
        "def to_int_proxy(x):\n",
        "    if pd.isna(x):\n",
        "        return None\n",
        "    try:\n",
        "        return int(x)\n",
        "    except Exception:\n",
        "        try:\n",
        "            return int(float(x))\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "metrics_df['storage_rows_proxy'] = metrics_df['storage_size_mb'].apply(to_int_proxy)\n",
        "metrics_df['cu_used_int'] = metrics_df['cu_used'].apply(to_int_proxy)\n",
        "\n",
        "metrics_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate storage_size_mb if missing (best effort)\n",
        "import math\n",
        "\n",
        "def calculate_storage_for_table(table_path):\n",
        "    try:\n",
        "        import mssparkutils\n",
        "        files = mssparkutils.fs.ls(table_path)\n",
        "        size_mb = sum(f.size for f in files) / (1024 * 1024)\n",
        "        return size_mb\n",
        "    except Exception:\n",
        "        return float('nan')\n",
        "\n",
        "# Identify unique targets and fill missing storage_size_mb when possible\n",
        "for idx, row in metrics_df.iterrows():\n",
        "    if ('storage_size_mb' in row and (pd.isna(row['storage_size_mb']) or math.isnan(row['storage_size_mb']))) and row.get('update_strategy'):\n",
        "        # crude extraction: use test_case_id to guess table name (customize if needed)\n",
        "        tc = row.get('test_case_id')\n",
        "        table_map = {\n",
        "            'TC.01.x': '/lakehouse/BenchmarkLakehouse/Tables/delta_refresh_load',\n",
        "            'TC.02.x': '/lakehouse/BenchmarkLakehouse/Tables/wh_table_refresh_load',\n",
        "            'TC.03.x': '/lakehouse/BenchmarkLakehouse/Tables/delta_refresh_load',\n",
        "            'TC.04.x': '/lakehouse/BenchmarkLakehouse/Tables/wh_table_refresh_load',\n",
        "            'TC.05.x': '/lakehouse/BenchmarkLakehouse/Tables/delta_compare_load',\n",
        "            'TC.06.x': '/lakehouse/BenchmarkLakehouse/Tables/wh_table_compare_load',\n",
        "            'TC.07.x': '/lakehouse/BenchmarkLakehouse/Tables/delta_increment_load',\n",
        "            'TC.08.x': '/lakehouse/BenchmarkLakehouse/Tables/wh_table_increment_load',\n",
        "        }\n",
        "        table_path = table_map.get(tc, None)\n",
        "        if table_path:\n",
        "            metrics_df.at[idx, 'storage_size_mb'] = calculate_storage_for_table(table_path)\n",
        "            # refresh proxy column when we populate storage_size_mb\n",
        "            try:\n",
        "                metrics_df.at[idx, 'storage_rows_proxy'] = int(metrics_df.at[idx, 'storage_size_mb'])\n",
        "            except Exception:\n",
        "                metrics_df.at[idx, 'storage_rows_proxy'] = None\n",
        "\n",
        "# Update the integer proxy column after any fills\n",
        "metrics_df['storage_size_mb'] = pd.to_numeric(metrics_df['storage_size_mb'], errors='coerce')\n",
        "metrics_df['storage_rows_proxy'] = metrics_df['storage_size_mb'].apply(lambda x: int(x) if not pd.isna(x) else None)\n",
        "metrics_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initial ingestion performance (stacked: Read_Time = spinup_time_s, Load Time = ingest_time_s)\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Select the Full Refresh metrics rows for Delta and Warehouse formats\n",
        "ingest_df = metrics_df[\n",
        "    (metrics_df['update_strategy'] == 'Full Refresh')\n",
        "].copy()\n",
        "\n",
        "if ingest_df.empty:\n",
        "    print(\"No Full Refresh metrics found in metrics_df.\")\n",
        "else:\n",
        "    # If multiple runs exist, take the latest row per format\n",
        "    if 'timestamp' in ingest_df.columns:\n",
        "        ingest_summary = ingest_df.sort_values('timestamp').groupby('format', as_index=False).last()\n",
        "    else:\n",
        "        ingest_summary = ingest_df.groupby('format', as_index=False).last()\n",
        "\n",
        "    # Ensure a stable order of formats for plotting\n",
        "    formats = ['Delta', 'Warehouse']\n",
        "    ingest_summary = ingest_summary.set_index('format').reindex(formats).reset_index()\n",
        "\n",
        "    # Extract read (spinup) and load (ingest) times; coerce missing to 0.0\n",
        "    read_times = ingest_summary['spinup_time_s'].fillna(0.0).astype(float).values\n",
        "    load_times = ingest_summary['ingest_time_s'].fillna(0.0).astype(float).values\n",
        "    labels = ingest_summary['format'].fillna('').values\n",
        "\n",
        "    # Plot stacked bars\n",
        "    x = np.arange(len(labels))\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    ax.bar(x, read_times, color='skyblue', label='Read Time (spinup)')\n",
        "    ax.bar(x, load_times, bottom=read_times, color='orange', label='Load Time (ingest)')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.set_title(\"Initial Ingestion Time by Format (Read = shared, Load = per-format)\")\n",
        "    ax.set_ylabel(\"Time (s)\")\n",
        "    ax.legend()\n",
        "\n",
        "    # Annotate component and total values\n",
        "    max_total = float(np.max(read_times + load_times)) if len(labels) > 0 else 0.0\n",
        "    for i in range(len(labels)):\n",
        "        r = read_times[i]\n",
        "        l = load_times[i]\n",
        "        total = r + l\n",
        "        if r > 0:\n",
        "            ax.text(x[i], r / 2, f\"{r:.2f}s\", ha='center', va='center', color='white', fontsize=9)\n",
        "        if l > 0:\n",
        "            ax.text(x[i], r + l / 2, f\"{l:.2f}s\", ha='center', va='center', color='black', fontsize=9)\n",
        "        ax.text(x[i], total + max_total * 0.03, f\"Total: {total:.2f}s\", ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Update performance comparison\n",
        "update_df = metrics_df[metrics_df['update_strategy'].isin(['Full Compare', 'Incremental'])]\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(8,4))\n",
        "for strategy in update_df['update_strategy'].unique():\n",
        "    strat_df = update_df[update_df['update_strategy'] == strategy]\n",
        "    plt.bar(strat_df['format'] + \" \" + strat_df['update_strategy'], strat_df['ingest_time_s'], label=strategy)\n",
        "plt.title(\"Update Time by Strategy and Format\")\n",
        "plt.xlabel(\"Strategy\")\n",
        "plt.ylabel(\"Update Time (s)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print('Update performance chart complete')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Query performance comparison\n",
        "query_df = metrics_df[metrics_df['query_type'].notna()]\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(query_df['query_type'], query_df['query_time_s'], color='seagreen')\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Query Performance Comparison\")\n",
        "plt.xlabel(\"Query Type\")\n",
        "plt.ylabel(\"Query Time (s)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print('Query performance chart complete')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Storage cost comparison (use integer proxy column created above)\n",
        "storage_df = metrics_df.dropna(subset=['storage_rows_proxy'])\n",
        "storage_summary = storage_df.groupby(['format', 'update_strategy'])['storage_rows_proxy'].mean().reset_index()\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(storage_summary['format'] + \" \" + storage_summary['update_strategy'], storage_summary['storage_rows_proxy'], color='orchid')\n",
        "plt.title(\"Storage Size by Target Table\")\n",
        "plt.xlabel(\"Target Table\")\n",
        "plt.ylabel(\"Storage Size (rows) â€” row-count proxy\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print('Storage comparison chart complete')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Show all metrics as interactive table\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "# Expose the two proxy columns for clarity in the table view\n",
        "display(metrics_df.assign(storage_rows_proxy=metrics_df['storage_rows_proxy'], cu_used_int=metrics_df['cu_used_int']))\n",
        "print('Displayed metrics table')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualize Metrics Matrix (Compact 5Ã—6)\n",
        "\n",
        "This compact matrix provides the small-multiples view we discussed: six columns (strategies) Ã— five rows (metrics). It uses the same metrics table and the integer row-count proxies for storage_s[...]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build and display the 5x6 metrics matrix\n",
        "import math\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import ticker\n",
        "import pandas as pd\n",
        "\n",
        "cols = [\n",
        "    (\"Warehouse\", \"Full Refresh\", \"TC.04.x\"),\n",
        "    (\"Warehouse\", \"Full Compare\", \"TC.06.x\"),\n",
        "    (\"Warehouse\", \"Incremental\", \"TC.08.x\"),\n",
        "    (\"Delta\",     \"Full Refresh\", \"TC.03.x\"),\n",
        "    (\"Delta\",     \"Full Compare\", \"TC.05.x\"),\n",
        "    (\"Delta\",     \"Incremental\", \"TC.07.x\"),\n",
        "]\n",
        "ingest_tc_by_format = {\"Delta\": \"TC.01.x\", \"Warehouse\": \"TC.02.x\"}\n",
        "\n",
        "def latest_row_for_testcase(df, tc):\n",
        "    if tc not in df['test_case_id'].values:\n",
        "        return None\n",
        "    sub = df[df['test_case_id'] == tc]\n",
        "    if 'timestamp' in sub.columns:\n",
        "        sub = sub.sort_values('timestamp')\n",
        "    return sub.iloc[-1]\n",
        "\n",
        "def sget_int(s, colname, default=0):\n",
        "    if s is None or colname not in s.index:\n",
        "        return int(default)\n",
        "    val = s[colname]\n",
        "    if pd.isna(val):\n",
        "        return int(default)\n",
        "    try:\n",
        "        return int(val)\n",
        "    except Exception:\n",
        "        try:\n",
        "            return int(float(val))\n",
        "        except Exception:\n",
        "            return int(default)\n",
        "\n",
        "def sget_float(s, colname, default=0.0):\n",
        "    if s is None or colname not in s.index:\n",
        "        return float(default)\n",
        "    val = s[colname]\n",
        "    if pd.isna(val):\n",
        "        return float(default)\n",
        "    try:\n",
        "        return float(val)\n",
        "    except Exception:\n",
        "        return float(default)\n",
        "\n",
        "ingest_rows = []\n",
        "ingest_time_s = []\n",
        "update_rows = []\n",
        "update_time_s = []\n",
        "storage_rows = []\n",
        "col_labels = []\n",
        "\n",
        "for fmt, strat, update_tc in cols:\n",
        "    col_labels.append(f\"{fmt}\\n{strat}\")\n",
        "    ingest_tc = ingest_tc_by_format.get(fmt)\n",
        "    ingest_row = latest_row_for_testcase(metrics_df, ingest_tc)\n",
        "    ingest_rows.append(sget_int(ingest_row, 'rows', default=0))\n",
        "    ingest_time_s.append(sget_float(ingest_row, 'ingest_time_s', default=0.0))\n",
        "\n",
        "    update_row = latest_row_for_testcase(metrics_df, update_tc)\n",
        "    ur = 0\n",
        "    if update_row is not None:\n",
        "        if 'cu_used' in update_row.index and (not pd.isna(update_row['cu_used'])):\n",
        "            try:\n",
        "                ur = int(update_row['cu_used'])\n",
        "            except Exception:\n",
        "                ur = sget_int(update_row, 'rows', default=0)\n",
        "        else:\n",
        "            ur = sget_int(update_row, 'rows', default=0)\n",
        "    update_rows.append(int(ur))\n",
        "    update_time_s.append(sget_float(update_row, 'ingest_time_s', default=0.0))\n",
        "\n",
        "    sr = None\n",
        "    if update_row is not None and 'storage_size_mb' in update_row.index and (not pd.isna(update_row['storage_size_mb'])):\n",
        "        try:\n",
        "            sr = int(update_row['storage_size_mb'])\n",
        "        except Exception:\n",
        "            sr = None\n",
        "    if sr is None and ingest_row is not None and 'storage_size_mb' in ingest_row.index and (not pd.isna(ingest_row['storage_size_mb'])):\n",
        "        try:\n",
        "            sr = int(ingest_row['storage_size_mb'])\n",
        "        except Exception:\n",
        "            sr = None\n",
        "    if sr is None:\n",
        "        sr = 0\n",
        "    storage_rows.append(int(sr))\n",
        "\n",
        "matrix_df = pd.DataFrame(\n",
        "    data=[ingest_rows, ingest_time_s, update_rows, update_time_s, storage_rows],\n",
        "    index=[\"Ingestion rows\", \"Ingestion time_s\", \"Update rows\", \"Update time_s\", \"Storage rows\"],\n",
        "    columns=col_labels\n",
        ")\n",
        "\n",
        "print(\"Metrics matrix (values):\")\n",
        "display(matrix_df)\n",
        "\n",
        "n_rows, n_cols = matrix_df.shape\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(3 * n_cols, 2.2 * n_rows), squeeze=False)\n",
        "plt.subplots_adjust(hspace=0.6, wspace=0.6, top=0.93)\n",
        "fig.suptitle(\"Metrics matrix (rows = metrics, cols = strategies)\", fontsize=16)\n",
        "\n",
        "colors = [\"#2a9d8f\"] * n_cols\n",
        "\n",
        "for r_i, metric in enumerate(matrix_df.index):\n",
        "    vals = matrix_df.loc[metric].astype(float).values\n",
        "    vmin = 0.0\n",
        "    vmax = float(np.nanmax(vals)) if len(vals) > 0 else 1.0\n",
        "    if math.isclose(vmax, 0.0, abs_tol=1e-12):\n",
        "        vmax = 1.0\n",
        "    pad = vmax * 0.06\n",
        "    y_min, y_max = vmin, vmax + pad\n",
        "\n",
        "    for c_i, col_label in enumerate(matrix_df.columns):\n",
        "        ax = axes[r_i][c_i]\n",
        "        val = matrix_df.at[metric, col_label]\n",
        "        ax.bar([0], [val], width=0.6, color=colors[c_i])\n",
        "        ax.set_xlim(-0.8, 0.8)\n",
        "        ax.set_ylim(y_min, y_max)\n",
        "        ax.set_xticks([])\n",
        "        ax.yaxis.set_major_locator(ticker.MaxNLocator(3))\n",
        "\n",
        "        if r_i == 0:\n",
        "            ax.set_title(col_label, fontsize=9)\n",
        "\n",
        "        if c_i == 0:\n",
        "            ax.set_ylabel(metric, fontsize=10)\n",
        "\n",
        "        try:\n",
        "            label_text = f\"{int(val):,}\"\n",
        "        except Exception:\n",
        "            label_text = f\"{val}\"\n",
        "        ax.text(0, y_min - (y_max - y_min) * 0.14, label_text, ha='center', va='top', fontsize=10)\n",
        "        for spine in ['top', 'right', 'left']:\n",
        "            ax.spines[spine].set_visible(False)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "summary = matrix_df.T.reset_index().rename(columns={'index': 'strategy'})\n",
        "print('\\nSummary (columns = metrics):')\n",
        "display(summary)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "completion",
      "metadata": {},
      "source": [
        "print(\"Completion\")\n",
        "print(\"Visualizations produced from MetricsLakehouse.metrics. Controller metrics merge complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Visualize Metrics\n",
    "### ðŸ”— Ensure `BenchmarkLakehouse` is connected as a data source before running.\n",
    "\n",
    "This notebook compares performance and storage across different ingestion, update, and query strategies.\n",
    "\n",
    "Activities visualized:\n",
    "- Initial ingestion\n",
    "- Update\n",
    "- Query\n",
    "- Storage cost for each target\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load metrics table from Lakehouse (or local CSV for dev) and normalize proxy fields\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "try:\n",
    "    metrics_df = spark.read.table(\"BenchmarkLakehouse.metrics\").toPandas()\n",
    "except Exception:\n",
    "    metrics_df = pd.read_csv(\"metrics.csv\")\n",
    "\n",
    "# Normalize numeric proxy fields (storage_size_mb and cu_used may be stored as floats in the metrics table)\n",
    "metrics_df['storage_size_mb'] = pd.to_numeric(metrics_df.get('storage_size_mb', pd.Series(np.nan)), errors='coerce')\n",
    "metrics_df['cu_used'] = pd.to_numeric(metrics_df.get('cu_used', pd.Series(np.nan)), errors='coerce')\n",
    "\n",
    "# Create integer proxy columns for display and plotting (None for missing)\n",
    "def to_int_proxy(x):\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return int(float(x))\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "metrics_df['storage_rows_proxy'] = metrics_df['storage_size_mb'].apply(to_int_proxy)\n",
    "metrics_df['cu_used_int'] = metrics_df['cu_used'].apply(to_int_proxy)\n",
    "\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate storage_size_mb if missing (best effort)\n",
    "import math\n",
    "\n",
    "def calculate_storage_for_table(table_path):\n",
    "    try:\n",
    "        import mssparkutils\n",
    "        files = mssparkutils.fs.ls(table_path)\n",
    "        size_mb = sum(f.size for f in files) / (1024 * 1024)\n",
    "        return size_mb\n",
    "    except Exception:\n",
    "        return float('nan')\n",
    "\n",
    "# Identify unique targets and fill missing storage_size_mb when possible\n",
    "for idx, row in metrics_df.iterrows():\n",
    "    if ('storage_size_mb' in row and (pd.isna(row['storage_size_mb']) or math.isnan(row['storage_size_mb']))) and row['update_strategy']:\n",
    "        # crude extraction: use test_case_id to guess table name (customize if needed)\n",
    "        tc = row['test_case_id']\n",
    "        table_map = {\n",
    "            'TC.01.x': '/lakehouse/BenchmarkLakehouse/Tables/delta_refresh_load',\n",
    "            'TC.02.x': '/lakehouse/BenchmarkLakehouse/Tables/wh_table_refresh_load',\n",
    "            'TC.03.x': '/lakehouse/BenchmarkLakehouse/Tables/delta_refresh_load',\n",
    "            'TC.04.x': '/lakehouse/BenchmarkLakehouse/Tables/wh_table_refresh_load',\n",
    "            'TC.05.x': '/lakehouse/BenchmarkLakehouse/Tables/delta_compare_load',\n",
    "            'TC.06.x': '/lakehouse/BenchmarkLakehouse/Tables/wh_table_compare_load',\n",
    "            'TC.07.x': '/lakehouse/BenchmarkLakehouse/Tables/delta_increment_load',\n",
    "            'TC.08.x': '/lakehouse/BenchmarkLakehouse/Tables/wh_table_increment_load',\n",
    "        }\n",
    "        table_path = table_map.get(tc, None)\n",
    "        if table_path:\n",
    "            metrics_df.at[idx, 'storage_size_mb'] = calculate_storage_for_table(table_path)\n",
    "            # refresh proxy column when we populate storage_size_mb\n",
    "            try:\n",
    "                metrics_df.at[idx, 'storage_rows_proxy'] = int(metrics_df.at[idx, 'storage_size_mb'])\n",
    "            except Exception:\n",
    "                metrics_df.at[idx, 'storage_rows_proxy'] = None\n",
    "\n",
    "# Update the integer proxy column after any fills\n",
    "metrics_df['storage_size_mb'] = pd.to_numeric(metrics_df['storage_size_mb'], errors='coerce')\n",
    "metrics_df['storage_rows_proxy'] = metrics_df['storage_size_mb'].apply(lambda x: int(x) if not pd.isna(x) else None)\n",
    "metrics_df\n"
   ]
  },
  {
  "cell_type": "code",
  "metadata": {
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark"
    }
  },
  "execution_count": null,
  "outputs": [],
  "source": [
    "# Initial ingestion performance (stacked: Read_Time = spinup_time_s, Load Time = ingest_time_s)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Select the Full Refresh metrics rows for TC.01.x (Delta) and TC.02.x (Warehouse)\n",
    "ingest_df = metrics_df[\n",
    "    (metrics_df['update_strategy'] == 'Full Refresh') &\n",
    "    (metrics_df['test_case_id'].isin(['TC.01.x', 'TC.02.x']))\n",
    "].copy()\n",
    "\n",
    "if ingest_df.empty:\n",
    "    print(\"No Full Refresh metrics for TC.01.x/TC.02.x found in metrics_df.\")\n",
    "else:\n",
    "    # If multiple runs exist, take the latest row per format\n",
    "    if 'timestamp' in ingest_df.columns:\n",
    "        ingest_summary = ingest_df.sort_values('timestamp').groupby('format', as_index=False).last()\n",
    "    else:\n",
    "        ingest_summary = ingest_df.groupby('format', as_index=False).last()\n",
    "\n",
    "    # Ensure a stable order of formats for plotting\n",
    "    formats = ['Delta', 'Warehouse']\n",
    "    ingest_summary = ingest_summary.set_index('format').reindex(formats).reset_index()\n",
    "\n",
    "    # Extract read (spinup) and load (ingest) times; coerce missing to 0.0\n",
    "    read_times = ingest_summary['spinup_time_s'].fillna(0.0).astype(float).values\n",
    "    load_times = ingest_summary['ingest_time_s'].fillna(0.0).astype(float).values\n",
    "    labels = ingest_summary['format'].fillna('').values\n",
    "\n",
    "    # Plot stacked bars\n",
    "    x = np.arange(len(labels))\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.bar(x, read_times, color='skyblue', label='Read Time (spinup)')\n",
    "    ax.bar(x, load_times, bottom=read_times, color='orange', label='Load Time (ingest)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_title(\"Initial Ingestion Time by Format (Read = shared, Load = per-format)\")\n",
    "    ax.set_ylabel(\"Time (s)\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Annotate component and total values\n",
    "    max_total = float(np.max(read_times + load_times)) if len(labels) > 0 else 0.0\n",
    "    for i in range(len(labels)):\n",
    "        r = read_times[i]\n",
    "        l = load_times[i]\n",
    "        total = r + l\n",
    "        if r > 0:\n",
    "            ax.text(x[i], r / 2, f\"{r:.2f}s\", ha='center', va='center', color='white', fontsize=9)\n",
    "        if l > 0:\n",
    "            ax.text(x[i], r + l / 2, f\"{l:.2f}s\", ha='center', va='center', color='black', fontsize=9)\n",
    "        ax.text(x[i], total + max_total * 0.03, f\"Total: {total:.2f}s\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.show()\n"
  ]
},
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Update performance comparison\n",
    "update_df = metrics_df[metrics_df['update_strategy'].isin(['Full Compare', 'Incremental'])]\n",
    "plt.figure(figsize=(8,4))\n",
    "for strategy in update_df['update_strategy'].unique():\n",
    "    strat_df = update_df[update_df['update_strategy'] == strategy]\n",
    "    plt.bar(strat_df['format'] + \" \" + strat_df['update_strategy'], strat_df['ingest_time_s'], label=strategy)\n",
    "plt.title(\"Update Time by Strategy and Format\")\n",
    "plt.xlabel(\"Strategy\")\n",
    "plt.ylabel(\"Update Time (s)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Query performance comparison\n",
    "query_df = metrics_df[metrics_df['query_type'].notna()]\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(query_df['query_type'], query_df['query_time_s'], color='seagreen')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Query Performance Comparison\")\n",
    "plt.xlabel(\"Query Type\")\n",
    "plt.ylabel(\"Query Time (s)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Storage cost comparison (use integer proxy column created above)\n",
    "storage_df = metrics_df.dropna(subset=['storage_rows_proxy'])\n",
    "storage_summary = storage_df.groupby(['format', 'update_strategy'])['storage_rows_proxy'].mean().reset_index()\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(storage_summary['format'] + \" \" + storage_summary['update_strategy'], storage_summary['storage_rows_proxy'], color='orchid')\n",
    "plt.title(\"Storage Size by Target Table\")\n",
    "plt.xlabel(\"Target Table\")\n",
    "plt.ylabel(\"Storage Size (rows) â€” row-count proxy\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show all metrics as interactive table\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "# Expose the two proxy columns for clarity in the table view\n",
    "display(metrics_df.assign(storage_rows_proxy=metrics_df['storage_rows_proxy'], cu_used_int=metrics_df['cu_used_int']))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Visualization complete. Review charts above for performance and storage comparisons across ingestion, update, and query activities.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Metrics Matrix (Compact 5Ã—6)\n",
    "\n",
    "This compact matrix provides the small-multiples view we discussed: six columns (strategies) Ã— five rows (metrics). It uses the same metrics table and the integer row-count proxies for storage_size_mb and cu_used."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build and display the 5x6 metrics matrix\n",
    "import math\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "\n",
    "cols = [\n",
    "    (\"Warehouse\", \"Full Refresh\", \"TC.04.x\"),\n",
    "    (\"Warehouse\", \"Full Compare\", \"TC.06.x\"),\n",
    "    (\"Warehouse\", \"Incremental\", \"TC.08.x\"),\n",
    "    (\"Delta\",     \"Full Refresh\", \"TC.03.x\"),\n",
    "    (\"Delta\",     \"Full Compare\", \"TC.05.x\"),\n",
    "    (\"Delta\",     \"Incremental\", \"TC.07.x\"),\n",
    "]\n",
    "ingest_tc_by_format = {\"Delta\": \"TC.01.x\", \"Warehouse\": \"TC.02.x\"}\n",
    "\n",
    "def latest_row_for_testcase(df, tc):\n",
    "    if tc not in df['test_case_id'].values:\n",
    "        return None\n",
    "    sub = df[df['test_case_id'] == tc]\n",
    "    if 'timestamp' in sub.columns:\n",
    "        sub = sub.sort_values('timestamp')\n",
    "    return sub.iloc[-1]\n",
    "\n",
    "def sget_int(s, colname, default=0):\n",
    "    if s is None or colname not in s.index:\n",
    "        return int(default)\n",
    "    val = s[colname]\n",
    "    if pd.isna(val):\n",
    "        return int(default)\n",
    "    try:\n",
    "        return int(val)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return int(float(val))\n",
    "        except Exception:\n",
    "            return int(default)\n",
    "\n",
    "def sget_float(s, colname, default=0.0):\n",
    "    if s is None or colname not in s.index:\n",
    "        return float(default)\n",
    "    val = s[colname]\n",
    "    if pd.isna(val):\n",
    "        return float(default)\n",
    "    try:\n",
    "        return float(val)\n",
    "    except Exception:\n",
    "        return float(default)\n",
    "\n",
    "ingest_rows = []\n",
    "ingest_time_s = []\n",
    "update_rows = []\n",
    "update_time_s = []\n",
    "storage_rows = []\n",
    "col_labels = []\n",
    "\n",
    "for fmt, strat, update_tc in cols:\n",
    "    col_labels.append(f\"{fmt}\\n{strat}\")\n",
    "    ingest_tc = ingest_tc_by_format.get(fmt)\n",
    "    ingest_row = latest_row_for_testcase(metrics_df, ingest_tc)\n",
    "    ingest_rows.append(sget_int(ingest_row, 'rows', default=0))\n",
    "    ingest_time_s.append(sget_float(ingest_row, 'ingest_time_s', default=0.0))\n",
    "\n",
    "    update_row = latest_row_for_testcase(metrics_df, update_tc)\n",
    "    ur = 0\n",
    "    if update_row is not None:\n",
    "        if 'cu_used' in update_row.index and (not pd.isna(update_row['cu_used'])):\n",
    "            try:\n",
    "                ur = int(update_row['cu_used'])\n",
    "            except Exception:\n",
    "                ur = sget_int(update_row, 'rows', default=0)\n",
    "        else:\n",
    "            ur = sget_int(update_row, 'rows', default=0)\n",
    "    update_rows.append(int(ur))\n",
    "    update_time_s.append(sget_float(update_row, 'ingest_time_s', default=0.0))\n",
    "\n",
    "    sr = None\n",
    "    if update_row is not None and 'storage_size_mb' in update_row.index and (not pd.isna(update_row['storage_size_mb'])):\n",
    "        try:\n",
    "            sr = int(update_row['storage_size_mb'])\n",
    "        except Exception:\n",
    "            sr = None\n",
    "    if sr is None and ingest_row is not None and 'storage_size_mb' in ingest_row.index and (not pd.isna(ingest_row['storage_size_mb'])):\n",
    "        try:\n",
    "            sr = int(ingest_row['storage_size_mb'])\n",
    "        except Exception:\n",
    "            sr = None\n",
    "    if sr is None:\n",
    "        sr = 0\n",
    "    storage_rows.append(int(sr))\n",
    "\n",
    "import pandas as pd\n",
    "matrix_df = pd.DataFrame(\n",
    "    data=[ingest_rows, ingest_time_s, update_rows, update_time_s, storage_rows],\n",
    "    index=[\"Ingestion rows\", \"Ingestion time_s\", \"Update rows\", \"Update time_s\", \"Storage rows\"],\n",
    "    columns=col_labels\n",
    ")\n",
    "\n",
    "print(\"Metrics matrix (values):\")\n",
    "display(matrix_df)\n",
    "\n",
    "n_rows, n_cols = matrix_df.shape\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(3 * n_cols, 2.2 * n_rows), squeeze=False)\n",
    "plt.subplots_adjust(hspace=0.6, wspace=0.6, top=0.93)\n",
    "fig.suptitle(\"Metrics matrix (rows = metrics, cols = strategies)\", fontsize=16)\n",
    "\n",
    "colors = [\"#2a9d8f\"] * n_cols\n",
    "\n",
    "for r_i, metric in enumerate(matrix_df.index):\n",
    "    vals = matrix_df.loc[metric].astype(float).values\n",
    "    vmin = 0.0\n",
    "    vmax = float(np.nanmax(vals)) if len(vals) > 0 else 1.0\n",
    "    if math.isclose(vmax, 0.0, abs_tol=1e-12):\n",
    "        vmax = 1.0\n",
    "    pad = vmax * 0.06\n",
    "    y_min, y_max = vmin, vmax + pad\n",
    "\n",
    "    for c_i, col_label in enumerate(matrix_df.columns):\n",
    "        ax = axes[r_i][c_i]\n",
    "        val = matrix_df.at[metric, col_label]\n",
    "        ax.bar([0], [val], width=0.6, color=colors[c_i])\n",
    "        ax.set_xlim(-0.8, 0.8)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        ax.set_xticks([])\n",
    "        ax.yaxis.set_major_locator(ticker.MaxNLocator(3))\n",
    "\n",
    "        if r_i == 0:\n",
    "            ax.set_title(col_label, fontsize=9)\n",
    "\n",
    "        if c_i == 0:\n",
    "            ax.set_ylabel(metric, fontsize=10)\n",
    "\n",
    "        try:\n",
    "            label_text = f\"{int(val):,}\"\n",
    "        except Exception:\n",
    "            label_text = f\"{val}\"\n",
    "        ax.text(0, y_min - (y_max - y_min) * 0.14, label_text, ha='center', va='top', fontsize=10)\n",
    "        for spine in ['top', 'right', 'left']:\n",
    "            ax.spines[spine].set_visible(False)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "summary = matrix_df.T.reset_index().rename(columns={'index': 'strategy'})\n",
    "print('\\nSummary (columns = metrics):')\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "completion",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Completion\")\n",
    "print(\"Visualizations produced, per TC.16.x.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "synapse_pyspark",
   "display_name": "Synapse PySpark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

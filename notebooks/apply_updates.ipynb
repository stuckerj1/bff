{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ],
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "  \"conf\": {\n",
        "    \"spark.notebook.parameters\": \"{}\"\n",
        "  },\n",
        "  \"defaultLakehouse\": {\n",
        "    \"name\": \"BenchmarkLakehouse\"\n",
        "  }\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““2. Apply Updates\n",
        "### ðŸ’¤ Wake up Azure SQL before running.\n",
        "\n",
        "Happyâ€‘path notebook that implements three strategies: Full Refresh, Full Compare (append events), and Incremental (append updates).\n",
        "This notebook follows the ingest_data conventions for parameter handling and metrics. It expects the deployment tooling to populate the %%configure cell with the parameter set (see provision_notebooks.py).\n",
        "Style: minimal, linear, assert preconditions, debug prints, let errors surface."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Imports and small helpers used across cells\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit\n",
        "# synapsesql needs this import to work\n",
        "from com.microsoft.spark.fabric import Constants\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
        "print('Imports ready')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Helper: make specified columns nullable (used for warehouse writes to match schema expectations)\n",
        "from pyspark.sql.types import StructType, StructField\n",
        "def make_columns_nullable(df, columns=None):\n",
        "    if columns is None:\n",
        "        columns = [f.name for f in df.schema.fields]\n",
        "    new_schema = StructType([\n",
        "        StructField(f.name, f.dataType, True) if f.name in columns else f\n",
        "        for f in df.schema.fields\n",
        "    ])\n",
        "    return df.sparkSession.createDataFrame(df.rdd, schema=new_schema)\n",
        "print('make_columns_nullable ready')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Parameters and global variables (happy-path, assert presence)\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "conf_key = 'spark.notebook.parameters'\n",
        "conf_str = None\n",
        "try:\n",
        "    conf_str = spark.conf.get(conf_key, None)\n",
        "except Exception:\n",
        "    conf_str = None\n",
        "if not conf_str:\n",
        "    try:\n",
        "        conf_str = spark.sparkContext.getConf().get(conf_key, None)\n",
        "    except Exception:\n",
        "        conf_str = None\n",
        "if not conf_str:\n",
        "    raise SystemExit('Missing spark.notebook.parameters. Provisioning/dev should populate the cell before run.')\n",
        "\n",
        "params = json.loads(conf_str)\n",
        "\n",
        "# Minimal expected keys; we assert existence but do not defensively default other than optional AZURE_SQL_* fields inserted by provisioner\n",
        "required = ['name', 'dataset_name', 'source', 'format', 'update_strategy']\n",
        "missing = [k for k in required if k not in params]\n",
        "if missing:\n",
        "    raise SystemExit(f\"Missing required parameters: {', '.join(missing)}\")\n",
        "\n",
        "test_case_name = params['name']\n",
        "dataset_name = params['dataset_name']\n",
        "source = str(params['source']).lower()   # 'lakehouse' or 'sql'\n",
        "fmt = str(params['format']).lower()      # 'delta' or 'warehouse'\n",
        "update_strategy = str(params['update_strategy']).lower()  # 'full refresh', 'full compare', 'incremental'\n",
        "notes = params.get('notes', '')\n",
        "\n",
        "# SQL connection info if present\n",
        "AZURE_SQL_SERVER = params.get('AZURE_SQL_SERVER')\n",
        "AZURE_SQL_DB = params.get('AZURE_SQL_DB')\n",
        "AZURE_SQL_SCHEMA = params.get('AZURE_SQL_SCHEMA', 'dbo')\n",
        "\n",
        "# Reuse naming logic from ingest_data: sanitized target name\n",
        "import re\n",
        "sanitized_name = re.sub(r\"[^a-z0-9_]\", \"\", re.sub(r\"[\\s-]+\", \"_\", test_case_name.strip().lower()))\n",
        "target_table = sanitized_name\n",
        "\n",
        "# Use the controller (BFF-Controller) DataSourceLakehouse ABFSS paths for source data (match ingest_data pattern)\n",
        "controller_workspace_name = 'BFF-Controller'\n",
        "controller_lakehouse_name = 'DataSourceLakehouse'\n",
        "abfss_account = 'onelake.dfs.fabric.microsoft.com'\n",
        "container = controller_workspace_name.replace(' ', '')\n",
        "\n",
        "# Build ABFSS URIs to controller lakehouse Files area (point to directory for Spark reads)\n",
        "source_current_parquet_dir = f\"abfss://{container}@{abfss_account}/{controller_lakehouse_name}.Lakehouse/Files/{dataset_name}current\"\n",
        "source_updates_parquet = f\"abfss://{container}@{abfss_account}/{controller_lakehouse_name}.Lakehouse/Files/{dataset_name}updates/updates.parquet\"\n",
        "\n",
        "# Targets (fallback to same names as ingest_data)\n",
        "target_lakehouse = params.get('target_lakehouse', 'BenchmarkLakehouse')\n",
        "target_warehouse = params.get('target_warehouse', 'BenchmarkWarehouse')\n",
        "\n",
        "# Staging naming for warehouse flows\n",
        "staging_table = f\"{AZURE_SQL_SCHEMA}.dbo.stg_{sanitized_name}\"\n",
        "\n",
        "print(f\"Loaded params: test_case={test_case_name} dataset={dataset_name} source={source} format={fmt} strategy={update_strategy}\")\n",
        "print('Source ABFSS paths:', source_current_parquet_dir, source_updates_parquet)\n",
        "\n",
        "# Expose as globals for later cells\n",
        "globals().update({\n",
        "    'test_case_name': test_case_name,\n",
        "    'dataset_name': dataset_name,\n",
        "    'source': source,\n",
        "    'fmt': fmt,\n",
        "    'update_strategy': update_strategy,\n",
        "    'AZURE_SQL_SERVER': AZURE_SQL_SERVER,\n",
        "    'AZURE_SQL_DB': AZURE_SQL_DB,\n",
        "    'AZURE_SQL_SCHEMA': AZURE_SQL_SCHEMA,\n",
        "    'source_current_parquet_dir': source_current_parquet_dir,\n",
        "    'source_updates_parquet': source_updates_parquet,\n",
        "    'target_lakehouse': target_lakehouse,\n",
        "    'target_warehouse': target_warehouse,\n",
        "    'target_table': target_table,\n",
        "    'staging_table': staging_table,\n",
        "    'notes': notes\n",
        "})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Metrics helper (append a single-row metrics entry to target_lakehouse.metrics)\n",
        "metrics_schema = StructType([\n",
        "    StructField(\"test_case_id\", StringType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"source\", StringType(), True),\n",
        "    StructField(\"format\", StringType(), True),\n",
        "    StructField(\"rows\", IntegerType(), True),\n",
        "    StructField(\"update_strategy\", StringType(), True),\n",
        "    StructField(\"action\", StringType(), True),\n",
        "    StructField(\"ingest_time_s\", FloatType(), True),\n",
        "    StructField(\"spinup_time_s\", FloatType(), True),\n",
        "    StructField(\"query_type\", StringType(), True),\n",
        "    StructField(\"query_time_s\", FloatType(), True),\n",
        "    StructField(\"notes\", StringType(), True)\n",
        "])\n",
        "def append_metrics(action, rows, ingest_time_s, spinup_time_s=0.0, query_type='N/A', query_time_s=float('nan'), notes_local=None):\n",
        "    notes_local = notes_local if notes_local is not None else notes\n",
        "    row = [(test_case_name, datetime.now(), source, fmt.upper(), int(rows), update_strategy.title(), action, float(ingest_time_s), float(spinup_time_s), query_type, float(query_time_s), notes_local)]\n",
        "    spark.createDataFrame(row, schema=metrics_schema).write.mode('append').saveAsTable(f\"{target_lakehouse}.metrics\")\n",
        "    print('Metrics appended for action=', action)\n",
        "print('Metrics helper ready')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Helper cell: canonicalize ts_* columns to legacy Spark TimestampType\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def canonicalize_timestamps(df, prefix=\"ts_\"):\n",
        "    \"\"\"\n",
        "    Cast any columns starting with prefix to legacy Spark TimestampType ('timestamp').\n",
        "    Returns the DataFrame (unchanged if no ts_* columns present).\n",
        "    \"\"\"\n",
        "    ts_cols = [c for c in df.columns if c.startswith(prefix)]\n",
        "    if not ts_cols:\n",
        "        return df\n",
        "    print(\"Canonicalizing timestamp columns to legacy Spark TimestampType:\", ts_cols)\n",
        "    for c in ts_cols:\n",
        "        df = df.withColumn(c, col(c).cast(\"timestamp\"))\n",
        "    return df\n",
        "\n",
        "print('canonicalize_timestamps helper ready')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# SQL helpers: token + connection reused from ingest_data patterns (mssparkutils is available as per note)\n",
        "from notebookutils import mssparkutils\n",
        "SQL_COPT_SS_ACCESS_TOKEN = 1256\n",
        "import struct\n",
        "import pyodbc\n",
        "\n",
        "def _token_struct():\n",
        "    t = mssparkutils.credentials.getToken('https://database.windows.net/')\n",
        "    exptoken = b''.join(bytes([c]) + b'\\x00' for c in t.encode('utf-8'))\n",
        "    return struct.pack('=i', len(exptoken)) + exptoken\n",
        "\n",
        "def _pyodbc_conn_with_retry(server=None, database=None, timeout=120, retries=2, backoff=2):\n",
        "    server = server or AZURE_SQL_SERVER\n",
        "    database = database or AZURE_SQL_DB\n",
        "    if not server or not database:\n",
        "        raise RuntimeError('AZURE_SQL_SERVER and AZURE_SQL_DB must be set (or passed in)')\n",
        "    if not server.lower().endswith('.database.windows.net'):\n",
        "        server = server.rstrip('.') + '.database.windows.net'\n",
        "    conn_str = (\n",
        "        'Driver={ODBC Driver 18 for SQL Server};'\n",
        "        f'Server=tcp:{server},1433;'\n",
        "        f'Database={database};'\n",
        "        'Encrypt=yes;TrustServerCertificate=no;'\n",
        "    )\n",
        "    last_exc = None\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            return pyodbc.connect(conn_str, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: _token_struct()}, timeout=timeout)\n",
        "        except Exception as e:\n",
        "            last_exc = e\n",
        "            if attempt < retries:\n",
        "                time.sleep(backoff * attempt)\n",
        "            else:\n",
        "                raise\n",
        "    raise last_exc\n",
        "\n",
        "print('_pyodbc_conn_with_retry helper ready')\n",
        "\n",
        "# Open a connection/cursor if SQL source or warehouse destination is used (happy-path reuse)\n",
        "conn = None\n",
        "cur = None\n",
        "if source == 'sql' or fmt == 'warehouse':\n",
        "    assert AZURE_SQL_SERVER and AZURE_SQL_DB, 'AZURE_SQL_SERVER and AZURE_SQL_DB required for SQL source or warehouse destination'\n",
        "    conn = _pyodbc_conn_with_retry(server=AZURE_SQL_SERVER, database=AZURE_SQL_DB)\n",
        "    cur = conn.cursor()\n",
        "    print('Opened SQL connection')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Refresh â€” parquet source\n",
        "Read the canonical current parquet snapshot from the controller lakehouse Files/<dataset>current directory and overwrite the destination (Delta or Warehouse)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if update_strategy not in ('full refresh', 'full_refresh', 'fullrefresh') or source != 'lakehouse':\n",
        "    print('Skipping parquet Full Refresh cell (strategy != full refresh or source != lakehouse)')\n",
        "else:\n",
        "    assert source == 'lakehouse', 'Parquet Full Refresh requires source=lakehouse'\n",
        "    print(f\"Starting Full Refresh (parquet -> {fmt}) for test_case: {test_case_name}\")\n",
        "    t0 = time.time()\n",
        "    # Read from controller lakehouse Files/<dataset>current directory (Spark reads dir of part files)\n",
        "    df_src = spark.read.parquet(str(source_current_parquet_dir))\n",
        "    # canonicalize timestamp columns to legacy Spark TimestampType to avoid NTZ issues\n",
        "    df_src = canonicalize_timestamps(df_src)\n",
        "    src_count = df_src.count()\n",
        "    print('Read source_current rows=', src_count)\n",
        "\n",
        "    if fmt == 'delta':\n",
        "        table_full = f\"{target_lakehouse}.{target_table}\"\n",
        "        print('Writing Delta table ->', table_full)\n",
        "        df_src.write.mode('overwrite').saveAsTable(table_full)\n",
        "    elif fmt == 'warehouse':\n",
        "        table_full = f\"{target_warehouse}.dbo.{target_table}\"\n",
        "        print('Writing Warehouse table ->', table_full)\n",
        "        # ensure nullable update_type if present\n",
        "        df_write = make_columns_nullable(df_src, columns=[c for c in df_src.columns if c == 'update_type'])\n",
        "        df_write.write.mode('overwrite').synapsesql(table_full)\n",
        "    else:\n",
        "        raise SystemExit(f'Unsupported destination format: {fmt}')\n",
        "\n",
        "    t1 = time.time()\n",
        "    ingest_time = t1 - t0\n",
        "    append_metrics('full_refresh_write', rows=src_count, ingest_time_s=ingest_time, notes_local='parquet source full refresh')\n",
        "    print(f'Full Refresh complete: rows_written={src_count} ingest_time_s={ingest_time:.2f}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Refresh â€” SQL source\n",
        "Read the canonical current table from Azure SQL and overwrite the destination. (No staging for Delta per rules; for Warehouse we write via synapsesql.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if update_strategy not in ('full refresh', 'full_refresh', 'fullrefresh') or source != 'sql':\n",
        "    print('Skipping SQL Full Refresh cell (strategy != full refresh or source != sql)')\n",
        "else:\n",
        "    assert source == 'sql', 'SQL Full Refresh requires source=sql'\n",
        "    print(f\"Starting Full Refresh (sql -> {fmt}) for test_case: {test_case_name}\")\n",
        "    t0 = time.time()\n",
        "    # Read current table from SQL into pandas then spark (happy-path)\n",
        "    table_name_sql = f\"{AZURE_SQL_SCHEMA}.current_{dataset_name}\"\n",
        "    print(f'Reading SQL table: {table_name_sql} from {AZURE_SQL_SERVER}/{AZURE_SQL_DB}')\n",
        "    pdf = pd.read_sql(f'SELECT * FROM {table_name_sql}', conn)\n",
        "    df_src = spark.createDataFrame(pdf)\n",
        "    # canonicalize timestamp columns to legacy Spark TimestampType to avoid NTZ issues\n",
        "    df_src = canonicalize_timestamps(df_src)\n",
        "    src_count = df_src.count()\n",
        "    print('Read source_current rows (from SQL)=', src_count)\n",
        "\n",
        "    if fmt == 'delta':\n",
        "        table_full = f\"{target_lakehouse}.{target_table}\"\n",
        "        print('Writing Delta table ->', table_full)\n",
        "        df_src.write.mode('overwrite').saveAsTable(table_full)\n",
        "    elif fmt == 'warehouse':\n",
        "        table_full = f\"{target_warehouse}.dbo.{target_table}\"\n",
        "        print('Writing Warehouse table ->', table_full)\n",
        "        df_write = make_columns_nullable(df_src, columns=[c for c in df_src.columns if c == 'update_type'])\n",
        "        df_write.write.mode('overwrite').synapsesql(table_full)\n",
        "    else:\n",
        "        raise SystemExit(f'Unsupported destination format: {fmt}')\n",
        "\n",
        "    t1 = time.time()\n",
        "    ingest_time = t1 - t0\n",
        "    append_metrics('full_refresh_write_sql', rows=src_count, ingest_time_s=ingest_time, notes_local='sql source full refresh')\n",
        "    print(f'Full Refresh (SQL source) complete: rows_written={src_count} ingest_time_s={ingest_time:.2f}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Compare â€” parquet source\n",
        "Compare source_current parquet to destination snapshot and append classified event rows to the destination event-log."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if update_strategy not in ('full compare', 'full_compare', 'fullcompare') or source != 'lakehouse':\n",
        "    print('Skipping parquet Full Compare cell (strategy != full compare or source != lakehouse)')\n",
        "else:\n",
        "    assert source == 'lakehouse', 'Parquet Full Compare requires source=lakehouse'\n",
        "    print(f\"Starting Full Compare (parquet -> {fmt}) for test_case: {test_case_name}\")\n",
        "    t0 = time.time()\n",
        "    df_src = spark.read.parquet(str(source_current_parquet_dir))\n",
        "    # canonicalize timestamp columns to legacy Spark TimestampType to avoid NTZ issues\n",
        "    df_src = canonicalize_timestamps(df_src)\n",
        "    print('Source rows=', df_src.count())\n",
        "\n",
        "    # Read destination snapshot depending on destination format\n",
        "    if fmt == 'delta':\n",
        "        df_dest = spark.table(f\"{target_lakehouse}.{target_table}\")\n",
        "    elif fmt == 'warehouse':\n",
        "        # read via JDBC into spark (happy-path use pandas fallback for parity with other notebooks)\n",
        "        table_sql = f\"{target_warehouse}.dbo.{target_table}\"\n",
        "        pdf_dest = pd.read_sql(f'SELECT * FROM {table_sql}', conn)\n",
        "        df_dest = spark.createDataFrame(pdf_dest)\n",
        "        df_dest = canonicalize_timestamps(df_dest)\n",
        "    else:\n",
        "        raise SystemExit(f'Unsupported destination format: {fmt}')\n",
        "    print('Destination rows=', df_dest.count())\n",
        "\n",
        "    # Classify changes: inserts, updates, deletes, anomalies\n",
        "    # Inserts: in source but not in dest\n",
        "    inserts = df_src.join(df_dest.select('id'), on='id', how='left_anti').withColumn('update_type', lit('insert'))\n",
        "    # Deletes: in dest but not in source -> append dest row but mark delete\n",
        "    deletes = df_dest.join(df_src.select('id'), on='id', how='left_anti').withColumn('update_type', lit('delete'))\n",
        "    # Potential updates: in both\n",
        "    both = df_src.alias('s').join(df_dest.alias('d'), on='id', how='inner')\n",
        "    updates = both.filter(col('s.ts_1') > col('d.ts_1'))\n",
        "    updates = updates.select('s.*').withColumn('update_type', lit('update'))\n",
        "    anomalies = both.filter(col('d.ts_1') > col('s.ts_1'))\n",
        "    anomaly_count = anomalies.count()\n",
        "    if anomaly_count > 0:\n",
        "        print(f'WARNING: anomalies detected where destination.ts_1 > source.ts_1: count={anomaly_count}')\n",
        "\n",
        "    # Build events DF: append inserts + updates + deletes (make sure schemas align)\n",
        "    # Ensure delete rows have same columns as source: use dest row (we set update_type above)\n",
        "    events = inserts.unionByName(updates, allowMissingColumns=True).unionByName(deletes, allowMissingColumns=True)\n",
        "    events_count = events.count()\n",
        "    print('Total events to append=', events_count, '| inserts=', inserts.count(), 'updates=', updates.count(), 'deletes=', deletes.count())\n",
        "\n",
        "    # Write events to destination event-log\n",
        "    if fmt == 'delta':\n",
        "        events.write.mode('append').saveAsTable(f\"{target_lakehouse}.{target_table}\")\n",
        "    else:\n",
        "        events_write = make_columns_nullable(events, columns=[c for c in events.columns if c == 'update_type'])\n",
        "        events_write.write.mode('append').synapsesql(f\"{target_warehouse}.dbo.{target_table}\")\n",
        "\n",
        "    t1 = time.time()\n",
        "    ingest_time = t1 - t0\n",
        "    append_metrics('full_compare_append', rows=events_count, ingest_time_s=ingest_time, notes_local='parquet source full compare')\n",
        "    print('Full Compare parquet complete: events_appended=', events_count, 'ingest_time_s=', ingest_time)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Compare â€” SQL source\n",
        "When destination is Delta: do Spark compare (read via pandas->spark). When destination is Warehouse: stage source to staging table and run server-side classification (single server-side T-SQL set of INSERTs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if update_strategy not in ('full compare', 'full_compare', 'fullcompare') or source != 'sql':\n",
        "    print('Skipping SQL Full Compare cell (strategy != full compare or source != sql)')\n",
        "else:\n",
        "    assert source == 'sql', 'SQL Full Compare requires source=sql'\n",
        "    print(f\"Starting Full Compare (sql -> {fmt}) for test_case: {test_case_name}\")\n",
        "    # Read source_current from SQL into pandas then spark\n",
        "    t0 = time.time()\n",
        "    table_name_sql = f\"{AZURE_SQL_SCHEMA}.current_{dataset_name}\"\n",
        "    print(f'Reading SQL table: {table_name_sql}')\n",
        "    pdf_src = pd.read_sql(f'SELECT * FROM {table_name_sql}', conn)\n",
        "    df_src = spark.createDataFrame(pdf_src)\n",
        "    # canonicalize timestamp columns to legacy Spark TimestampType to avoid NTZ issues\n",
        "    df_src = canonicalize_timestamps(df_src)\n",
        "    print('Source rows=', df_src.count())\n",
        "\n",
        "    if fmt == 'delta':\n",
        "        # read destination snapshot\n",
        "        df_dest = spark.table(f\"{target_lakehouse}.{target_table}\")\n",
        "        print('Destination rows=', df_dest.count())\n",
        "        both = df_src.alias('s').join(df_dest.alias('d'), on='id', how='inner')\n",
        "        inserts = df_src.join(df_dest.select('id'), on='id', how='left_anti').withColumn('update_type', lit('insert'))\n",
        "        deletes = df_dest.join(df_src.select('id'), on='id', how='left_anti').withColumn('update_type', lit('delete'))\n",
        "        updates = both.filter(col('s.ts_1') > col('d.ts_1')).select('s.*').withColumn('update_type', lit('update'))\n",
        "        anomalies = both.filter(col('d.ts_1') > col('s.ts_1'))\n",
        "        anomaly_count = anomalies.count()\n",
        "        if anomaly_count > 0:\n",
        "            print(f'WARNING: anomalies detected where dest.ts_1 > src.ts_1: count={anomaly_count}')\n",
        "        events = inserts.unionByName(updates, allowMissingColumns=True).unionByName(deletes, allowMissingColumns=True)\n",
        "        events_count = events.count()\n",
        "        events.write.mode('append').saveAsTable(f\"{target_lakehouse}.{target_table}\")\n",
        "        t1 = time.time()\n",
        "        append_metrics('full_compare_sql_to_delta', rows=events_count, ingest_time_s=(t1-t0), notes_local='sql source -> delta full compare')\n",
        "        print('Full Compare SQL->Delta complete: events_appended=', events_count)\n",
        "\n",
        "    elif fmt == 'warehouse':\n",
        "        # Stage source into warehouse staging table then run server-side T-SQL to classify and append events\n",
        "        print('Staging source to warehouse staging table ->', staging_table)\n",
        "        # overwrite staging with df_src\n",
        "        df_src_write = make_columns_nullable(df_src, columns=[c for c in df_src.columns])\n",
        "        df_src_write.write.mode('overwrite').synapsesql(staging_table)\n",
        "        staged_count = df_src.count()\n",
        "        print('Staged rows=', staged_count)\n",
        "\n",
        "        # Build column lists for dynamic T-SQL (happy-path: rely on df_src.columns)\n",
        "        cols = df_src.columns\n",
        "        col_list = ', '.join(f'[{c}]' for c in cols)\n",
        "        select_s_cols = ', '.join(f's.[{c}]' for c in cols)\n",
        "        select_t_cols = ', '.join(f't.[{c}]' for c in cols)\n",
        "\n",
        "        # Compose T-SQL statements to INSERT classified rows into target event-log table\n",
        "        target_full = f\"{target_warehouse}.dbo.{target_table}\"\n",
        "        sql_statements = []\n",
        "        # Inserts: in staging not in target\n",
        "        sql_statements.append(f\"INSERT INTO {target_full} ({col_list}, [update_type]) SELECT {select_s_cols}, 'insert' FROM {staging_table} s LEFT JOIN {target_full} t ON s.id = t.id WHERE t.id IS NULL;\")\n",
        "        # Updates: in both and staging.ts_1 > target.ts_1\n",
        "        sql_statements.append(f\"INSERT INTO {target_full} ({col_list}, [update_type]) SELECT {select_s_cols}, 'update' FROM {staging_table} s JOIN {target_full} t ON s.id = t.id WHERE s.ts_1 > t.ts_1;\")\n",
        "        # Deletes: in target not in staging -> append target row with update_type='delete'\n",
        "        sql_statements.append(f\"INSERT INTO {target_full} ({col_list}, [update_type]) SELECT {select_t_cols}, 'delete' FROM {target_full} t LEFT JOIN {staging_table} s ON t.id = s.id WHERE s.id IS NULL;\")\n",
        "        # Anomaly count (dest newer than source)\n",
        "        sql_statements.append(f\"SELECT COUNT(1) AS anomaly_count FROM {target_full} t JOIN {staging_table} s ON t.id = s.id WHERE t.ts_1 > s.ts_1;\")\n",
        "\n",
        "        # Execute the statements sequentially and capture anomaly count from the final SELECT\n",
        "        print('Executing server-side classification SQL...')\n",
        "        for i, stmt in enumerate(sql_statements):\n",
        "            cur.execute(stmt)\n",
        "            # If this was the final SELECT, fetch anomaly_count\n",
        "            if stmt.strip().upper().startswith('SELECT'):\n",
        "                row = cur.fetchone()\n",
        "                anomaly_count = row[0] if row else 0\n",
        "                if anomaly_count > 0:\n",
        "                    print(f'WARNING: anomalies detected (dest.ts_1 > src.ts_1): {anomaly_count}')\n",
        "        conn.commit()\n",
        "\n",
        "        # Cleanup staging\n",
        "        print('Dropping staging table:', staging_table)\n",
        "        cur.execute(f'DROP TABLE IF EXISTS {staging_table};')\n",
        "        conn.commit()\n",
        "\n",
        "        t1 = time.time()\n",
        "        ingest_time = t1 - t0\n",
        "        # For metrics we approximate rows appended as staged_count (server statements appended rows internally)\n",
        "        append_metrics('full_compare_sql_to_warehouse', rows=staged_count, ingest_time_s=ingest_time, notes_local='sql source -> warehouse full compare (server-side)')\n",
        "        print('Full Compare SQL->Warehouse complete: staged_rows=', staged_count)\n",
        "\n",
        "    else:\n",
        "        raise SystemExit(f'Unsupported destination format: {fmt}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Incremental â€” append updates\n",
        "Append the updates snapshot from source (parquet or SQL updates table) to the destination event-log table. No compare, simple append."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if update_strategy not in ('incremental', 'increment', 'incremental_update') or source not in ('lakehouse','sql'):\n",
        "    print('Skipping Incremental cell (strategy != incremental or unsupported source)')\n",
        "else:\n",
        "    print(f\"Starting Incremental append ({source} -> {fmt}) for test_case: {test_case_name}\")\n",
        "    t0 = time.time()\n",
        "    if source == 'lakehouse':\n",
        "        # read updates parquet from controller lakehouse Files/<dataset>updates/updates.parquet\n",
        "        df_updates = spark.read.parquet(str(source_updates_parquet))\n",
        "        # canonicalize timestamp columns to legacy Spark TimestampType to avoid NTZ issues\n",
        "        df_updates = canonicalize_timestamps(df_updates)\n",
        "    elif source == 'sql':\n",
        "        table_updates = f\"{AZURE_SQL_SCHEMA}.updates_{dataset_name}\"\n",
        "        pdf_upd = pd.read_sql(f'SELECT * FROM {table_updates}', conn)\n",
        "        df_updates = spark.createDataFrame(pdf_upd)\n",
        "        df_updates = canonicalize_timestamps(df_updates)\n",
        "    else:\n",
        "        raise SystemExit(f'Unsupported source for incremental: {source}')\n",
        "\n",
        "    upd_count = df_updates.count()\n",
        "    print('Updates rows to append=', upd_count)\n",
        "\n",
        "    if fmt == 'delta':\n",
        "        df_updates.write.mode('append').saveAsTable(f\"{target_lakehouse}.{target_table}\")\n",
        "    elif fmt == 'warehouse':\n",
        "        df_updates_write = make_columns_nullable(df_updates, columns=[c for c in df_updates.columns if c == 'update_type'])\n",
        "        df_updates_write.write.mode('append').synapsesql(f\"{target_warehouse}.dbo.{target_table}\")\n",
        "    else:\n",
        "        raise SystemExit(f'Unsupported destination format: {fmt}')\n",
        "\n",
        "    t1 = time.time()\n",
        "    ingest_time = t1 - t0\n",
        "    append_metrics('incremental_append', rows=upd_count, ingest_time_s=ingest_time, notes_local='incremental append')\n",
        "    print(f'Incremental append complete: rows_appended={upd_count} ingest_time_s={ingest_time:.2f}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup & summary\n",
        "Drop staging if left behind and print the run summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('\\nRun summary:')\n",
        "print(f' test_case: {test_case_name} | dataset: {dataset_name} | source: {source} | format: {fmt} | strategy: {update_strategy}')\n",
        "\n",
        "# Ensure staging dropped if present (happy-path, run only if staging exists)\n",
        "if conn is not None and cur is not None:\n",
        "    try:\n",
        "        cur.execute(f\"DROP TABLE IF EXISTS {staging_table};\")\n",
        "        conn.commit()\n",
        "        print('Ensured staging table dropped:', staging_table)\n",
        "    except Exception:\n",
        "        print('Staging drop may have failed or staging not used (continuing)')\n",
        "    finally:\n",
        "        cur.close()\n",
        "        conn.close()\n",
        "        print('Closed SQL connection')\n",
        "\n",
        "print('Apply updates run complete.')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# ðŸ““ 3. Apply Updates\n",
    "## Update Benchmarking for TC.03.xâ€“TC.08.x\n",
    "\n",
    "This notebook simulates and benchmarks update strategies for both Delta and Warehouse tables:\n",
    "- TC.03.x/TC.04.x: Full Refresh (replace table with latest state)\n",
    "- TC.05.x/TC.06.x: Full Compare (append event history for insert/update/delete)\n",
    "- TC.07.x/TC.08.x: Incremental Update (append event history from update slice)\n",
    "\n",
    "Metrics are logged for each test case. All event log tables use `update_type` ('insert', 'update', 'delete')."
   ]
  },
  {
   "cell_type": "code",
   "id": "setup",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import col, lit"
   ]
  },
  {
   "cell_type": "code",
   "id": "params",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Table names and paths\n",
    "target_lakehouse = \"BenchmarkLakehouse\"\n",
    "target_warehouse = \"BenchmarkWarehouse\"\n",
    "\n",
    "row_count = 10000  # Change as needed\n",
    "\n",
    "base_file = f\"abfss://FabricBenchmarking@onelake.dfs.fabric.microsoft.com/DataSourceLakehouse.Lakehouse/Files/base/base_{row_count}_parquet.parquet\"\n",
    "updates_file = f\"abfss://FabricBenchmarking@onelake.dfs.fabric.microsoft.com/DataSourceLakehouse.Lakehouse/Files/updates/updates_{row_count}_parquet.parquet\"\n",
    "\n",
    "delta_tables = {\n",
    "    \"refresh\": \"delta_refresh_load\",\n",
    "    \"compare\": \"delta_compare_load\",\n",
    "    \"increment\": \"delta_increment_load\"\n",
    "}\n",
    "warehouse_tables = {\n",
    "    \"refresh\": \"wh_table_refresh_load\",\n",
    "    \"compare\": \"wh_table_compare_load\",\n",
    "    \"increment\": \"wh_table_increment_load\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "id": "metrics_schema",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Metrics schema (already exists in BenchmarkLakehouse)\n",
    "metrics_schema = StructType([\n",
    "    StructField(\"test_case_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"format\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"rows\", IntegerType(), True),\n",
    "    StructField(\"update_strategy\", StringType(), True),\n",
    "    StructField(\"ingest_time_s\", FloatType(), True),\n",
    "    StructField(\"spinup_time_s\", FloatType(), True),\n",
    "    StructField(\"storage_size_mb\", FloatType(), True),\n",
    "    StructField(\"query_type\", StringType(), True),\n",
    "    StructField(\"query_time_s\", FloatType(), True),\n",
    "    StructField(\"cu_used\", FloatType(), True),\n",
    "    StructField(\"notes\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc03_title",
   "metadata": {},
   "source": [
    "## TC.03.x: Full Refresh â€” Delta Table\n",
    "\n",
    "Simulates a full refresh by applying all updates to base data and replacing the table with the latest state."
   ]
  },
  {
   "cell_type": "code",
   "id": "tc03",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load base and updates\n",
    "base_df = spark.read.format(\"parquet\").load(base_file)\n",
    "updates_df = spark.read.format(\"parquet\").load(updates_file)\n",
    "\n",
    "# Apply updates to base (simulate latest state)\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "df_all = base_df.unionByName(updates_df, allowMissingColumns=True)\n",
    "\n",
    "# Latest record per id (for insert/update, skip deletes)\n",
    "window = Window.partitionBy(\"id\").orderBy(col(\"ts_1\").desc())\n",
    "df_all = df_all.withColumn(\"rn\", row_number().over(window))\n",
    "df_current = df_all.filter((col(\"update_type\") != \"delete\") & (col(\"rn\") == 1)).drop(\"rn\")\n",
    "\n",
    "# Timer start\n",
    "tc03_start = time.time()\n",
    "df_current.write.mode(\"overwrite\").saveAsTable(f\"{target_lakehouse}.{delta_tables['refresh']}\")\n",
    "tc03_end = time.time()\n",
    "tc03_ingest_time = tc03_end - tc03_start\n",
    "\n",
    "# Storage size\n",
    "try:\n",
    "    import mssparkutils\n",
    "    table_path = f\"/lakehouse/{target_lakehouse}/Tables/{delta_tables['refresh']}\"\n",
    "    storage_files = mssparkutils.fs.ls(table_path)\n",
    "    storage_size_mb = sum(f.size for f in storage_files) / (1024 * 1024)\n",
    "except Exception:\n",
    "    storage_size_mb = float('nan')\n",
    "\n",
    "# Metrics\n",
    "metrics_tc03 = [\n",
    "    (\n",
    "        \"TC.03.x\",\n",
    "        datetime.now(),\n",
    "        \"Delta\",\n",
    "        \"Tables\",\n",
    "        df_current.count(),\n",
    "        \"Full Refresh\",\n",
    "        tc03_ingest_time,\n",
    "        float('nan'),\n",
    "        storage_size_mb,\n",
    "        \"N/A\",\n",
    "        float('nan'),\n",
    "        float('nan'),\n",
    "        \"Full refresh with applied updates\"\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_tc03, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')\n",
    "print(f\"TC.03.x (Delta Full Refresh) complete | Ingest time: {tc03_ingest_time:.2f}s | Storage: {storage_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc04_title",
   "metadata": {},
   "source": [
    "## TC.04.x: Full Refresh â€” Warehouse Table\n",
    "\n",
    "Simulates a full refresh by replacing the warehouse table with the latest state."
   ]
  },
  {
   "cell_type": "code",
   "id": "tc04",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from com.microsoft.spark.fabric import Constants\n",
    "\n",
    "# Cast timestamp_ntz columns for warehouse compatibility\n",
    "for c in df_current.columns:\n",
    "    if dict(df_current.dtypes)[c] == \"timestamp_ntz\":\n",
    "        df_current = df_current.withColumn(c, col(c).cast(\"timestamp\"))\n",
    "\n",
    "tc04_start = time.time()\n",
    "df_current.write.mode(\"overwrite\").synapsesql(f\"{target_warehouse}.dbo.{warehouse_tables['refresh']}\")\n",
    "tc04_end = time.time()\n",
    "tc04_ingest_time = tc04_end - tc04_start\n",
    "\n",
    "# Metrics\n",
    "metrics_tc04 = [\n",
    "    (\n",
    "        \"TC.04.x\",\n",
    "        datetime.now(),\n",
    "        \"Warehouse\",\n",
    "        \"Tables\",\n",
    "        df_current.count(),\n",
    "        \"Full Refresh\",\n",
    "        tc04_ingest_time,\n",
    "        float('nan'),\n",
    "        float('nan'),\n",
    "        \"N/A\",\n",
    "        float('nan'),\n",
    "        float('nan'),\n",
    "        \"Full refresh with applied updates\"\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_tc04, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')\n",
    "print(f\"TC.04.x (Warehouse Full Refresh) complete | Ingest time: {tc04_ingest_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc05_title",
   "metadata": {},
   "source": [
    "## TC.05.x: Full Compare â€” Delta Table\n",
    "\n",
    "Compares current data to existing event log, appends insert/update/delete history."
   ]
  },
  {
   "cell_type": "code",
   "id": "tc05",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For benchmarking, use updates_df as batch of changes for event log\n",
    "tc05_start = time.time()\n",
    "updates_df.write.mode(\"append\").saveAsTable(f\"{target_lakehouse}.{delta_tables['compare']}\")\n",
    "tc05_end = time.time()\n",
    "tc05_ingest_time = tc05_end - tc05_start\n",
    "\n",
    "try:\n",
    "    import mssparkutils\n",
    "    table_path = f\"/lakehouse/{target_lakehouse}/Tables/{delta_tables['compare']}\"\n",
    "    storage_files = mssparkutils.fs.ls(table_path)\n",
    "    storage_size_mb = sum(f.size for f in storage_files) / (1024 * 1024)\n",
    "except Exception:\n",
    "    storage_size_mb = float('nan')\n",
    "\n",
    "metrics_tc05 = [\n",
    "    (\n",
    "        \"TC.05.x\",\n",
    "        datetime.now(),\n",
    "        \"Delta\",\n",
    "        \"Tables\",\n",
    "        updates_df.count(),\n",
    "        \"Full Compare\",\n",
    "        tc05_ingest_time,\n",
    "        float('nan'),\n",
    "        storage_size_mb,\n",
    "        \"N/A\",\n",
    "        float('nan'),\n",
    "        float('nan'),\n",
    "        \"Full compare: append events to event log\"\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_tc05, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')\n",
    "print(f\"TC.05.x (Delta Compare) complete | Ingest time: {tc05_ingest_time:.2f}s | Storage: {storage_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc06_title",
   "metadata": {},
   "source": [
    "## TC.06.x: Full Compare â€” Warehouse Table\n",
    "\n",
    "Appends event history to warehouse event log table."
   ]
  },
  {
   "cell_type": "code",
   "id": "tc06",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cast timestamp_ntz columns for warehouse compatibility\n",
    "for c in updates_df.columns:\n",
    "    if dict(updates_df.dtypes)[c] == \"timestamp_ntz\":\n",
    "        updates_df = updates_df.withColumn(c, col(c).cast(\"timestamp\"))\n",
    "\n",
    "tc06_start = time.time()\n",
    "updates_df.write.mode(\"append\").synapsesql(f\"{target_warehouse}.dbo.{warehouse_tables['compare']}\")\n",
    "tc06_end = time.time()\n",
    "tc06_ingest_time = tc06_end - tc06_start\n",
    "\n",
    "metrics_tc06 = [\n",
    "    (\n",
    "        \"TC.06.x\",\n",
    "        datetime.now(),\n",
    "        \"Warehouse\",\n",
    "        \"Tables\",\n",
    "        updates_df.count(),\n",
    "        \"Full Compare\",\n",
    "        tc06_ingest_time,\n",
    "        float('nan'),\n",
    "        float('nan'),\n",
    "        \"N/A\",\n",
    "        float('nan'),\n",
    "        float('nan'),\n",
    "        \"Full compare: append events to event log\"\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_tc06, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')\n",
    "print(f\"TC.06.x (Warehouse Compare) complete | Ingest time: {tc06_ingest_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc07_title",
   "metadata": {},
   "source": [
    "## TC.07.x: Incremental Update â€” Delta Table\n",
    "\n",
    "Appends all update events to the incremental Delta event log table."
   ]
  },
  {
   "cell_type": "code",
   "id": "tc07",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "tc07_start = time.time()\n",
    "updates_df.write.mode(\"append\").saveAsTable(f\"{target_lakehouse}.{delta_tables['increment']}\")\n",
    "tc07_end = time.time()\n",
    "tc07_ingest_time = tc07_end - tc07_start\n",
    "\n",
    "try:\n",
    "    import mssparkutils\n",
    "    table_path = f\"/lakehouse/{target_lakehouse}/Tables/{delta_tables['increment']}\"\n",
    "    storage_files = mssparkutils.fs.ls(table_path)\n",
    "    storage_size_mb = sum(f.size for f in storage_files) / (1024 * 1024)\n",
    "except Exception:\n",
    "    storage_size_mb = float('nan')\n",
    "\n",
    "metrics_tc07 = [\n",
    "    (\n",
    "        \"TC.07.x\",\n",
    "        datetime.now(),\n",
    "        \"Delta\",\n",
    "        \"Tables\",\n",
    "        updates_df.count(),\n",
    "        \"Incremental\",\n",
    "        tc07_ingest_time,\n",
    "        float('nan'),\n",
    "        storage_size_mb,\n",
    "        \"N/A\",\n",
    "        float('nan'),\n",
    "        float('nan'),\n",
    "        \"Incremental: append events to event log\"\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_tc07, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')\n",
    "print(f\"TC.07.x (Delta Incremental) complete | Ingest time: {tc07_ingest_time:.2f}s | Storage: {storage_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc08_title",
   "metadata": {},
   "source": [
    "## TC.08.x: Incremental Update â€” Warehouse Table\n",
    "\n",
    "Appends all update events to the incremental warehouse event log table."
   ]
  },
  {
   "cell_type": "code",
   "id": "tc08",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cast timestamp_ntz columns for warehouse compatibility\n",
    "for c in updates_df.columns:\n",
    "    if dict(updates_df.dtypes)[c] == \"timestamp_ntz\":\n",
    "        updates_df = updates_df.withColumn(c, col(c).cast(\"timestamp\"))\n",
    "\n",
    "tc08_start = time.time()\n",
    "updates_df.write.mode(\"append\").synapsesql(f\"{target_warehouse}.dbo.{warehouse_tables['increment']}\")\n",
    "tc08_end = time.time()\n",
    "tc08_ingest_time = tc08_end - tc08_start\n",
    "\n",
    "metrics_tc08 = [\n",
    "    (\n",
    "        \"TC.08.x\",\n",
    "        datetime.now(),\n",
    "        \"Warehouse\",\n",
    "        \"Tables\",\n",
    "        updates_df.count(),\n",
    "        \"Incremental\",\n",
    "        tc08_ingest_time,\n",
    "        float('nan'),\n",
    "        float('nan'),\n",
    "        \"N/A\",\n",
    "        float('nan'),\n",
    "        float('nan'),\n",
    "        \"Incremental: append events to event log\"\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_tc08, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')\n",
    "print(f\"TC.08.x (Warehouse Incremental) complete | Ingest time: {tc08_ingest_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "completion",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Completion\")\n",
    "print(\"Updates applied and metrics logged for TC.03.x through TC.08.x.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "synapse_pyspark",
   "display_name": "Synapse PySpark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

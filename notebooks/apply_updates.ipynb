{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# ðŸ““ 3. Apply Updates\n",
    "## Update Benchmarking for TC.03.xâ€“TC.08.x\n",
    "### Ensure `DataSourceLakehouse` & `BenchmarkLakehouse` are connected as data sources before running.\n",
    "\n",
    "This notebook simulates and benchmarks update strategies for both Delta and Warehouse tables:\n",
    "- TC.03.x/TC.04.x: Full Refresh (replace table with latest state)\n",
    "- TC.05.x/TC.06.x: Full Compare (append event history for insert/update/delete)\n",
    "- TC.07.x/TC.08.x: Incremental Update (append event history from update slice)\n",
    "\n",
    "Metrics are logged for each test case. All event log tables use `update_type` ('insert', 'update', 'delete').\n",
    "\n",
    "Notes: storage_size_mb and cu_used are used as integer row-count proxies (not MB/CU) per the lightweight proxy approach."
   ]
  },
  {
   "cell_type": "code",
   "id": "setup",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import col, lit"
   ]
  },
  {
   "cell_type": "code",
   "id": "make_nullable_helper",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Helper function to make columns nullable in a Spark DataFrame\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "\n",
    "def make_columns_nullable(df, columns=None):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame where the specified columns (or all columns if None) are set to nullable=True in the schema.\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = [f.name for f in df.schema.fields]\n",
    "    new_schema = StructType([\n",
    "        StructField(f.name, f.dataType, True) if f.name in columns else f\n",
    "        for f in df.schema.fields\n",
    "    ])\n",
    "    return df.sparkSession.createDataFrame(df.rdd, schema=new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "id": "params",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Table names and paths\n",
    "target_lakehouse = \"BenchmarkLakehouse\"\n",
    "target_warehouse = \"BenchmarkWarehouse\"\n",
    "\n",
    "row_count = 10000  # Change as needed\n",
    "\n",
    "base_file = f\"abfss://FabricBenchmarking@onelake.dfs.fabric.microsoft.com/DataSourceLakehouse.Lakehouse/Files/base/base_{row_count}_parquet.parquet\"\n",
    "updates_file = f\"abfss://FabricBenchmarking@onelake.dfs.fabric.microsoft.com/DataSourceLakehouse.Lakehouse/Files/updates/updates_{row_count}_parquet.parquet\"\n",
    "\n",
    "delta_tables = {\n",
    "    \"refresh\": \"delta_refresh_load\",\n",
    "    \"compare\": \"delta_compare_load\",\n",
    "    \"increment\": \"delta_increment_load\"\n",
    "}\n",
    "warehouse_tables = {\n",
    "    \"refresh\": \"wh_table_refresh_load\",\n",
    "    \"compare\": \"wh_table_compare_load\",\n",
    "    \"increment\": \"wh_table_increment_load\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "id": "metrics_schema",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Metrics schema (storage_size_mb and cu_used are integer row-count proxies)\n",
    "metrics_schema = StructType([\n",
    "    StructField(\"test_case_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"format\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"rows\", IntegerType(), True),\n",
    "    StructField(\"update_strategy\", StringType(), True),\n",
    "    StructField(\"ingest_time_s\", FloatType(), True),\n",
    "    StructField(\"spinup_time_s\", FloatType(), True),\n",
    "    StructField(\"storage_size_mb\", IntegerType(), True),\n",
    "    StructField(\"query_type\", StringType(), True),\n",
    "    StructField(\"query_time_s\", FloatType(), True),\n",
    "    StructField(\"cu_used\", IntegerType(), True),\n",
    "    StructField(\"notes\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc03_title",
   "metadata": {},
   "source": [
    "## TC.03.x: Full Refresh â€” Delta Table\n",
    "\n",
    "Simulates a full refresh by applying all updates to base data and replacing the table with the latest state."
   ]
  },
  {
   "cell_type": "code",
   "id": "tc03",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load base and updates using spark.read.parquet for correct timestamp type\n",
    "base_df = spark.read.parquet(base_file)\n",
    "updates_df = spark.read.parquet(updates_file)\n",
    "\n",
    "# Apply updates to base (simulate latest state)\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "df_all = base_df.unionByName(updates_df, allowMissingColumns=True)\n",
    "\n",
    "# Latest record per id (for insert/update, skip deletes)\n",
    "window = Window.partitionBy(\"id\").orderBy(col(\"ts_1\").desc())\n",
    "df_all = df_all.withColumn(\"rn\", row_number().over(window))\n",
    "df_current = df_all.filter((col(\"update_type\") != \"delete\") & (col(\"rn\") == 1)).drop(\"rn\")\n",
    "\n",
    "# For full refresh, set all update_type to 'insert' (best practice for snapshot)\n",
    "df_current = df_current.withColumn(\"update_type\", lit(\"insert\"))\n",
    "\n",
    "# Timer start\n",
    "tc03_start = time.time()\n",
    "df_current.write.mode(\"overwrite\").saveAsTable(f\"{target_lakehouse}.{delta_tables['refresh']}\")\n",
    "tc03_end = time.time()\n",
    "tc03_ingest_time = tc03_end - tc03_start\n",
    "\n",
    "# Compute counts once (avoid repeated .count())\n",
    "base_count = base_df.count()\n",
    "updates_count = updates_df.count()\n",
    "final_count = df_current.count()\n",
    "\n",
    "# Storage size (row-count proxy)\n",
    "try:\n",
    "    import mssparkutils\n",
    "    table_path = f\"/lakehouse/{target_lakehouse}/Tables/{delta_tables['refresh']}\"\n",
    "    storage_files = mssparkutils.fs.ls(table_path)\n",
    "    storage_size_mb = int(sum(f.size for f in storage_files) / (1024 * 1024))\n",
    "except Exception:\n",
    "    storage_size_mb = int(final_count)  # use row count in lieu of storage size\n",
    "\n",
    "# Metrics: cu_used = source rows scanned + rows written (per your preference)\n",
    "metrics_tc03 = [\n",
    "    (\n",
    "        \"TC.03.x\",\n",
    "        datetime.now(),\n",
    "        \"Delta\",\n",
    "        \"Tables\",\n",
    "        int(final_count),\n",
    "        \"Full Refresh\",\n",
    "        tc03_ingest_time,\n",
    "        None,\n",
    "        int(storage_size_mb),\n",
    "        \"N/A\",\n",
    "        None,\n",
    "        int(row_count + final_count),  # rows processed proxy (source scanned + rows written)\n",
    "        \"Full refresh with applied updates\"\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_tc03, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')\n",
    "print(f\"TC.03.x (Delta Full Refresh) complete | Ingest time: {tc03_ingest_time:.2f}s | Storage(rows proxy): {storage_size_mb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc04_title",
   "metadata": {},
   "source": [
    "## TC.04.x: Full Refresh â€” Warehouse Table\n",
    "\n",
    "Simulates a full refresh by replacing the warehouse table with the latest state."
   ]
  },
  {
   "cell_type": "code",
   "id": "tc04",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from com.microsoft.spark.fabric import Constants\n",
    "\n",
    "# For full refresh, set all update_type to 'insert'\n",
    "df_current = df_current.withColumn(\"update_type\", lit(\"insert\"))\n",
    "\n",
    "# Ensure update_type is nullable to match warehouse schema\n",
    "df_current = make_columns_nullable(df_current, columns=[\"update_type\"])\n",
    "\n",
    "tc04_start = time.time()\n",
    "df_current.write.mode(\"overwrite\").synapsesql(f\"{target_warehouse}.dbo.{warehouse_tables['refresh']}\")\n",
    "tc04_end = time.time()\n",
    "tc04_ingest_time = tc04_end - tc04_start\n",
    "\n",
    "# compute counts once\n",
    "base_count = base_df.count()\n",
    "final_count = df_current.count()\n",
    "\n",
    "# storage proxy = final row count\n",
    "storage_size_mb = int(final_count)\n",
    "\n",
    "# Metrics\n",
    "metrics_tc04 = [\n",
    "    (\n",
    "        \"TC.04.x\",\n",
    "        datetime.now(),\n",
    "        \"Warehouse\",\n",
    "        \"Tables\",\n",
    "        int(final_count),\n",
    "        \"Full Refresh\",\n",
    "        tc04_ingest_time,\n",
    "        None,\n",
    "        int(storage_size_mb),\n",
    "        \"N/A\",\n",
    "        None,\n",
    "        int(row_count + final_count),  # rows processed proxy\n",
    "        \"Full refresh with applied updates\"\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_tc04, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')\n",
    "print(f\"TC.04.x (Warehouse Full Refresh) complete | Ingest time: {tc04_ingest_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc05_title",
   "metadata": {},
   "source": [
    "## TC.05.x: Full Compare â€” Delta Table\n",
    "\n",
    "Compares current data to existing event log, appends insert/update/delete history."
   ]
  },
  {
   "cell_type": "code",
   "id": "tc05",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For benchmarking, use updates_df as batch of changes for event log\n",
    "tc05_start = time.time()\n",
    "updates_df.write.mode(\"append\").saveAsTable(f\"{target_lakehouse}.{delta_tables['compare']}\")\n",
    "tc05_end = time.time()\n",
    "tc05_ingest_time = tc05_end - tc05_start\n",
    "\n",
    "# compute counts once\n",
    "base_count = base_df.count()\n",
    "updates_count = updates_df.count()\n",
    "\n",
    "try:\n",
    "    import mssparkutils\n",
    "    table_path = f\"/lakehouse/{target_lakehouse}/Tables/{delta_tables['compare']}\"\n",
    "    storage_files = mssparkutils.fs.ls(table_path)\n",
    "    storage_size_mb = int(sum(f.size for f in storage_files) / (1024 * 1024))\n",
    "except Exception:\n",
    "    storage_size_mb = int(row_count + df_current.count())  # use row count in lieu of storage size\n",
    "\n",
    "# cu_used: treating updates/deletes as inserts -> base_count + updates_count\n",
    "cu_used = int(row_count + updates_count)\n",
    "\n",
    "# preferred: final event-log table count if accessible\n",
    "try:\n",
    "    final_compare_count = spark.table(f\"{target_lakehouse}.{delta_tables['compare']}\").count()\n",
    "except Exception:\n",
    "    final_compare_count = row_count + updates_count\n",
    "\n",
    "metrics_tc05 = [\n",
    "    (\n",
    "        \"TC.05.x\",\n",
    "        datetime.now(),\n",
    "        \"Delta\",\n",
    "        \"Tables\",\n",
    "        int(updates_count),\n",
    "        \"Full Compare\",\n",
    "        tc05_ingest_time,\n",
    "        None,\n",
    "        int(final_compare_count),\n",
    "        \"N/A\",\n",
    "        None,\n",
    "        cu_used,\n",
    "        \"Full compare: append events to event log\"\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_tc05, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')\n",
    "print(f\"TC.05.x (Delta Compare) complete | Ingest time: {tc05_ingest_time:.2f}s | Storage(rows proxy): {final_compare_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc06_title",
   "metadata": {},
   "source": [
    "## TC.06.x: Full Compare â€” Warehouse Table\n",
    "\n",
    "Appends event history to warehouse event log table."
   ]
  },
  {
   "cell_type": "code",
   "id": "tc06",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Ensure update_type is nullable to match warehouse schema\n",
    "updates_df_nullable = make_columns_nullable(updates_df, columns=[\"update_type\"])\n",
    "\n",
    "tc06_start = time.time()\n",
    "updates_df_nullable.write.mode(\"append\").synapsesql(f\"{target_warehouse}.dbo.{warehouse_tables['compare']}\")\n",
    "tc06_end = time.time()\n",
    "tc06_ingest_time = tc06_end - tc06_start\n",
    "\n",
    "# compute counts once\n",
    "updates_count_nullable = updates_df_nullable.count()\n",
    "\n",
    "storage_size_mb = int(row_count + df_current.count())  # fallback row-count proxy\n",
    "\n",
    "# cu_used: treating updates/deletes as inserts -> row_count + updates_count_nullable\n",
    "cu_used = int(row_count + updates_count_nullable)\n",
    "\n",
    "try:\n",
    "    final_compare_wh_count = spark.table(f\"{target_warehouse}.dbo.{warehouse_tables['compare']}\").count()\n",
    "except Exception:\n",
    "    final_compare_wh_count = row_count + updates_count_nullable\n",
    "\n",
    "metrics_tc06 = [\n",
    "    (\n",
    "        \"TC.06.x\",\n",
    "        datetime.now(),\n",
    "        \"Warehouse\",\n",
    "        \"Tables\",\n",
    "        int(updates_count_nullable),\n",
    "        \"Full Compare\",\n",
    "        tc06_ingest_time,\n",
    "        None,\n",
    "        int(final_compare_wh_count),\n",
    "        \"N/A\",\n",
    "        None,\n",
    "        cu_used,\n",
    "        \"Full compare: append events to event log\"\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_tc06, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')\n",
    "print(f\"TC.06.x (Warehouse Compare) complete | Ingest time: {tc06_ingest_time:.2f}s | Storage(rows proxy): {final_compare_wh_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc07_title",
   "metadata": {},
   "source": [
    "## TC.07.x: Incremental Update â€” Delta Table\n",
    "\n",
    "Appends all update events to the incremental Delta event log table."
   ]
  },
  {
   "cell_type": "code",
   "id": "tc07",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "tc07_start = time.time()\n",
    "updates_df.write.mode(\"append\").saveAsTable(f\"{target_lakehouse}.{delta_tables['increment']}\")\n",
    "tc07_end = time.time()\n",
    "tc07_ingest_time = tc07_end - tc07_start\n",
    "\n",
    "# compute counts once\n",
    "updates_count = updates_df.count()\n",
    "\n",
    "try:\n",
    "    import mssparkutils\n",
    "    table_path = f\"/lakehouse/{target_lakehouse}/Tables/{delta_tables['increment']}\"\n",
    "    storage_files = mssparkutils.fs.ls(table_path)\n",
    "    storage_size_mb = int(sum(f.size for f in storage_files) / (1024 * 1024))\n",
    "except Exception:\n",
    "    storage_size_mb = int(row_count + updates_count)  # use row count in lieu of storage size\n",
    "\n",
    "metrics_tc07 = [\n",
    "    (\n",
    "        \"TC.07.x\",\n",
    "        datetime.now(),\n",
    "        \"Delta\",\n",
    "        \"Tables\",\n",
    "        int(updates_count),\n",
    "        \"Incremental\",\n",
    "        tc07_ingest_time,\n",
    "        None,\n",
    "        int(storage_size_mb),\n",
    "        \"N/A\",\n",
    "        None,\n",
    "        int(updates_count),  # use rows processed in lieu of cu_used\n",
    "        \"Incremental: append events to event log\"\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_tc07, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')\n",
    "print(f\"TC.07.x (Delta Incremental) complete | Ingest time: {tc07_ingest_time:.2f}s | Storage(rows proxy): {storage_size_mb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc08_title",
   "metadata": {},
   "source": [
    "## TC.08.x: Incremental Update â€” Warehouse Table\n",
    "\n",
    "Appends all update events to the incremental warehouse event log table."
   ]
  },
  {
   "cell_type": "code",
   "id": "tc08",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Ensure update_type is nullable to match warehouse schema\n",
    "updates_df_nullable = make_columns_nullable(updates_df, columns=[\"update_type\"])\n",
    "\n",
    "tc08_start = time.time()\n",
    "updates_df_nullable.write.mode(\"append\").synapsesql(f\"{target_warehouse}.dbo.{warehouse_tables['increment']}\")\n",
    "tc08_end = time.time()\n",
    "tc08_ingest_time = tc08_end - tc08_start\n",
    "\n",
    "updates_count_nullable = updates_df_nullable.count()\n",
    "\n",
    "try:\n",
    "    final_inc_wh_count = spark.table(f\"{target_warehouse}.dbo.{warehouse_tables['increment']}\").count()\n",
    "except Exception:\n",
    "    final_inc_wh_count = row_count + updates_count_nullable\n",
    "\n",
    "metrics_tc08 = [\n",
    "    (\n",
    "        \"TC.08.x\",\n",
    "        datetime.now(),\n",
    "        \"Warehouse\",\n",
    "        \"Tables\",\n",
    "        int(updates_count_nullable),\n",
    "        \"Incremental\",\n",
    "        tc08_ingest_time,\n",
    "        None,\n",
    "        int(final_inc_wh_count),\n",
    "        \"N/A\",\n",
    "        None,\n",
    "        int(updates_count_nullable),  # use rows processed in lieu of cu_used\n",
    "        \"Incremental: append events to event log\"\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_tc08, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')\n",
    "print(f\"TC.08.x (Warehouse Incremental) complete | Ingest time: {tc08_ingest_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "completion",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Completion\")\n",
    "print(\"Updates applied and metrics logged for TC.03.x through TC.08.x.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "synapse_pyspark",
   "display_name": "Synapse PySpark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

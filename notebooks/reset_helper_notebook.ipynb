{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# ðŸ““ Reset Helper Notebook\n",
    "\n",
    "Use this notebook to reset elements for benchmarking workflows.\n",
    "- **Cell 1:** Delete all synthetic data files from DataSourceLakehouse\n",
    "- **Cell 2:** Delete all tables from BenchmarkLakehouse\n",
    "- **Cell 3:** Delete all tables from BenchmarkWarehouse\n",
    "\n",
    "Run individual cells as needed for targeted resets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reset_synthetic_data",
   "metadata": {},
   "source": [
    "## 1. Reset Synthetic Data\n",
    "Delete all files from DataSourceLakehouse (base & updates folders)."
   ]
  },
  {
   "cell_type": "code",
   "id": "reset_data_files",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# WARNING: This deletes ALL synthetic data files from DataSourceLakehouse\n",
    "lakehouse_files = [\n",
    "    '/lakehouse/DataSourceLakehouse/Files/base/',\n",
    "    '/lakehouse/DataSourceLakehouse/Files/updates/'\n",
    "]\n",
    "try:\n",
    "    import mssparkutils\n",
    "    for folder in lakehouse_files:\n",
    "        files = mssparkutils.fs.ls(folder)\n",
    "        for f in files:\n",
    "            mssparkutils.fs.rm(f.path, recurse=True)\n",
    "    print('All synthetic data files deleted from DataSourceLakehouse.')\n",
    "except Exception as e:\n",
    "    print('Error during synthetic data reset:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reset_lakehouse_tables",
   "metadata": {},
   "source": [
    "## 2. Reset Ingestion - Lakehouse\n",
    "Delete all tables from BenchmarkLakehouse."
   ]
  },
  {
   "cell_type": "code",
   "id": "reset_lakehouse",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# WARNING: This drops ALL tables from BenchmarkLakehouse\n",
    "try:\n",
    "    tables = spark.catalog.listTables('BenchmarkLakehouse')\n",
    "    for t in tables:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS BenchmarkLakehouse.{t.name}\")\n",
    "    print('All tables dropped from BenchmarkLakehouse.')\n",
    "except Exception as e:\n",
    "    print('Error during BenchmarkLakehouse table reset:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reset_warehouse_tables",
   "metadata": {},
   "source": [
    "## 3. Reset Ingestion - Warehouse\n",
    "Delete all tables from BenchmarkWarehouse."
   ]
  },
  {
   "cell_type": "code",
   "id": "reset_warehouse",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# WARNING: This drops ALL tables from BenchmarkWarehouse\n",
    "try:\n",
    "    tables = spark.catalog.listTables('BenchmarkWarehouse')\n",
    "    for t in tables:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS BenchmarkWarehouse.{t.name}\")\n",
    "    print('All tables dropped from BenchmarkWarehouse.')\n",
    "except Exception as e:\n",
    "    print('Error during BenchmarkWarehouse table reset:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "done",
   "metadata": {},
   "source": [
    "---\n",
    "Run individual cells as needed. Use with cautionâ€”actions are destructive!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "synapse_pyspark",
   "display_name": "Synapse PySpark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "synapse_pyspark",
   "display_name": "Synapse PySpark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [
     "parameters"
    ],
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "%%configure -f\n",
    "{\n",
    "  \"conf\": {\n",
    "    \"spark.notebook.parameters\": \"{\\\"runs\\\":[{\\\"name\\\":\\\"BFF-10k-LH-to-Delta-Full-Refresh\\\",\\\"dataset_name\\\":\\\"10k\\\",\\\"source\\\":\\\"lakehouse\\\",\\\"format\\\":\\\"delta\\\",\\\"update_strategy\\\":\\\"Full Refresh\\\"},{\\\"name\\\":\\\"BFF-10k-SQL-to-WH-Full-Compare\\\",\\\"dataset_name\\\":\\\"10k\\\",\\\"source\\\":\\\"sql\\\",\\\"format\\\":\\\"warehouse\\\",\\\"update_strategy\\\":\\\"Full Compare\\\"},{\\\"name\\\":\\\"BFF-1M-LH-to-WH-Increment\\\",\\\"dataset_name\\\":\\\"1m\\\",\\\"source\\\":\\\"lakehouse\\\",\\\"format\\\":\\\"warehouse\\\",\\\"update_strategy\\\":\\\"Full Refresh\\\"}]}\"\n",
    "  }\n",
    "}\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run Benchmarks\n",
    "### ðŸ”— Wake up Azure SQL Database if SQL is source.\n",
    "\n",
    "Loop over the hard-coded parameter sets and trigger the `1.IngestData` notebook in each target workspace.\n",
    "This notebook contains a %%configure cell with the run list (hard-coded from config/test_parameter_sets.yml).\n",
    "\n",
    "Note: this runner requires a notebook-managed token (mssparkutils). It must run inside a Fabric workspace."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# Runner cell for 4.RunBenchmarks.\n",
    "# Behavior:\n",
    "# - Trigger RunNotebook jobs for each parameter-set (notebook '1.IngestData')\n",
    "# - Poll the returned job instance Location URL until terminal (including \"Completed\")\n",
    "# - Keep concise progress/status prints; comment out verbose debug prints\n",
    "# - Add the notebook displayName to the run summary entries\n",
    "#\n",
    "# NOTE: This requires notebook-managed token via mssparkutils and must run inside Fabric.\n",
    "\n",
    "import json, time\n",
    "import requests\n",
    "\n",
    "API_BASE = \"https://api.fabric.microsoft.com/v1\"\n",
    "UPLOAD_TIMEOUT = 120\n",
    "POLL_INTERVAL = 5        # seconds between polls\n",
    "POLL_TIMEOUT_SECONDS = 900\n",
    "\n",
    "# Read runs from the %%configure cell\n",
    "params_raw = None\n",
    "try:\n",
    "    params_raw = spark.conf.get('spark.notebook.parameters')\n",
    "except Exception:\n",
    "    params_raw = None\n",
    "if not params_raw:\n",
    "    raise SystemExit('spark.notebook.parameters not set. Ensure the %%configure cell is present and contains the runs list.')\n",
    "\n",
    "params = json.loads(params_raw)\n",
    "runs = params.get('runs', [])\n",
    "if not runs:\n",
    "    print('No runs found in spark.notebook.parameters.runs')\n",
    "\n",
    "# Notebook-managed token (required)\n",
    "try:\n",
    "    from notebookutils import mssparkutils\n",
    "    token = mssparkutils.credentials.getToken('https://api.fabric.microsoft.com/')\n",
    "except Exception:\n",
    "    raise SystemExit('Failed to obtain notebook-managed token via mssparkutils. This notebook must be run inside a Fabric workspace.')\n",
    "\n",
    "if not token:\n",
    "    raise SystemExit('mssparkutils returned no token. This notebook must be run inside a Fabric workspace.')\n",
    "\n",
    "headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}\n",
    "\n",
    "# Helpers\n",
    "def workspace_id_by_name(display_name):\n",
    "    wr = requests.get(f'{API_BASE}/workspaces', headers=headers, timeout=30)\n",
    "    wr.raise_for_status()\n",
    "    for w in wr.json().get('value', []):\n",
    "        if w.get('displayName') == display_name:\n",
    "            return w.get('id')\n",
    "    return None\n",
    "\n",
    "def item_id_for_notebook(workspace_id, notebook_display):\n",
    "    items_url = f'{API_BASE}/workspaces/{workspace_id}/items'\n",
    "    ir = requests.get(items_url, headers=headers, timeout=30)\n",
    "    ir.raise_for_status()\n",
    "    for it in ir.json().get('value', []):\n",
    "        if it.get('displayName') == notebook_display and it.get('type') == 'Notebook':\n",
    "            return it.get('id')\n",
    "    return None\n",
    "\n",
    "def build_exec_params(raw_params: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Convert raw params into the Fabric RunNotebook parameters shape:\n",
    "      { \"name\": {\"value\":\"<string>\", \"type\":\"string\"}, ... }\n",
    "    Complex values are JSON-encoded into string values.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for k, v in (raw_params or {}).items():\n",
    "        if v is None:\n",
    "            sval = \"\"\n",
    "        elif isinstance(v, (str, int, float, bool)):\n",
    "            sval = str(v)\n",
    "        else:\n",
    "            sval = json.dumps(v, ensure_ascii=False)\n",
    "        out[str(k)] = {\"value\": sval, \"type\": \"string\"}\n",
    "    return out\n",
    "\n",
    "def trigger_run_and_get_location(workspace_id, artifact_id, raw_param_obj):\n",
    "    exec_params = build_exec_params(raw_param_obj)\n",
    "    payload = {\n",
    "        \"executionData\": {\n",
    "            \"parameters\": exec_params,\n",
    "            \"configuration\": {}\n",
    "        }\n",
    "    }\n",
    "    run_url = f\"{API_BASE}/workspaces/{workspace_id}/items/{artifact_id}/jobs/instances?jobType=RunNotebook\"\n",
    "    # Progress print (keeps high-level trace)\n",
    "    print('  POST run_url:', run_url)\n",
    "    # Debug prints commented out for now\n",
    "    # print('  payload (truncated):', json.dumps(payload)[:1000])\n",
    "    rr = requests.post(run_url, headers=headers, json=payload, timeout=UPLOAD_TIMEOUT)\n",
    "    status = rr.status_code\n",
    "    # capture Location/Operation headers and return them\n",
    "    loc = rr.headers.get('Location') or rr.headers.get('Operation-Location') or rr.headers.get('Azure-AsyncOperation')\n",
    "    # Detailed headers/debugging commented out\n",
    "    # try:\n",
    "    #     print('  response headers:')\n",
    "    #     for k, v in rr.headers.items():\n",
    "    #         print(f'    {k}: {v}')\n",
    "    # except Exception:\n",
    "    #     pass\n",
    "    body_preview = None\n",
    "    try:\n",
    "        if rr.text:\n",
    "            body_preview = rr.text[:2000]\n",
    "    except Exception:\n",
    "        body_preview = None\n",
    "    return status, body_preview, loc\n",
    "\n",
    "def poll_job_instance(location_url, timeout_seconds=POLL_TIMEOUT_SECONDS, interval_seconds=POLL_INTERVAL):\n",
    "    \"\"\"\n",
    "    Poll the job instance URL (GET) until status is terminal or timeout.\n",
    "    Returns the last JSON response (or None on non-JSON).\n",
    "    Terminal statuses include 'Succeeded', 'Failed', 'Cancelled', 'Completed', 'SucceededWithWarnings'.\n",
    "    \"\"\"\n",
    "    deadline = time.time() + timeout_seconds\n",
    "    last_json = None\n",
    "    print(\"  Polling job instance:\", location_url)\n",
    "    terminal_states = {\"succeeded\", \"failed\", \"cancelled\", \"completed\", \"succeededwithwarnings\"}\n",
    "    while time.time() < deadline:\n",
    "        try:\n",
    "            gr = requests.get(location_url, headers=headers, timeout=30)\n",
    "            # Progress/status print\n",
    "            print(\"    GET op status:\", gr.status_code)\n",
    "            if gr.status_code == 200:\n",
    "                try:\n",
    "                    job = gr.json()\n",
    "                    last_json = job\n",
    "                except Exception:\n",
    "                    # Non-JSON response; keep last_json as None\n",
    "                    last_json = None\n",
    "                status = (last_json or {}).get(\"status\") if isinstance(last_json, dict) else None\n",
    "                if status:\n",
    "                    print(f\"    job instance status: {status}\")\n",
    "                    if str(status).lower() in terminal_states:\n",
    "                        return last_json\n",
    "            else:\n",
    "                print(\"    non-200 response while polling:\", gr.status_code)\n",
    "        except Exception as e:\n",
    "            print(\"    poll GET failed:\", e)\n",
    "        time.sleep(interval_seconds)\n",
    "    print(\"  Poll timed out after\", timeout_seconds, \"seconds\")\n",
    "    return last_json\n",
    "\n",
    "# Runner loop (with polling the returned job instance). Include notebook name in the summary.\n",
    "notebook_display = \"1.IngestData\"\n",
    "results = []\n",
    "for run_cfg in runs:\n",
    "    ws_name = run_cfg.get('name')\n",
    "    if not ws_name:\n",
    "        results.append({'workspace': None, 'notebook': notebook_display, 'status': 'skipped_no_name', 'cfg': run_cfg})\n",
    "        continue\n",
    "\n",
    "    print('\\n==> Processing run for workspace:', ws_name)\n",
    "    ws_id = workspace_id_by_name(ws_name)\n",
    "    if not ws_id:\n",
    "        print('  workspace not found:', ws_name)\n",
    "        results.append({'workspace': ws_name, 'notebook': notebook_display, 'status': 'workspace_not_found'})\n",
    "        continue\n",
    "    print('  workspace_id:', ws_id)\n",
    "\n",
    "    item_id = item_id_for_notebook(ws_id, notebook_display)\n",
    "    if not item_id:\n",
    "        print(f'  {notebook_display} notebook not found in workspace:', ws_name)\n",
    "        results.append({'workspace': ws_name, 'workspace_id': ws_id, 'notebook': notebook_display, 'status': 'notebook_not_found'})\n",
    "        continue\n",
    "    print('  found item_id:', item_id)\n",
    "\n",
    "    status_code, body_preview, loc = trigger_run_and_get_location(ws_id, item_id, run_cfg)\n",
    "    print('  trigger run response code:', status_code)\n",
    "\n",
    "    polled = None\n",
    "    if loc:\n",
    "        polled = poll_job_instance(loc, timeout_seconds=POLL_TIMEOUT_SECONDS, interval_seconds=POLL_INTERVAL)\n",
    "        if polled is None:\n",
    "            print('  polling produced no JSON result or timed out')\n",
    "            results.append({'workspace': ws_name, 'workspace_id': ws_id, 'notebook': notebook_display, 'item_id': item_id, 'status_code': status_code, 'location': loc, 'polled': None})\n",
    "        else:\n",
    "            # keep failureReason if present, but do not print verbose debug\n",
    "            fr = polled.get('failureReason') if isinstance(polled, dict) else None\n",
    "            if fr:\n",
    "                print('  job failed with failureReason:', fr.get('message') if isinstance(fr, dict) else fr)\n",
    "            # final state print (concise)\n",
    "            print('  final job instance status:', polled.get('status') if isinstance(polled, dict) else polled)\n",
    "            results.append({'workspace': ws_name, 'workspace_id': ws_id, 'notebook': notebook_display, 'item_id': item_id, 'status_code': status_code, 'location': loc, 'polled': polled})\n",
    "    else:\n",
    "        print('  no Location/Operation header returned by run POST; cannot poll instance')\n",
    "        results.append({'workspace': ws_name, 'workspace_id': ws_id, 'notebook': notebook_display, 'item_id': item_id, 'status_code': status_code, 'location': None})\n",
    "\n",
    "    # small pause between triggers\n",
    "    time.sleep(1)\n",
    "\n",
    "print('\\nRun summary:')\n",
    "print(json.dumps(results, indent=2))\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}

{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ],
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "  \"conf\": {\n",
        "    \"spark.notebook.parameters\": \"{}\"\n",
        "  },\n",
        "  \"defaultLakehouse\": {\n",
        "    \"name\": \"BenchmarkLakehouse\"\n",
        "  }\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ““ 3. Queries\n",
        "## Query Performance Benchmarking\n",
        "\n",
        "### ðŸ”— Ensure `BenchmarkLakehouse`/`BenchmarkWarehouse` are reachable before running.\n",
        "\n",
        "This notebook runs query performance against the single target defined by the notebook parameters (one parameter_set per workspace/run).\n",
        "It runs only the queries relevant to the configured format (Delta or Warehouse) and logs concise progress prints at the end of cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "import json\n",
        "import time\n",
        "import re\n",
        "from datetime import datetime\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
        "print('Imports ready')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "params"
      },
      "source": [
        "# Load parameters from spark.notebook.parameters (fail hard if missing/incorrect)\n",
        "conf_key = 'spark.notebook.parameters'\n",
        "conf_str = spark.conf.get(conf_key)\n",
        "if not conf_str:\n",
        "    # fail hard per instruction\n",
        "    raise SystemExit('Missing spark.notebook.parameters (expected the parameter set in %%configure)')\n",
        "\n",
        "params = json.loads(conf_str)\n",
        "\n",
        "# Required params (fail hard if not present)\n",
        "test_case_name = params['name']\n",
        "dataset_name = params['dataset_name']\n",
        "fmt = params['format']  # expected 'delta' or 'warehouse'\n",
        "target_lakehouse = params['target_lakehouse']\n",
        "target_warehouse = params['target_warehouse']\n",
        "\n",
        "# sanitized name (table name) â€” same logic as other notebooks\n",
        "sanitized_name = re.sub(r\"[^a-z0-9_]\", \"\", re.sub(r\"[\\s-]+\", \"_\", test_case_name.strip().lower()))\n",
        "target_table = sanitized_name\n",
        "\n",
        "print('Loaded params:')\n",
        "print('  test_case_name:', test_case_name)\n",
        "print('  dataset_name:', dataset_name)\n",
        "print('  format:', fmt)\n",
        "print('  target_table:', target_table)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "metrics_schema"
      },
      "source": [
        "# Metrics schema used to log the query timings to the lakehouse metrics table\n",
        "metrics_schema = StructType([\n",
        "    StructField(\"test_case_id\", StringType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"format\", StringType(), True),\n",
        "    StructField(\"location\", StringType(), True),\n",
        "    StructField(\"rows\", IntegerType(), True),\n",
        "    StructField(\"update_strategy\", StringType(), True),\n",
        "    StructField(\"ingest_time_s\", FloatType(), True),\n",
        "    StructField(\"spinup_time_s\", FloatType(), True),\n",
        "    StructField(\"storage_size_mb\", FloatType(), True),\n",
        "    StructField(\"query_type\", StringType(), True),\n",
        "    StructField(\"query_time_s\", FloatType(), True),\n",
        "    StructField(\"cu_used\", FloatType(), True),\n",
        "    StructField(\"notes\", StringType(), True)\n",
        "])\n",
        "print('Metrics schema ready')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "load_target_table"
      },
      "source": [
        "# Load the single target table depending on the configured format\n",
        "if fmt.lower() == 'delta':\n",
        "    lakehouse_table = f\"{target_lakehouse}.{target_table}\"\n",
        "    print(f'Loading Delta table: {lakehouse_table}')\n",
        "    df = spark.read.table(lakehouse_table)\n",
        "    target_location = 'Lakehouse'\n",
        "elif fmt.lower() == 'warehouse':\n",
        "    warehouse_table = f\"{target_warehouse}.dbo.{target_table}\"\n",
        "    print(f'Loading Warehouse table via synapsesql: {warehouse_table}')\n",
        "    # synapsesql read requires the Fabric connector\n",
        "    from com.microsoft.spark.fabric import Constants\n",
        "    df = spark.read.synapsesql(warehouse_table)\n",
        "    target_location = 'Warehouse'\n",
        "else:\n",
        "    raise SystemExit(f'Unsupported format in parameters: {fmt}')\n",
        "\n",
        "print('Loaded target table into DataFrame')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "query_funcs"
      },
      "source": [
        "# Define simple query functions (assume schema as in ingest/generate flows)\n",
        "def q_filter(df):\n",
        "    return df.filter(col('cat_1') == 'A').count()\n",
        "\n",
        "def q_aggregate(df):\n",
        "    return df.groupBy('cat_1').agg({ 'num_1': 'avg', 'num_2': 'max' }).count()\n",
        "\n",
        "def q_batch(df):\n",
        "    # select updates after a reasonable cutoff (keeps static string as before)\n",
        "    return df.filter((col('update_type') == 'update') & (col('ts_1') > '2025-01-01')).count()\n",
        "\n",
        "def q_topn(df):\n",
        "    return df.orderBy(col('num_1').desc()).limit(10).count()\n",
        "\n",
        "print('Query functions ready')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_queries"
      },
      "source": [
        "# Run the four queries exactly once against the single target and record timings\n",
        "query_list = [\n",
        "    ('Filter', q_filter),\n",
        "    ('Aggregate', q_aggregate),\n",
        "    ('Batch', q_batch),\n",
        "    ('TopN', q_topn),\n",
        "]\n",
        "metrics = []\n",
        "for qname, qfunc in query_list:\n",
        "    t0 = time.time()\n",
        "    rows = qfunc(df)\n",
        "    t1 = time.time()\n",
        "    elapsed = t1 - t0\n",
        "    print(f\"{qname}: {elapsed:.3f}s (rows={rows})\")\n",
        "    metrics.append({'query': qname, 'rows': int(rows), 'time_s': float(elapsed)})\n",
        "\n",
        "print('All queries executed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "display_metrics"
      },
      "source": [
        "# Display metrics locally (pandas) â€” concise view\n",
        "import pandas as pd\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "display(metrics_df)\n",
        "print('Displayed metrics dataframe')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "log_to_metrics"
      },
      "source": [
        "# Log metrics to the lakehouse metrics table (use test_case_name as test_case_id)\n",
        "def log_query_to_metrics(test_case_id, format_name, location, rows, query_type, query_time_s, notes=''):\n",
        "    metrics_row = [(\n",
        "        test_case_id,\n",
        "        datetime.now(),\n",
        "        format_name,\n",
        "        location,\n",
        "        int(rows),\n",
        "        '',                # update_strategy (N/A)\n",
        "        float('nan'),      # ingest_time_s (N/A)\n",
        "        float('nan'),      # spinup_time_s (N/A)\n",
        "        float('nan'),      # storage_size_mb (N/A)\n",
        "        query_type,\n",
        "        float(query_time_s),\n",
        "        float('nan'),      # cu_used (N/A)\n",
        "        notes\n",
        "    )]\n",
        "    spark.createDataFrame(metrics_row, schema=metrics_schema).write.mode('append').saveAsTable(f\"{target_lakehouse}.metrics\")\n",
        "print('Log-to-metrics helper ready')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "log_metrics_loop"
      },
      "source": [
        "# Persist metrics for this run (use format label capitalized)\n",
        "format_label = 'Delta' if fmt.lower() == 'delta' else 'Warehouse'\n",
        "for m in metrics:\n",
        "    log_query_to_metrics(\n",
        "        test_case_id=test_case_name,\n",
        "        format_name=format_label,\n",
        "        location=target_location,\n",
        "        rows=m['rows'],\n",
        "        query_type=m['query'],\n",
        "        query_time_s=m['time_s'],\n",
        "        notes=f\"Query performance ({m['query']})\"\n",
        "    )\n",
        "print('Logged all metrics to lakehouse.metrics')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "completion"
      },
      "source": [
        "print('Queries notebook complete for test_case:', test_case_name, 'format:', fmt)\n",
        "print('Sanitized table name used:', target_table)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}

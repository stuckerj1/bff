{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6630581",
   "metadata": {},
   "source": [
    "# ðŸ““ Synthetic Data Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c099ab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Synthetic Data Generation for Fabric Benchmarking\n",
    "# Purpose: Generate base datasets and incremental update slices for capacity and performance testing\n",
    "\n",
    "# ---\n",
    "# 1. Parameter Setup\n",
    "\n",
    "# Parameters (can be set via notebook widgets or workflow inputs)\n",
    "row_count = 10000  # Options: 10000, 1000000\n",
    "format = \"parquet\" # Options: \"parquet\", \"delta\"\n",
    "schema_config = {\n",
    "    \"categorical_fields\": 3,\n",
    "    \"numeric_fields\": 5,\n",
    "    \"timestamp_fields\": 2\n",
    "}\n",
    "distribution = \"uniform\"  # Options: \"uniform\", \"skewed\", \"null_injection\"\n",
    "seed = 42  # For deterministic output\n",
    "\n",
    "# Lakehouse output folders (Fabric)\n",
    "base_output_folder = \"/lakehouse/default/Files/base/\"\n",
    "updates_output_folder = \"/lakehouse/default/Files/updates/\"\n",
    "cdc_output_folder = \"/lakehouse/default/Files/cdc/\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---\n",
    "# 2. Schema Definition\n",
    "\n",
    "def create_synthetic_schema(config):\n",
    "    import random\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    schema = {}\n",
    "    # Categorical fields\n",
    "    for i in range(config[\"categorical_fields\"]):\n",
    "        schema[f\"cat_{i+1}\"] = lambda n: np.random.choice(['A', 'B', 'C', 'D'], size=n)\n",
    "    # Numeric fields\n",
    "    for i in range(config[\"numeric_fields\"]):\n",
    "        schema[f\"num_{i+1}\"] = lambda n: np.random.uniform(0, 1000, size=n)\n",
    "    # Timestamp fields\n",
    "    for i in range(config[\"timestamp_fields\"]):\n",
    "        start = pd.Timestamp(\"2023-01-01\")\n",
    "        schema[f\"ts_{i+1}\"] = lambda n: pd.date_range(start, periods=n)\n",
    "    return schema\n",
    "\n",
    "# ---\n",
    "# 3. Synthetic Data Generation\n",
    "\n",
    "def generate_base_dataframe(row_count, schema, distribution, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    data = {}\n",
    "    for col, gen in schema.items():\n",
    "        if \"num\" in col and distribution == \"skewed\":\n",
    "            data[col] = np.random.exponential(500, size=row_count)\n",
    "        elif \"num\" in col and distribution == \"null_injection\":\n",
    "            col_data = gen(row_count)\n",
    "            null_mask = np.random.rand(row_count) < 0.05\n",
    "            col_data[null_mask] = np.nan\n",
    "            data[col] = col_data\n",
    "        else:\n",
    "            data[col] = gen(row_count)\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"id\"] = np.arange(1, row_count + 1)\n",
    "    return df\n",
    "\n",
    "schema = create_synthetic_schema(schema_config)\n",
    "base_df = generate_base_dataframe(row_count, schema, distribution, seed)\n",
    "\n",
    "# ---\n",
    "# 4. Save Base Dataset (Parquet/Delta)\n",
    "\n",
    "os.makedirs(base_output_folder, exist_ok=True)\n",
    "base_file = f\"{base_output_folder}base_{row_count}_{format}.parquet\"\n",
    "base_df.to_parquet(base_file)\n",
    "\n",
    "# If using Delta format (with PySpark), add logic here:\n",
    "# TODO: Save as Delta table if format == \"delta\"\n",
    "\n",
    "print(f\"Base dataset saved: {base_file}\")\n",
    "\n",
    "# ---\n"
    ]
},
{
  "cell_type": "code",
  "execution_count": null,
  "id": "spark-validation-cell",
  "metadata": {
    "tags": [],
    "language": "python"
  },
  "outputs": [],
  "source": [
    "%%pyspark\n",
    "# Minimal PySpark cell to validate kernel and Lakehouse binding\n",
    "df = spark.range(100).withColumnRenamed(\"id\", \"synthetic_id\")\n",
    "df.write.mode(\"overwrite\").parquet(\"/lakehouse/default/Files/test_spark_output/\")\n",
    "print(\"PySpark test write complete.\")\n"
  ]
},
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "next-cell-id",
      "metadata": {},
      "outputs": [],
      "source": [
    "# 5. Generate Incremental Update Slices (Batch + CDC)\n",
    "\n",
    "def generate_updates(df, change_pct=0.01, new_pct=0.005, delete_pct=0.001, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    n = len(df)\n",
    "    # Changed rows\n",
    "    changed_count = int(n * change_pct)\n",
    "    change_idx = np.random.choice(df.index, changed_count, replace=False)\n",
    "    changed_df = df.loc[change_idx].copy()\n",
    "    changed_df[\"num_1\"] += np.random.uniform(1, 10, changed_count)  # Example modification\n",
    "    changed_df[\"update_type\"] = \"update\"\n",
    "    # New rows\n",
    "    new_count = int(n * new_pct)\n",
    "    new_df = df.sample(new_count).copy()\n",
    "    new_df[\"id\"] = np.arange(n + 1, n + new_count + 1)\n",
    "    new_df[\"update_type\"] = \"insert\"\n",
    "    # Deleted rows\n",
    "    delete_count = int(n * delete_pct)\n",
    "    delete_idx = np.random.choice(df.index, delete_count, replace=False)\n",
    "    deleted_df = df.loc[delete_idx][[\"id\"]].copy()\n",
    "    deleted_df[\"update_type\"] = \"delete\"\n",
    "    # Combine updates\n",
    "    updates = pd.concat([changed_df, new_df, deleted_df], ignore_index=True)\n",
    "    return updates\n",
    "\n",
    "updates_df = generate_updates(base_df)\n",
    "\n",
    "# ---\n",
    "# 6. Save Updates and CDC Slices\n",
    "\n",
    "os.makedirs(updates_output_folder, exist_ok=True)\n",
    "os.makedirs(cdc_output_folder, exist_ok=True)\n",
    "updates_file = f\"{updates_output_folder}updates_{row_count}_{format}.parquet\"\n",
    "cdc_file = f\"{cdc_output_folder}cdc_{row_count}_{format}.parquet\"\n",
    "\n",
    "updates_df.to_parquet(updates_file)\n",
    "# For CDC, use only changes (updates + deletes)\n",
    "cdc_df = updates_df[updates_df[\"update_type\"] != \"insert\"]\n",
    "cdc_df.to_parquet(cdc_file)\n",
    "\n",
    "print(f\"Updates saved: {updates_file}\")\n",
    "print(f\"CDC slice saved: {cdc_file}\")\n",
    "\n",
    "# ---\n",
    "# 7. Metadata Logging\n",
    "\n",
    "def log_metadata(file_path, row_count, format, distribution, update_type):\n",
    "    import json\n",
    "    meta = {\n",
    "        \"file\": file_path,\n",
    "        \"rows\": row_count,\n",
    "        \"format\": format,\n",
    "        \"distribution\": distribution,\n",
    "        \"update_type\": update_type,\n",
    "        \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    meta_file = file_path.replace(\".parquet\", \".meta.json\")\n",
    "    with open(meta_file, \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    print(f\"Metadata logged: {meta_file}\")\n",
    "\n",
    "log_metadata(base_file, row_count, format, distribution, \"base\")\n",
    "log_metadata(updates_file, len(updates_df), format, distribution, \"batch_updates\")\n",
    "log_metadata(cdc_file, len(cdc_df), format, distribution, \"cdc_updates\")\n",
    "\n",
    "# ---\n",
    "# 8. Summary and Next Steps\n",
    "\n",
    "print(\"Synthetic data generation complete.\")\n",
    "print(\"Proceed to ingestion workflows and benchmarking.\")\n",
    "\n",
    "# ---\n",
    "# 9. Diagnostics: List Lakehouse Files\n",
    "try:\n",
    "    import msparkutils\n",
    "    print(\"Lakehouses:\", msparkutils.lakehouse.listLakehouses())\n",
    "    print(\"Base folder files:\", msparkutils.fs.ls(base_output_folder))\n",
    "    print(\"Updates folder files:\", msparkutils.fs.ls(updates_output_folder))\n",
    "    print(\"CDC folder files:\", msparkutils.fs.ls(cdc_output_folder))\n",
    "except ImportError:\n",
    "    print(\"msparkutils not available. Skipping Lakehouse diagnostics.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

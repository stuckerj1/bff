{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "%%configure\n",
        "# All values that may be overridden by an orchestrator are declared here.\n",
        "# This makes it # easy to validate when invoked programmatically with different numbers.\n",
        "\n",
        "# -- Datasets array (interactive defaults are different than automation defaults).\n",
        "DATASETS_PARAM = [\n",
        "    {\n",
        "        \"name\": \"1k\",\n",
        "        \"row_count\": 1000,\n",
        "        \"change_fraction\": 0.01,\n",
        "        \"new_fraction\": 0.005,\n",
        "        \"delete_fraction\": 0.001,\n",
        "        \"seed\": 42,\n",
        "        \"description\": \"Interactive small dataset (1k rows)\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"100k\",\n",
        "        \"row_count\": 100000,\n",
        "        \"change_fraction\": 0.01,\n",
        "        \"new_fraction\": 0.005,\n",
        "        \"delete_fraction\": 0.001,\n",
        "        \"seed\": 42,\n",
        "        \"description\": \"Interactive medium dataset (100k rows)\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Do the test cases (datasets and parameter sets) include Azure SQL as a data source?\n",
        "PUSH_TO_AZURE_SQL = True\n",
        "\n",
        "# Azure SQL / ODBC parameters (interactive defaults). \n",
        "# These should be passed in from the automation handler.\n",
        "AZURE_SQL_SERVER = \"benchmarking-bff\"\n",
        "AZURE_SQL_DB = \"benchmarking\"\n",
        "\n",
        "# These connection details do not need to be passed in but can be optionally changed\n",
        "AZURE_SQL_CONNECTION = f\"Driver={{ODBC Driver 18 for SQL Server}};Server=tcp:{AZURE_SQL_SERVER}.database.windows.net,1433;Database={AZURE_SQL_DB};Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30\"\n",
        "AZURE_SQL_SCHEMA = \"dbo\"\n",
        "\n",
        "distribution = \"uniform\"  # Options: \"uniform\", \"skewed\", \"null_injection\"\n",
        "seed = 42  # For deterministic output\n",
        "\n",
        "# Lakehouse / local output folders -- these will be dynamically created per-dataset in the loop\n",
        "# Orchestrators can overwrite any of the above by injecting values into the notebook \n",
        "# (e.g. pass 'DATASETS_PARAM' JSON, AZURE_SQL_SERVER, AZURE_SQL_DB into the notebook). \n",
        "# End of parameters cell. \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Data — minimal parquet writer (per-dataset paths) + optional SQL copy\n",
        "\n",
        "This notebook is intentionally minimal and focused. It loops DATASETS_PARAM, generates a base and updates parquet per dataset, writes per-dataset lakehouse paths (e.g. /lakehouse/default/Files/{name}base/), and also writes controller/local copies under data/controller/{name}/. Filenames are base.parquet and updates.parquet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import pytz\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "print('Imports OK — proceeding with generation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- Schema Definition\n",
        "Create a small synthetic schema generator (categorical, numeric, timestamps)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_synthetic_schema(config):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    schema = {}\n",
        "    # Categorical fields\n",
        "    for i in range(config[\"categorical_fields\"]):\n",
        "        schema[f\"cat_{i+1}\"] = lambda n, _choices=['A','B','C','D']: np.random.choice(_choices, size=n)\n",
        "    # Numeric fields\n",
        "    for i in range(config[\"numeric_fields\"]):\n",
        "        schema[f\"num_{i+1}\"] = lambda n: np.random.uniform(0, 1000, size=n)\n",
        "    # Timestamp fields\n",
        "    for i in range(config[\"timestamp_fields\"]):\n",
        "        start = pd.Timestamp(\"2023-01-01\")\n",
        "        schema[f\"ts_{i+1}\"] = lambda n: pd.date_range(start, periods=n, freq=\"ms\")\n",
        "    return schema\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- Synthetic Data Generation\n",
        "Generate the base dataframe from the schema and the selected distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate_base_dataframe(row_count, schema, distribution, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    data = {}\n",
        "    for col, gen in schema.items():\n",
        "        if \"num\" in col and distribution == \"skewed\":\n",
        "            data[col] = np.random.exponential(500, size=row_count)\n",
        "        elif \"num\" in col and distribution == \"null_injection\":\n",
        "            col_data = gen(row_count)\n",
        "            null_mask = np.random.rand(row_count) < 0.05\n",
        "            col_data[null_mask] = np.nan\n",
        "            data[col] = col_data\n",
        "        else:\n",
        "            data[col] = gen(row_count)\n",
        "    df = pd.DataFrame(data)\n",
        "    df[\"id\"] = np.arange(1, row_count + 1)\n",
        "    for col in df.columns:\n",
        "        if col.startswith(\"ts_\"):\n",
        "            df[col] = pd.to_datetime(df[col]).dt.tz_localize(pytz.UTC)\n",
        "    df[\"update_type\"] = \"insert\"\n",
        "    return df\n",
        "\n",
        "schema = create_synthetic_schema(schema_config)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- Save Loop (per-dataset paths and filenames)\n",
        "Loop DATASETS_PARAM, set per-dataset lakehouse paths and write base.parquet and updates.parquet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data_root = Path('data')\n",
        "for ds in DATASETS_PARAM:\n",
        "    name = ds.get('name')\n",
        "    row_count_ds = int(ds.get('row_count', 10000))\n",
        "    change_fraction = float(ds.get('change_fraction', 0.01))\n",
        "    new_fraction = float(ds.get('new_fraction', 0.0))\n",
        "    delete_fraction = float(ds.get('delete_fraction', 0.0))\n",
        "    seed_ds = int(ds.get('seed')) if ds.get('seed') is not None else seed\n",
        "    print(f\"\\n=== Generating dataset {name}: rows={row_count_ds}\")\n",
        "\n",
        "    # Per-dataset lakehouse paths (as requested)\n",
        "    base_output_folder = f\"/lakehouse/default/Files/{name}base/\"\n",
        "    updates_output_folder = f\"/lakehouse/default/Files/{name}updates/\"\n",
        "\n",
        "    # Local controller area (always write a local copy under data/controller/{name}/)\n",
        "    local_dir = data_root / name\n",
        "    local_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Generate base and updates\n",
        "    base_df = generate_base_dataframe(row_count_ds, schema, distribution, seed=seed_ds)\n",
        "    base_file_lake = os.path.join(base_output_folder, 'base.parquet')\n",
        "    updates_file_lake = os.path.join(updates_output_folder, 'updates.parquet')\n",
        "\n",
        "    # Ensure directories exist locally and for lakehouse path (os.makedirs will be a no-op if not permitted)\n",
        "    try:\n",
        "        os.makedirs(base_output_folder, exist_ok=True)\n",
        "    except Exception:\n",
        "        # running outside Fabric: ignore\n",
        "        pass\n",
        "    try:\n",
        "        os.makedirs(updates_output_folder, exist_ok=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Write local copies first\n",
        "    base_file_local = local_dir / 'base.parquet'\n",
        "    base_df.to_parquet(str(base_file_local), engine='pyarrow', index=False, coerce_timestamps='ms')\n",
        "    print('  wrote local base:', base_file_local)\n",
        "\n",
        "    updates_df = generate_updates(base_df, change_pct=change_fraction, new_pct=new_fraction, delete_pct=delete_fraction, seed=seed_ds)\n",
        "    updates_file_local = local_dir / 'updates.parquet'\n",
        "    updates_df.to_parquet(str(updates_file_local), engine='pyarrow', index=False, coerce_timestamps='ms')\n",
        "    print('  wrote local updates:', updates_file_local)\n",
        "\n",
        "    # Try to write lakehouse copies (best-effort)\n",
        "    try:\n",
        "        # Write bytes if possible (some runtimes may allow write to mount paths)\n",
        "        Path(base_output_folder).mkdir(parents=True, exist_ok=True)\n",
        "        (Path(base_output_folder) / 'base.parquet').write_bytes(base_file_local.read_bytes())\n",
        "        Path(updates_output_folder).mkdir(parents=True, exist_ok=True)\n",
        "        (Path(updates_output_folder) / 'updates.parquet').write_bytes(updates_file_local.read_bytes())\n",
        "        print('  wrote lakehouse copies ->', base_output_folder, updates_output_folder)\n",
        "    except Exception:\n",
        "        # not fatal; running locally or no permission to write lakehouse path\n",
        "        print('  could not write to lakehouse path (running outside Fabric or no permission); skipped lakehouse writes')\n",
        "\n",
        "    # Write controller copies (local controller area)\n",
        "    ctrl_dir = data_root / 'controller' / name\n",
        "    ctrl_dir.mkdir(parents=True, exist_ok=True)\n",
        "    (ctrl_dir / 'base.parquet').write_bytes(base_file_local.read_bytes())\n",
        "    (ctrl_dir / 'updates.parquet').write_bytes(updates_file_local.read_bytes())\n",
        "    print('  wrote controller copies ->', ctrl_dir)\n",
        "\n",
        "print('\\nGeneration loop complete.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- Metadata Logging\n",
        "Log simple metadata about the files created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def log_metadata(file_path, row_count_val, fmt, distribution_val, update_type):\n",
        "    import json\n",
        "    meta = {\n",
        "        \"file\": file_path,\n",
        "        \"rows\": row_count_val,\n",
        "        \"format\": fmt,\n",
        "        \"distribution\": distribution_val,\n",
        "        \"update_type\": update_type,\n",
        "        \"timestamp\": pd.Timestamp.now().isoformat()\n",
        "    }\n",
        "    meta_file = str(file_path).replace('.parquet', '.meta.json')\n",
        "    with open(meta_file, 'w') as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "    print(f\"Metadata logged: {meta_file}\")\n",
        "\n",
        "# Log metadata for each dataset's local files\n",
        "for ds in DATASETS_PARAM:\n",
        "    name = ds.get('name')\n",
        "    row_count_ds = int(ds.get('row_count', 10000))\n",
        "    local_dir = data_root / name\n",
        "    base_file_local = local_dir / 'base.parquet'\n",
        "    updates_file_local = local_dir / 'updates.parquet'\n",
        "    if base_file_local.exists():\n",
        "        log_metadata(str(base_file_local), row_count_ds, format, distribution, 'base')\n",
        "    if updates_file_local.exists():\n",
        "        log_metadata(str(updates_file_local), len(pd.read_parquet(str(updates_file_local), engine='pyarrow')), format, distribution, 'batch_updates')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}

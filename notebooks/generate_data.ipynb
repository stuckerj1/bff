{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "%%configure\n",
        "# All values that may be overridden by an orchestrator are declared here.\n",
        "# This makes it # easy to validate when invoked programmatically with different numbers.\n",
        "\n",
        "# -- Datasets array (interactive defaults are different than automation defaults).\n",
        "DATASETS_PARAM = [\n",
        "    {\n",
        "        \"name\": \"1k\",\n",
        "        \"row_count\": 1000,\n",
        "        \"change_fraction\": 0.01,\n",
        "        \"new_fraction\": 0.005,\n",
        "        \"delete_fraction\": 0.001,\n",
        "        \"seed\": 42,\n",
        "        \"description\": \"Interactive small dataset (1k rows)\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"100k\",\n",
        "        \"row_count\": 100000,\n",
        "        \"change_fraction\": 0.01,\n",
        "        \"new_fraction\": 0.005,\n",
        "        \"delete_fraction\": 0.001,\n",
        "        \"seed\": 42,\n",
        "        \"description\": \"Interactive medium dataset (100k rows)\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Do the test cases (datasets and parameter sets) include Azure SQL as a data source?\n",
        "PUSH_TO_AZURE_SQL = True\n",
        "\n",
        "# Azure SQL / ODBC parameters (interactive defaults). \n",
        "# These should be passed in from the automation handler.\n",
        "AZURE_SQL_SERVER = \"benchmarking-bff\"\n",
        "AZURE_SQL_DB = \"benchmarking\"\n",
        "\n",
        "# These connection details do not need to be passed in but can be optionally changed\n",
        "AZURE_SQL_CONNECTION = f\"Driver={{ODBC Driver 18 for SQL Server}};Server=tcp:{AZURE_SQL_SERVER}.database.windows.net,1433;Database={AZURE_SQL_DB};Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30\"\n",
        "AZURE_SQL_SCHEMA = \"dbo\"\n",
        "\n",
        "distribution = \"uniform\"  # Options: \"uniform\", \"skewed\", \"null_injection\"\n",
        "seed = 42  # For deterministic output\n",
        "\n",
        "# Lakehouse / local output folders -- these will be dynamically created for each dataset\n",
        "# base_output_folder = \"/lakehouse/default/Files/base{name}/\"\n",
        "# updates_output_folder = \"/lakehouse/default/Files/updates{name}/\"\n",
        "\n",
        "# Orchestrators can overwrite any of the above by injecting values into the notebook \n",
        "# (e.g. pass 'DATASETS_PARAM' JSON, AZURE_SQL_SERVER, AZURE_SQL_DB into the notebook). \n",
        "# End of parameters cell. \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Data — minimal parquet writer + optional SQL copy\n",
        "\n",
        "This notebook is intentionally minimal: it iterates the datasets declared in the parameters cell, produces a base.parquet and updates.parquet for each dataset, writes controller copies under `data/controller/{dataset}/`, and optionally uploads those tables into Azure SQL if `PUSH_TO_AZURE_SQL=True` and a connection string is provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import pytz\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "print('Imports OK — proceeding with generation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- 2. Schema Definition\n",
        "Create a small synthetic schema generator (categorical, numeric, timestamps)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_synthetic_schema(config):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    schema = {}\n",
        "    # Categorical fields\n",
        "    for i in range(config[\"categorical_fields\"]):\n",
        "        # Use a closure that captures the local RNG; return numpy arrays of choices\n",
        "        schema[f\"cat_{i+1}\"] = lambda n, _choices=['A','B','C','D']: np.random.choice(_choices, size=n)\n",
        "    # Numeric fields\n",
        "    for i in range(config[\"numeric_fields\"]):\n",
        "        schema[f\"num_{i+1}\"] = lambda n: np.random.uniform(0, 1000, size=n)\n",
        "    # Timestamp fields\n",
        "    for i in range(config[\"timestamp_fields\"]):\n",
        "        start = pd.Timestamp(\"2023-01-01\")\n",
        "        # Millisecond precision to guarantee Spark compatibility\n",
        "        schema[f\"ts_{i+1}\"] = lambda n: pd.date_range(start, periods=n, freq=\"ms\")\n",
        "    return schema\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- 3. Synthetic Data Generation\n",
        "Generate the base dataframe from the schema and the selected distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate_base_dataframe(row_count, schema, distribution, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    data = {}\n",
        "    for col, gen in schema.items():\n",
        "        if \"num\" in col and distribution == \"skewed\":\n",
        "            data[col] = np.random.exponential(500, size=row_count)\n",
        "        elif \"num\" in col and distribution == \"null_injection\":\n",
        "            col_data = gen(row_count)\n",
        "            null_mask = np.random.rand(row_count) < 0.05\n",
        "            col_data[null_mask] = np.nan\n",
        "            data[col] = col_data\n",
        "        else:\n",
        "            data[col] = gen(row_count)\n",
        "    df = pd.DataFrame(data)\n",
        "    df[\"id\"] = np.arange(1, row_count + 1)\n",
        "    # Ensure timestamp columns are pandas datetime64[ms] and timezone-aware (UTC)\n",
        "    for col in df.columns:\n",
        "        if col.startswith(\"ts_\"):\n",
        "            df[col] = pd.to_datetime(df[col]).dt.tz_localize(pytz.UTC)\n",
        "    # Add update_type column, always 'insert' for base\n",
        "    df[\"update_type\"] = \"insert\"\n",
        "    return df\n",
        "\n",
        "schema = create_synthetic_schema(schema_config)\n",
        "base_df = generate_base_dataframe(row_count, schema, distribution, seed)\n",
        "print(f\"Base dataframe created: rows={len(base_df)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- 4. Save Base Dataset (Parquet/Delta)\n",
        "Write the base dataset to parquet (or delta if you change format)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "os.makedirs('/tmp', exist_ok=True)\n",
        "base_output_folder = f\"/tmp/base_{row_count}/\" if not base_output_folder else base_output_folder\n",
        "updates_output_folder = f\"/tmp/updates_{row_count}/\" if not 'updates_output_folder' in globals() else updates_output_folder\n",
        "os.makedirs(base_output_folder, exist_ok=True)\n",
        "base_file = f\"{base_output_folder}base_{row_count}_{format}.parquet\"\n",
        "# All timestamps are timezone-aware UTC, update_type is 'insert'\n",
        "base_df.to_parquet(base_file, coerce_timestamps='ms', engine='pyarrow')\n",
        "print(f\"Base dataset saved: {base_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- 5. Generate Incremental Update Slices (Batch & CDC)\n",
        "Create updates (updates, inserts, deletes) and shuffle them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate_updates(df, change_pct=0.01, new_pct=0.005, delete_pct=0.001, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    n = len(df)\n",
        "    # Changed rows\n",
        "    changed_count = int(n * change_pct)\n",
        "    change_idx = np.random.choice(df.index, changed_count, replace=False)\n",
        "    changed_df = df.loc[change_idx].copy()\n",
        "    # Example modification of one numeric column if present\n",
        "    if \"num_1\" in changed_df.columns:\n",
        "        changed_df[\"num_1\"] = changed_df[\"num_1\"] + np.random.uniform(1, 10, changed_count)\n",
        "    changed_df[\"update_type\"] = \"update\"\n",
        "    # New rows\n",
        "    new_count = int(n * new_pct)\n",
        "    new_df = df.sample(new_count).copy()\n",
        "    new_df[\"id\"] = np.arange(n + 1, n + new_count + 1)\n",
        "    new_df[\"update_type\"] = \"insert\"\n",
        "    # Deleted rows\n",
        "    delete_count = int(n * delete_pct)\n",
        "    delete_idx = np.random.choice(df.index, delete_count, replace=False)\n",
        "    deleted_df = df.loc[delete_idx][[\"id\"]].copy()\n",
        "    deleted_df[\"update_type\"] = \"delete\"\n",
        "    # Combine and shuffle updates\n",
        "    updates = pd.concat([changed_df, new_df, deleted_df], ignore_index=True)\n",
        "    updates = updates.sample(frac=1, random_state=seed).reset_index(drop=True)  # Shuffle rows\n",
        "    # No need to process timestamps again—they are already UTC tz-aware\n",
        "    return updates\n",
        "\n",
        "updates_df = generate_updates(base_df, change_pct=schema_config.get('change_fraction', 0.01) if isinstance(schema_config, dict) else 0.01,\n",
        "                             new_pct=schema_config.get('new_fraction', 0.005) if isinstance(schema_config, dict) else 0.005,\n",
        "                             delete_pct=schema_config.get('delete_fraction', 0.001) if isinstance(schema_config, dict) else 0.001,\n",
        "                             seed=seed)\n",
        "print(f\"Updates dataframe created: rows={len(updates_df)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- 6. Save Updates (All Change Types, Shuffled)\n",
        "Write the updates parquet file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "os.makedirs(updates_output_folder, exist_ok=True)\n",
        "updates_file = f\"{updates_output_folder}updates_{row_count}_{format}.parquet\"\n",
        "updates_df.to_parquet(updates_file, coerce_timestamps='ms', engine='pyarrow')\n",
        "print(f\"Updates saved: {updates_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- 7. Metadata Logging\n",
        "Log simple metadata about the files created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def log_metadata(file_path, row_count, format, distribution, update_type):\n",
        "    import json\n",
        "    meta = {\n",
        "        \"file\": file_path,\n",
        "        \"rows\": row_count,\n",
        "        \"format\": format,\n",
        "        \"distribution\": distribution,\n",
        "        \"update_type\": update_type,\n",
        "        \"timestamp\": pd.Timestamp.now().isoformat()\n",
        "    }\n",
        "    meta_file = file_path.replace(\".parquet\", \".meta.json\")\n",
        "    with open(meta_file, \"w\") as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "    print(f\"Metadata logged: {meta_file}\")\n",
        "\n",
        "log_metadata(base_file, row_count, format, distribution, \"base\")\n",
        "log_metadata(updates_file, len(updates_df), format, distribution, \"batch_updates\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}

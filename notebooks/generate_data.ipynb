{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ],
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "  \"conf\": {\n",
        "    \"spark.notebook.parameters\": \"{\\\"DATASETS_PARAM\\\": [{\\\"name\\\": \\\"10k\\\", \\\"row_count\\\": 10000, \\\"change_fraction\\\": 0.01, \\\"new_fraction\\\": 0.005, \\\"delete_fraction\\\": 0.001, \\\"seed\\\": 42, \\\"description\\\": \\\"Small baseline dataset (10k rows)\\\"}, {\\\"name\\\": \\\"1m\\\", \\\"row_count\\\": 1000000, \\\"change_fraction\\\": 0.01, \\\"new_fraction\\\": 0.005, \\\"delete_fraction\\\": 0.001, \\\"seed\\\": 42, \\\"description\\\": \\\"Scale test dataset (1M rows)\\\"}], \\\"PUSH_TO_AZURE_SQL\\\": true, \\\"AZURE_SQL_SERVER\\\": \\\"benchmarking-bff\\\", \\\"AZURE_SQL_DB\\\": \\\"benchmarking\\\", \\\"AZURE_SQL_SCHEMA\\\": \\\"dbo\\\", \\\"distribution\\\": \\\"uniform\\\", \\\"seed\\\": 42}\"\n",
        "  },\n",
        "  \"defaultLakehouse\": {\n",
        "    \"name\": \"DataSourceLakehouse\"\n",
        "  }\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 0. Generate Data\n",
        "## Minimal parquet writer + Azure SQL push\n",
        "\n",
        "Parameters are supplied via spark.notebook.parameters in the %%configure cell. Token-based Azure SQL pushes use mssparkutils.credentials.getToken so no secrets or connection string credentials are required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "# Minimal schema_config used by the generator\n",
        "schema_config = {\n",
        "    \"categorical_fields\": 3,\n",
        "    \"numeric_fields\": 5,\n",
        "    \"timestamp_fields\": 2\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "# Unpack parameters from SparkConf (set via the %%configure -f cell). Keep this strict: no hidden defaults.\n",
        "import json\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "conf_key = \"spark.notebook.parameters\"\n",
        "conf_str = None\n",
        "try:\n",
        "    conf_str = spark.conf.get(conf_key, None)\n",
        "except Exception:\n",
        "    conf_str = None\n",
        "if not conf_str:\n",
        "    try:\n",
        "        conf_str = spark.sparkContext.getConf().get(conf_key, None)\n",
        "    except Exception:\n",
        "        conf_str = None\n",
        "if not conf_str:\n",
        "    raise RuntimeError(\"spark.notebook.parameters not found in SparkConf. Ensure the %%configure -f cell ran and the session was restarted.\")\n",
        "\n",
        "params = json.loads(conf_str)\n",
        "\n",
        "# Assign only what's passed in; let missing/malformed values raise errors upstream.\n",
        "DATASETS_PARAM = params[\"DATASETS_PARAM\"]\n",
        "PUSH_TO_AZURE_SQL = params.get(\"PUSH_TO_AZURE_SQL\", False)\n",
        "AZURE_SQL_SERVER = params.get(\"AZURE_SQL_SERVER\")\n",
        "AZURE_SQL_DB = params.get(\"AZURE_SQL_DB\")\n",
        "AZURE_SQL_SCHEMA = params.get(\"AZURE_SQL_SCHEMA\", \"dbo\")\n",
        "distribution = params.get(\"distribution\", \"uniform\")\n",
        "seed = params.get(\"seed\", 42)\n",
        "\n",
        "# Ensure AZURE_SQL_SERVER is a fully-qualified host name for Azure SQL (add .database.windows.net if omitted)\n",
        "if AZURE_SQL_SERVER:\n",
        "    if not AZURE_SQL_SERVER.lower().endswith(\".database.windows.net\"):\n",
        "        AZURE_SQL_SERVER = AZURE_SQL_SERVER.rstrip(\".\") + \".database.windows.net\"\n",
        "\n",
        "# Optional quick DNS/TCP check for visibility (non-fatal; logs a warning but doesn't stop the notebook)\n",
        "try:\n",
        "    import socket\n",
        "    _info = socket.getaddrinfo(AZURE_SQL_SERVER, 1433)\n",
        "    # quick TCP connection test (short timeout)\n",
        "    s = socket.create_connection((AZURE_SQL_SERVER, 1433), timeout=5)\n",
        "    s.close()\n",
        "    print(f\"AZURE_SQL_SERVER resolved and TCP 1433 OK: {AZURE_SQL_SERVER}\")\n",
        "except Exception as _e:\n",
        "    # Non-fatal: warn the user so they can fix parameters or DNS if needed\n",
        "    print(\"AZURE_SQL_SERVER DNS/TCP check warning:\", _e)\n",
        "\n",
        "print(f\"Loaded parameters: datasets={[d['name'] for d in DATASETS_PARAM]}, PUSH_TO_AZURE_SQL={PUSH_TO_AZURE_SQL}, AZURE_SQL_SERVER={AZURE_SQL_SERVER}, AZURE_SQL_DB={AZURE_SQL_DB}\")\n",
        "\n",
        "globals().update({\n",
        "    \"DATASETS_PARAM\": DATASETS_PARAM,\n",
        "    \"PUSH_TO_AZURE_SQL\": PUSH_TO_AZURE_SQL,\n",
        "    \"AZURE_SQL_SERVER\": AZURE_SQL_SERVER,\n",
        "    \"AZURE_SQL_DB\": AZURE_SQL_DB,\n",
        "    \"AZURE_SQL_SCHEMA\": AZURE_SQL_SCHEMA,\n",
        "    \"distribution\": distribution,\n",
        "    \"seed\": seed\n",
        "})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import pytz\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "print('Imports OK — proceeding with generation')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Helper: ensure timestamp columns are timezone-naive UTC before writing/parquet or pushing to SQL\n",
        "import datetime as _datetime\n",
        "def ensure_naive_utc(df_in):\n",
        "    \"\"\"Convert any ts_* columns to timezone-naive UTC datetimes in-place and return df.\n",
        "    - If column is tz-aware: convert to UTC then drop tzinfo.\n",
        "    - If column is naive: assume it's UTC (leave as naive) but coerce to datetime.\n",
        "    \"\"\"\n",
        "    for col in list(df_in.columns):\n",
        "        if col.startswith(\"ts_\"):\n",
        "            # coerce to datetimes first\n",
        "            s = pd.to_datetime(df_in[col], errors=\"coerce\")\n",
        "            # If series is tz-aware, convert to UTC then drop tzinfo -> naive UTC\n",
        "            try:\n",
        "                if getattr(s.dt, \"tz\", None) is not None:\n",
        "                    s = s.dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
        "            except Exception:\n",
        "                # Fallback per-value (handles mixed or unexpected types)\n",
        "                def _to_naive_utc(v):\n",
        "                    if pd.isna(v):\n",
        "                        return v\n",
        "                    if getattr(v, 'tzinfo', None) is not None:\n",
        "                        return v.astimezone(_datetime.timezone.utc).replace(tzinfo=None)\n",
        "                    return v\n",
        "                s = s.apply(_to_naive_utc)\n",
        "            # If naive, assume already UTC; store back\n",
        "            df_in[col] = s\n",
        "    return df_in\n",
        "\n",
        "print('ensure_naive_utc helper ready')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_synthetic_schema(config):\n",
        "    # deterministic seeds are set outside so runs are reproducible when seed is provided\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    schema = {}\n",
        "    for i in range(config[\"categorical_fields\"]):\n",
        "        schema[f\"cat_{i+1}\"] = lambda n, _choices=['A','B','C','D']: np.random.choice(_choices, size=n)\n",
        "    for i in range(config[\"numeric_fields\"]):\n",
        "        schema[f\"num_{i+1}\"] = lambda n: np.random.uniform(0, 1000, size=n)\n",
        "    for i in range(config[\"timestamp_fields\"]):\n",
        "        # generate naive timestamps; we'll localize to UTC in the dataframe to ensure timezone-compatibility\n",
        "        start = pd.Timestamp(\"2023-01-01\")\n",
        "        schema[f\"ts_{i+1}\"] = lambda n: pd.date_range(start, periods=n, freq=\"ms\")\n",
        "    return schema\n",
        "\n",
        "schema = create_synthetic_schema(schema_config)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate_base_dataframe(row_count, schema, distribution, seed):\n",
        "    \"\"\"Parameters are required — no defaults. Prints inputs for traceability.\n",
        "    This version enforces dtypes at generation time so downstream push logic can be minimal.\n",
        "    \"\"\"\n",
        "    print(f\"generate_base_dataframe called with row_count={row_count}, distribution={distribution}, seed={seed}\")\n",
        "    np.random.seed(seed)\n",
        "    data = {}\n",
        "    for col, gen in schema.items():\n",
        "        if \"num\" in col and distribution == \"skewed\":\n",
        "            data[col] = np.random.exponential(500, size=row_count)\n",
        "        elif \"num\" in col and distribution == \"null_injection\":\n",
        "            col_data = gen(row_count)\n",
        "            null_mask = np.random.rand(row_count) < 0.05\n",
        "            col_data[null_mask] = np.nan\n",
        "            data[col] = col_data\n",
        "        else:\n",
        "            data[col] = gen(row_count)\n",
        "    df = pd.DataFrame(data)\n",
        "    # enforce id as int64\n",
        "    df[\"id\"] = np.arange(1, row_count + 1).astype(\"int64\")\n",
        "    # Timestamps: coerce and ensure timezone-awareness where necessary (generator intent) — will be normalized to naive UTC before write\n",
        "    for col in df.columns:\n",
        "        if col.startswith(\"ts_\"):\n",
        "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "            # if naive, localize to UTC (we'll normalize to naive UTC later)\n",
        "            if getattr(df[col].dt, 'tz', None) is None:\n",
        "                df[col] = df[col].dt.tz_localize(pytz.UTC)\n",
        "    # Numeric fields -> float64 (NaN used for missing)\n",
        "    for col in df.columns:\n",
        "        if col.startswith(\"num_\"):\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce').astype(\"float64\")\n",
        "    # Categorical fields -> pandas string dtype (keeps NA behavior)\n",
        "    for col in df.columns:\n",
        "        if col.startswith(\"cat_\"):\n",
        "            df[col] = df[col].astype(\"string\")\n",
        "    df[\"update_type\"] = \"insert\"\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate_updates(df, change_pct, new_pct, delete_pct, seed):\n",
        "    \"\"\"Parameters are required — no defaults. Prints inputs for traceability.\n",
        "    Generates changed rows, new inserted rows, and delete marker rows that preserve column structure.\n",
        "    \"\"\"\n",
        "    print(f\"generate_updates called with rows={len(df)}, change_pct={change_pct}, new_pct={new_pct}, delete_pct={delete_pct}, seed={seed}\")\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    n = len(df)\n",
        "\n",
        "    # changed rows\n",
        "    changed_count = int(n * change_pct)\n",
        "    if changed_count > 0:\n",
        "        change_idx = np.random.choice(df.index, changed_count, replace=False)\n",
        "        changed_df = df.loc[change_idx].copy()\n",
        "        if \"num_1\" in changed_df.columns:\n",
        "            changed_df[\"num_1\"] = changed_df[\"num_1\"] + np.random.uniform(1, 10, changed_count)\n",
        "        changed_df[\"update_type\"] = \"update\"\n",
        "    else:\n",
        "        changed_df = pd.DataFrame(columns=df.columns.tolist() + [\"update_type\"])[:0]\n",
        "\n",
        "    # new rows (inserts)\n",
        "    new_count = int(n * new_pct)\n",
        "    if new_count > 0:\n",
        "        new_df = df.sample(new_count).copy()\n",
        "        new_df[\"id\"] = np.arange(n + 1, n + new_count + 1)\n",
        "        new_df[\"update_type\"] = \"insert\"\n",
        "    else:\n",
        "        new_df = pd.DataFrame(columns=df.columns.tolist() + [\"update_type\"])[:0]\n",
        "\n",
        "    # deletes: produce rows with same columns but only id and update_type populated; others NA\n",
        "    delete_count = int(n * delete_pct)\n",
        "    if delete_count > 0:\n",
        "        delete_idx = np.random.choice(df.index, delete_count, replace=False)\n",
        "        # create a DataFrame with the same columns, filled with NA, then populate id and update_type\n",
        "        deleted_df = pd.DataFrame({c: [pd.NA] * delete_count for c in df.columns})\n",
        "        # set id values for deleted rows (preserve order)\n",
        "        deleted_df[\"id\"] = df.loc[delete_idx, \"id\"].values\n",
        "        deleted_df[\"update_type\"] = \"delete\"\n",
        "        # ensure column order matches original df and include update_type\n",
        "        if \"update_type\" not in df.columns:\n",
        "            # maintain original df columns then append update_type at the end\n",
        "            deleted_df = deleted_df[df.columns.tolist()]\n",
        "            deleted_df[\"update_type\"] = \"delete\"\n",
        "    else:\n",
        "        deleted_df = pd.DataFrame(columns=df.columns.tolist() + [\"update_type\"])[:0]\n",
        "\n",
        "    # combine and shuffle\n",
        "    updates = pd.concat([changed_df, new_df, deleted_df], ignore_index=True, sort=False)\n",
        "    if not updates.empty:\n",
        "        updates = updates.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "    return updates\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Concise push helper that trusts generation-time schema enforcement\n",
        "import struct\n",
        "import time\n",
        "import datetime\n",
        "import pyodbc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from notebookutils import mssparkutils\n",
        "\n",
        "SQL_COPT_SS_ACCESS_TOKEN = 1256\n",
        "\n",
        "def _token_struct():\n",
        "    t = mssparkutils.credentials.getToken(\"https://database.windows.net/\")\n",
        "    exptoken = b\"\".join(bytes([c]) + b\"\\x00\" for c in t.encode(\"utf-8\"))\n",
        "    return struct.pack(\"=i\", len(exptoken)) + exptoken\n",
        "\n",
        "def _col_type_from_name(col):\n",
        "    if col == \"id\":\n",
        "        return \"BIGINT\"\n",
        "    if col.startswith(\"num_\"):\n",
        "        return \"FLOAT\"\n",
        "    if col.startswith(\"cat_\"):\n",
        "        return \"NVARCHAR(100)\"\n",
        "    if col.startswith(\"ts_\"):\n",
        "        # Use DATETIME2 (no timezone) for broader client compatibility during fast ingestion\n",
        "        return \"DATETIME2\"\n",
        "    if col == \"update_type\":\n",
        "        return \"NVARCHAR(32)\"\n",
        "    return \"NVARCHAR(MAX)\"\n",
        "\n",
        "def _py_val(v):\n",
        "    if pd.isna(v):\n",
        "        return None\n",
        "    if isinstance(v, pd.Timestamp):\n",
        "        # convert pandas Timestamp to timezone-naive UTC datetime for DATETIME2 column\n",
        "        dt = v.to_pydatetime()\n",
        "        if getattr(dt, 'tzinfo', None) is not None:\n",
        "            # normalize to UTC then drop tzinfo\n",
        "            dt = dt.astimezone(datetime.timezone.utc).replace(tzinfo=None)\n",
        "        return dt\n",
        "    if isinstance(v, (np.integer, np.int64, np.int32)):\n",
        "        return int(v)\n",
        "    if isinstance(v, (np.floating, np.float64, np.float32)):\n",
        "        return float(v)\n",
        "    if isinstance(v, (np.bool_,)):\n",
        "        return bool(v)\n",
        "    return v\n",
        "\n",
        "def _pyodbc_conn_with_retry(server=None, database=None, timeout=120, retries=2, backoff=2):\n",
        "    server = server or globals().get(\"AZURE_SQL_SERVER\")\n",
        "    database = database or globals().get(\"AZURE_SQL_DB\")\n",
        "    if not server or not database:\n",
        "        raise RuntimeError(\"AZURE_SQL_SERVER and AZURE_SQL_DB must be set (or passed in)\")\n",
        "    conn_str = (\n",
        "        \"Driver={ODBC Driver 18 for SQL Server};\"\n",
        "        f\"Server=tcp:{server},1433;\"\n",
        "        f\"Database={database};\"\n",
        "        \"Encrypt=yes;TrustServerCertificate=no;\"\n",
        "    )\n",
        "    last_exc = None\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            return pyodbc.connect(conn_str, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: _token_struct()}, timeout=timeout)\n",
        "        except Exception as e:\n",
        "            last_exc = e\n",
        "            if attempt < retries:\n",
        "                time.sleep(backoff * attempt)\n",
        "            else:\n",
        "                raise\n",
        "    raise last_exc\n",
        "\n",
        "def push_df_concise(df, table_name, schema_name=\"dbo\", server=None, database=None):\n",
        "    \"\"\"Push dataframe to Azure SQL using concise, schema-driven rules.\n",
        "    Assumes generate_base_dataframe and generate_updates have already enforced dtypes.\n",
        "    Returns number of rows inserted.\n",
        "    \"\"\"\n",
        "    df2 = df.copy()\n",
        "    # minimal sanitization: empty-string -> NA\n",
        "    for c in df2.columns:\n",
        "        if df2[c].dtype == object:\n",
        "            df2[c] = df2[c].replace(\"\", pd.NA)\n",
        "    # DDL from naming convention\n",
        "    cols_ddl = [f\"[{c}] {_col_type_from_name(c)} NULL\" for c in df2.columns]\n",
        "    full_table = f\"{schema_name}.{table_name}\"\n",
        "    create_sql = f\"IF OBJECT_ID(N'{full_table}', 'U') IS NOT NULL DROP TABLE {full_table}; CREATE TABLE {full_table} ({', '.join(cols_ddl)});\"\n",
        "    conn = _pyodbc_conn_with_retry(server=server, database=database)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(create_sql)\n",
        "    conn.commit()\n",
        "    col_names = ['[' + c.replace('\"','') + ']' for c in df2.columns]\n",
        "    placeholders = \", \".join(\"?\" for _ in col_names)\n",
        "    insert_sql = f\"INSERT INTO {full_table} ({', '.join(col_names)}) VALUES ({placeholders})\"\n",
        "    records = [tuple(_py_val(v) for v in row) for row in df2.itertuples(index=False, name=None)]\n",
        "    cur.fast_executemany = True\n",
        "    cur.executemany(insert_sql, records)\n",
        "    conn.commit()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    return len(records)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "# Token-based fast uploader helper for large datasets (uses mssparkutils token + pyodbc fast_executemany)\n",
        "import math\n",
        "import time\n",
        "import struct\n",
        "import datetime as _datetime\n",
        "import numpy as _np\n",
        "import pandas as _pd\n",
        "\n",
        "def upload_df_token_fast(df, table_name, schema_name=\"dbo\", server=None, database=None, batch_size=5000, driver='ODBC Driver 18 for SQL Server'):\n",
        "    \"\"\"Upload a pandas DataFrame to Azure SQL using token-based pyodbc and batched fast_executemany.\n",
        "    - Drops and recreates the target table to match df columns.\n",
        "    - Uses mssparkutils.credentials.getToken for an AAD token and passes it via attrs_before.\n",
        "    - Batches inserts to reduce memory and commit overhead.\n",
        "    Returns number of rows inserted.\n",
        "    \"\"\"\n",
        "    # local imports to keep global namespace clean\n",
        "    from notebookutils import mssparkutils\n",
        "    import pyodbc\n",
        "\n",
        "    server = server or globals().get('AZURE_SQL_SERVER')\n",
        "    database = database or globals().get('AZURE_SQL_DB')\n",
        "    if not server or not database:\n",
        "        raise RuntimeError('server and database must be provided or set in globals')\n",
        "\n",
        "    # normalize timestamp columns to timezone-naive UTC (reuse ensure_naive_utc logic if present)\n",
        "    try:\n",
        "        # try to use ensure_naive_utc defined earlier\n",
        "        df = ensure_naive_utc(df)\n",
        "    except Exception:\n",
        "        # fallback localized conversion\n",
        "        for c in df.columns:\n",
        "            if c.startswith('ts_'):\n",
        "                s = _pd.to_datetime(df[c], errors='coerce')\n",
        "                try:\n",
        "                    if getattr(s.dt, 'tz', None) is not None:\n",
        "                        s = s.dt.tz_convert('UTC').dt.tz_localize(None)\n",
        "                except Exception:\n",
        "                    def _to_naive_utc(v):\n",
        "                        if _pd.isna(v):\n",
        "                            return v\n",
        "                        if getattr(v, 'tzinfo', None) is not None:\n",
        "                            return v.astimezone(_datetime.timezone.utc).replace(tzinfo=None)\n",
        "                        return v\n",
        "                    s = s.apply(_to_naive_utc)\n",
        "                df[c] = s\n",
        "\n",
        "    # build DDL\n",
        "    def _col_type(col):\n",
        "        if col == 'id':\n",
        "            return 'BIGINT'\n",
        "        if col.startswith('num_'):\n",
        "            return 'FLOAT'\n",
        "        if col.startswith('cat_'):\n",
        "            return 'NVARCHAR(100)'\n",
        "        if col.startswith('ts_'):\n",
        "            return 'DATETIME2'\n",
        "        if col == 'update_type':\n",
        "            return 'NVARCHAR(32)'\n",
        "        return 'NVARCHAR(MAX)'\n",
        "\n",
        "    cols_ddl = ', '.join(f'[{c}] {_col_type(c)} NULL' for c in df.columns)\n",
        "    full_table = f\"{schema_name}.{table_name}\"\n",
        "    create_sql = f\"IF OBJECT_ID(N'{full_table}', 'U') IS NOT NULL DROP TABLE {full_table}; CREATE TABLE {full_table} ({cols_ddl});\"\n",
        "\n",
        "    # get token and build token struct for pyodbc\n",
        "    t = mssparkutils.credentials.getToken('https://database.windows.net/')\n",
        "    exptoken = b\"\".join(bytes([c]) + b\"\\x00\" for c in t.encode('utf-8'))\n",
        "    token_struct = struct.pack('=i', len(exptoken)) + exptoken\n",
        "\n",
        "    conn_str = (\n",
        "        f\"Driver={{{driver}}};\"\n",
        "        f\"Server=tcp:{server},1433;\"\n",
        "        f\"Database={database};\"\n",
        "        \"Encrypt=yes;TrustServerCertificate=no;\"\n",
        "        \"ConnectRetryCount=4;\"\n",
        "    )\n",
        "    # open connection\n",
        "    conn = pyodbc.connect(conn_str, attrs_before={1256: token_struct}, timeout=300)\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    # makes sure Azure SQL Database is awake\n",
        "    deadline = time.time() + 60\n",
        "    while time.time() < deadline:\n",
        "        try:\n",
        "            cur.execute(\"SELECT DB_NAME() AS db, SUSER_SNAME() AS user_name\")\n",
        "            row = cur.fetchone()\n",
        "            if row:\n",
        "                print(f\"    Verified SQL connection: db={row[0]}, user={row[1]}\")\n",
        "                break\n",
        "        except Exception:\n",
        "            time.sleep(5)\n",
        "    else:\n",
        "        raise RuntimeError(\"Timed out waiting for Azure SQL\")\n",
        "        \n",
        "    # create/replace table\n",
        "    cur.execute(create_sql)\n",
        "    conn.commit()\n",
        "\n",
        "    # prepare insert\n",
        "    col_names = ['[' + c.replace('\"','') + ']' for c in df.columns]\n",
        "    placeholders = ', '.join('?' for _ in col_names)\n",
        "    insert_sql = f\"INSERT INTO {full_table} ({', '.join(col_names)}) VALUES ({placeholders})\"\n",
        "\n",
        "    def _to_py(v):\n",
        "        if _pd.isna(v):\n",
        "            return None\n",
        "        if isinstance(v, _pd.Timestamp):\n",
        "            dt = v.to_pydatetime()\n",
        "            if getattr(dt, 'tzinfo', None) is not None:\n",
        "                dt = dt.astimezone(_datetime.timezone.utc).replace(tzinfo=None)\n",
        "            return dt\n",
        "        if isinstance(v, (_np.integer,)):\n",
        "            return int(v)\n",
        "        if isinstance(v, (_np.floating,)):\n",
        "            return float(v)\n",
        "        if isinstance(v, (_np.bool_,)):\n",
        "            return bool(v)\n",
        "        return v\n",
        "\n",
        "    total = len(df)\n",
        "    if total == 0:\n",
        "        cur.close(); conn.close(); return 0\n",
        "\n",
        "    cur.fast_executemany = True\n",
        "    inserted = 0\n",
        "    for i in range(0, total, batch_size):\n",
        "        batch = df.iloc[i:i+batch_size]\n",
        "        records = [tuple(_to_py(v) for v in row) for row in batch.itertuples(index=False, name=None)]\n",
        "        cur.executemany(insert_sql, records)\n",
        "        conn.commit()\n",
        "        inserted += len(records)\n",
        "        print(f\"Uploaded {inserted}/{total} rows to {full_table} (batch finished)\")\n",
        "\n",
        "    cur.close(); conn.close()\n",
        "    return inserted\n",
        "\n",
        "print('upload_df_token_fast helper loaded')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- Save Loop (per-dataset paths and filenames) — strict, no fallbacks or extra validation\n",
        "This loop uses DATASETS_PARAM directly and lets errors surface if keys are missing or malformed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "data_root = Path('data')\n",
        "\n",
        "for ds in DATASETS_PARAM:\n",
        "    name = ds['name']\n",
        "    row_count_ds = int(ds['row_count'])\n",
        "    change_fraction = float(ds['change_fraction'])\n",
        "    new_fraction = float(ds['new_fraction'])\n",
        "    delete_fraction = float(ds['delete_fraction'])\n",
        "    seed_ds = int(ds['seed'])\n",
        "\n",
        "    print(f\"\\n=== Generating dataset {name}: rows={row_count_ds}\")\n",
        "    base_output_folder = f\"/lakehouse/default/Files/{name}base/\"\n",
        "    updates_output_folder = f\"/lakehouse/default/Files/{name}updates/\"\n",
        "    local_dir = data_root / name\n",
        "    local_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    base_df = generate_base_dataframe(row_count_ds, schema, distribution, seed_ds)\n",
        "    # Normalize ts_* columns to timezone-naive UTC before writing/parquet or push\n",
        "    base_df = ensure_naive_utc(base_df)\n",
        "    base_file_local = local_dir / 'base.parquet'\n",
        "    base_df.to_parquet(str(base_file_local), engine='pyarrow', index=False, coerce_timestamps='ms')\n",
        "    print('  wrote local base:', base_file_local)\n",
        "\n",
        "    updates_df = generate_updates(base_df, change_fraction, new_fraction, delete_fraction, seed_ds)\n",
        "    updates_df = ensure_naive_utc(updates_df)\n",
        "    updates_file_local = local_dir / 'updates.parquet'\n",
        "    updates_df.to_parquet(str(updates_file_local), engine='pyarrow', index=False, coerce_timestamps='ms')\n",
        "    print('  wrote local updates:', updates_file_local)\n",
        "\n",
        "    # Best-effort lakehouse write using filesystem APIs — keep simple; don't attempt retries here.\n",
        "    try:\n",
        "        Path(base_output_folder).mkdir(parents=True, exist_ok=True)\n",
        "        (Path(base_output_folder) / 'base.parquet').write_bytes(base_file_local.read_bytes())\n",
        "        Path(updates_output_folder).mkdir(parents=True, exist_ok=True)\n",
        "        (Path(updates_output_folder) / 'updates.parquet').write_bytes(updates_file_local.read_bytes())\n",
        "        print('  wrote lakehouse copies ->', base_output_folder, updates_output_folder)\n",
        "    except Exception:\n",
        "        print('  could not write to lakehouse path (skipped)')\n",
        "\n",
        "    ctrl_dir = data_root / 'controller' / name\n",
        "    ctrl_dir.mkdir(parents=True, exist_ok=True)\n",
        "    (ctrl_dir / 'base.parquet').write_bytes(base_file_local.read_bytes())\n",
        "    (ctrl_dir / 'updates.parquet').write_bytes(updates_file_local.read_bytes())\n",
        "    print('  wrote controller copies ->', ctrl_dir)\n",
        "\n",
        "    # Azure SQL push (token-based preferred). Keep behavior explicit: replace tables for current run.\n",
        "    if PUSH_TO_AZURE_SQL:\n",
        "        # prefer token-based fast upload if mssparkutils is available\n",
        "        try:\n",
        "            try:\n",
        "                # test availability\n",
        "                from notebookutils import mssparkutils\n",
        "                has_token = True\n",
        "            except Exception:\n",
        "                has_token = False\n",
        "\n",
        "            if has_token:\n",
        "                print('  Using token-based fast upload for SQL (upload_df_token_fast)')\n",
        "                upload_df_token_fast(base_df, f\"base_{name}\", schema_name=AZURE_SQL_SCHEMA, server=AZURE_SQL_SERVER, database=AZURE_SQL_DB, batch_size=5000)\n",
        "                upload_df_token_fast(updates_df, f\"updates_{name}\", schema_name=AZURE_SQL_SCHEMA, server=AZURE_SQL_SERVER, database=AZURE_SQL_DB, batch_size=5000)\n",
        "            else:\n",
        "                print('  mssparkutils token not available; falling back to push_df_concise (pyodbc token attempt inside)')\n",
        "                push_df_concise(base_df, f\"base_{name}\", schema_name=AZURE_SQL_SCHEMA)\n",
        "                push_df_concise(updates_df, f\"updates_{name}\", schema_name=AZURE_SQL_SCHEMA)\n",
        "        except Exception as e:\n",
        "            print('  azure sql push failed (continuing):', e)\n",
        "\n",
        "print('\\nGeneration loop complete.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- Metadata Logging\n",
        "Log simple metadata about the files created. Kept minimal to avoid masking upstream errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def log_metadata(file_path, row_count_val, fmt, distribution_val, update_type):\n",
        "    import json\n",
        "    meta = {\n",
        "        \"file\": file_path,\n",
        "        \"rows\": row_count_val,\n",
        "        \"format\": fmt,\n",
        "        \"distribution\": distribution_val,\n",
        "        \"update_type\": update_type,\n",
        "        \"timestamp\": pd.Timestamp.now().isoformat()\n",
        "    }\n",
        "    meta_file = str(file_path).replace('.parquet', '.meta.json')\n",
        "    with open(meta_file, 'w') as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "    print(f\"Metadata logged: {meta_file}\")\n",
        "\n",
        "for ds in DATASETS_PARAM:\n",
        "    name = ds['name']\n",
        "    row_count_ds = int(ds['row_count'])\n",
        "    local_dir = data_root / name\n",
        "    base_file_local = local_dir / 'base.parquet'\n",
        "    updates_file_local = local_dir / 'updates.parquet'\n",
        "    if base_file_local.exists():\n",
        "        log_metadata(str(base_file_local), row_count_ds, 'parquet', distribution, 'base')\n",
        "    if updates_file_local.exists():\n",
        "        log_metadata(str(updates_file_local), len(pd.read_parquet(str(updates_file_local), engine='pyarrow')), 'parquet', distribution, 'batch_updates')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}

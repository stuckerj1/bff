{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ],
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "  \"conf\": {\n",
        "    \"spark.notebook.parameters\": \"{\\\"DATASETS_PARAM\\\": [{\\\"name\\\": \\\"1k\\\", \\\"row_count\\\": 1000, \\\"change_fraction\\\": 0.01, \\\"new_fraction\\\": 0.005, \\\"delete_fraction\\\": 0.001, \\\"seed\\\": 42, \\\"description\\\": \\\"Interactive small dataset (1k rows)\\\"}, {\\\"name\\\": \\\"100k\\\", \\\"row_count\\\": 100000, \\\"change_fraction\\\": 0.01, \\\"new_fraction\\\": 0.005, \\\"delete_fraction\\\": 0.001, \\\"seed\\\": 42, \\\"description\\\": \\\"Interactive medium dataset (100k rows)\\\"}], \\\"PUSH_TO_AZURE_SQL\\\": true, \\\"AZURE_SQL_SERVER\\\": \\\"benchmarking-bff\\\", \\\"AZURE_SQL_DB\\\": \\\"benchmarking\\\", \\\"AZURE_SQL_CONNECTION\\\": \\\"Driver={ODBC Driver 18 for SQL Server};Server=tcp:benchmarking-bff.database.windows.net,1433;Database=benchmarking;Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30\\\", \\\"AZURE_SQL_SCHEMA\\\": \\\"dbo\\\", \\\"distribution\\\": \\\"uniform\\\", \\\"seed\\\": 42}\"\n",
        "  },\n",
        "  \"defaultLakehouse\": {\n",
        "    \"name\": \"DataSourceLakehouse\"\n",
        "  }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Data — minimal parquet writer (per-dataset paths) + optional SQL copy\n",
        "\n",
        "This notebook is intentionally minimal and focused. It loops DATASETS_PARAM, generates a base and updates parquet per dataset, writes per-dataset lakehouse paths (e.g. /lakehouse/default/Files/{name}base/), and also writes controller/local copies under data/{name}/. Filenames are base.parquet and updates.parquet."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "# Minimal schema_config used by the generator (kept outside the parameters cell so the first cell remains exact)\n",
        "schema_config = {\n",
        "    \"categorical_fields\": 3,\n",
        "    \"numeric_fields\": 5,\n",
        "    \"timestamp_fields\": 2\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "# Unpack parameters\n",
        "import json\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create/get SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Try to read the parameters string from SparkConf\n",
        "conf_key = \"spark.notebook.parameters\"\n",
        "conf_str = None\n",
        "\n",
        "# Prefer spark.conf, but fall back to sparkContext.conf\n",
        "try:\n",
        "    conf_str = spark.conf.get(conf_key, None)\n",
        "except Exception:\n",
        "    # spark.conf.get may raise if not present; try sparkContext\n",
        "    conf_str = None\n",
        "\n",
        "if not conf_str:\n",
        "    try:\n",
        "        conf_str = spark.sparkContext.getConf().get(conf_key, None)\n",
        "    except Exception:\n",
        "        conf_str = None\n",
        "\n",
        "if not conf_str:\n",
        "    raise RuntimeError(\n",
        "        \"spark.notebook.parameters not found in SparkConf. \"\n",
        "        \"Ensure the %%configure -f cell ran and the session was restarted.\"\n",
        "    )\n",
        "\n",
        "# Parse the JSON payload\n",
        "params = json.loads(conf_str)\n",
        "\n",
        "# Unpack parameters into notebook-level variables\n",
        "DATASETS_PARAM = params[\"DATASETS_PARAM\"]\n",
        "PUSH_TO_AZURE_SQL = params.get(\"PUSH_TO_AZURE_SQL\", False)\n",
        "AZURE_SQL_CONNECTION = params.get(\"AZURE_SQL_CONNECTION\", \"\")\n",
        "AZURE_SQL_SCHEMA = params.get(\"AZURE_SQL_SCHEMA\", \"dbo\")\n",
        "AZURE_SQL_SERVER = params.get(\"AZURE_SQL_SERVER\", \"\")\n",
        "AZURE_SQL_DB = params.get(\"AZURE_SQL_DB\", \"\")\n",
        "distribution = params.get(\"distribution\", \"uniform\")\n",
        "seed = params.get(\"seed\", 42)\n",
        "\n",
        "# Print a brief summary so you can verify everything is wired correctly\n",
        "print(\"Loaded parameters from SparkConf:\")\n",
        "print(f\"  datasets: {[d['name'] for d in DATASETS_PARAM]}\")\n",
        "print(f\"  PUSH_TO_AZURE_SQL: {PUSH_TO_AZURE_SQL}\")\n",
        "print(f\"  AZURE_SQL_SERVER: {AZURE_SQL_SERVER}\")\n",
        "print(f\"  AZURE_SQL_DB: {AZURE_SQL_DB}\")\n",
        "print(f\"  AZURE_SQL_CONNECTION: {AZURE_SQL_CONNECTION}\")\n",
        "print(f\"  AZURE_SQL_SCHEMA: {AZURE_SQL_SCHEMA}\")\n",
        "print(f\"  distribution: {distribution}\")\n",
        "print(f\"  seed: {seed}\")\n",
        "\n",
        "# (Optionally) expose to globals for the notebook runtime\n",
        "globals().update({\n",
        "    \"DATASETS_PARAM\": DATASETS_PARAM,\n",
        "    \"PUSH_TO_AZURE_SQL\": PUSH_TO_AZURE_SQL,\n",
        "    \"AZURE_SQL_CONNECTION\": AZURE_SQL_CONNECTION,\n",
        "    \"AZURE_SQL_SCHEMA\": AZURE_SQL_SCHEMA,\n",
        "    \"AZURE_SQL_SERVER\": AZURE_SQL_SERVER,\n",
        "    \"AZURE_SQL_DB\": AZURE_SQL_DB,\n",
        "    \"distribution\": distribution,\n",
        "    \"seed\": seed\n",
        "})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import pytz\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "print('Imports OK — proceeding with generation')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- Schema Definition\n",
        "Create a small synthetic schema generator (categorical, numeric, timestamps)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "def create_synthetic_schema(config):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    schema = {}\n",
        "    # Categorical fields\n",
        "    for i in range(config[\"categorical_fields\"]):\n",
        "        schema[f\"cat_{i+1}\"] = lambda n, _choices=['A','B','C','D']: np.random.choice(_choices, size=n)\n",
        "    # Numeric fields\n",
        "    for i in range(config[\"numeric_fields\"]):\n",
        "        schema[f\"num_{i+1}\"] = lambda n: np.random.uniform(0, 1000, size=n)\n",
        "    # Timestamp fields\n",
        "    for i in range(config[\"timestamp_fields\"]):\n",
        "        start = pd.Timestamp(\"2023-01-01\")\n",
        "        schema[f\"ts_{i+1}\"] = lambda n: pd.date_range(start, periods=n, freq=\"ms\")\n",
        "    return schema\n",
        "\n",
        "schema = create_synthetic_schema(schema_config)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- Synthetic Data Generation\n",
        "Generate the base dataframe from the schema and the selected distribution."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "def generate_base_dataframe(row_count, schema, distribution, seed):\n",
        "    \"\"\"Generate base dataframe. Parameters are required (no defaults).\"\"\"\n",
        "    # Print input parameters for visibility\n",
        "    print(f\"generate_base_dataframe called with row_count={row_count}, distribution={distribution}, seed={seed}\")\n",
        "    np.random.seed(seed)\n",
        "    data = {}\n",
        "    for col, gen in schema.items():\n",
        "        if \"num\" in col and distribution == \"skewed\":\n",
        "            data[col] = np.random.exponential(500, size=row_count)\n",
        "        elif \"num\" in col and distribution == \"null_injection\":\n",
        "            col_data = gen(row_count)\n",
        "            null_mask = np.random.rand(row_count) < 0.05\n",
        "            col_data[null_mask] = np.nan\n",
        "            data[col] = col_data\n",
        "        else:\n",
        "            data[col] = gen(row_count)\n",
        "    df = pd.DataFrame(data)\n",
        "    df[\"id\"] = np.arange(1, row_count + 1)\n",
        "    # Ensure timestamp columns are pandas datetime64[ms] and timezone-aware (UTC)\n",
        "    for col in df.columns:\n",
        "        if col.startswith(\"ts_\"):\n",
        "            df[col] = pd.to_datetime(df[col]).dt.tz_localize(pytz.UTC)\n",
        "    # Add update_type column, always 'insert' for base\n",
        "    df[\"update_type\"] = \"insert\"\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- Generate Updates (function)\n",
        "Create updates (updates, inserts, deletes) and shuffle them."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "def generate_updates(df, change_pct, new_pct, delete_pct, seed):\n",
        "    \"\"\"Generate updates dataframe. Parameters are required (no defaults).\"\"\"\n",
        "    # Print input parameters for visibility\n",
        "    print(f\"generate_updates called with rows={len(df)}, change_pct={change_pct}, new_pct={new_pct}, delete_pct={delete_pct}, seed={seed}\")\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    n = len(df)\n",
        "    # Changed rows\n",
        "    changed_count = int(n * change_pct)\n",
        "    change_idx = np.random.choice(df.index, changed_count, replace=False)\n",
        "    changed_df = df.loc[change_idx].copy()\n",
        "    # Example modification of one numeric column if present\n",
        "    if \"num_1\" in changed_df.columns:\n",
        "        changed_df[\"num_1\"] = changed_df[\"num_1\"] + np.random.uniform(1, 10, changed_count)\n",
        "    changed_df[\"update_type\"] = \"update\"\n",
        "    # New rows\n",
        "    new_count = int(n * new_pct)\n",
        "    new_df = df.sample(new_count).copy()\n",
        "    new_df[\"id\"] = np.arange(n + 1, n + new_count + 1)\n",
        "    new_df[\"update_type\"] = \"insert\"\n",
        "    # Deleted rows\n",
        "    delete_count = int(n * delete_pct)\n",
        "    delete_idx = np.random.choice(df.index, delete_count, replace=False)\n",
        "    deleted_df = df.loc[delete_idx][[\"id\"]].copy()\n",
        "    deleted_df[\"update_type\"] = \"delete\"\n",
        "    # Combine and shuffle updates\n",
        "    updates = pd.concat([changed_df, new_df, deleted_df], ignore_index=True)\n",
        "    updates = updates.sample(frac=1, random_state=seed).reset_index(drop=True)  # Shuffle rows\n",
        "    # No need to process timestamps again—they are already UTC tz-aware\n",
        "    return updates\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- Save Loop (per-dataset paths and filenames) — strict, no fallbacks or extra validation\n",
        "This loop uses DATASETS_PARAM directly and lets errors surface if keys are missing or malformed."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "from pathlib import Path\n",
        "data_root = Path('data')\n",
        "\n",
        "for ds in DATASETS_PARAM:\n",
        "    # Use the dataset fields directly; let errors surface if keys are missing or invalid\n",
        "    name = ds['name']\n",
        "    row_count_ds = int(ds['row_count'])\n",
        "    change_fraction = float(ds['change_fraction'])\n",
        "    new_fraction = float(ds['new_fraction'])\n",
        "    delete_fraction = float(ds['delete_fraction'])\n",
        "    seed_ds = int(ds['seed'])\n",
        "\n",
        "    print(f\"\\n=== Generating dataset {name}: rows={row_count_ds}\")\n",
        "\n",
        "    # Per-dataset lakehouse paths (as requested)\n",
        "    base_output_folder = f\"/lakehouse/default/Files/{name}base/\"\n",
        "    updates_output_folder = f\"/lakehouse/default/Files/{name}updates/\"\n",
        "\n",
        "    # Local controller area (always write a local copy under data/{name}/)\n",
        "    local_dir = data_root / name\n",
        "    local_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Generate base and updates\n",
        "    base_df = generate_base_dataframe(row_count_ds, schema, distribution, seed_ds)\n",
        "    base_file_local = local_dir / 'base.parquet'\n",
        "    base_df.to_parquet(str(base_file_local), engine='pyarrow', index=False, coerce_timestamps='ms')\n",
        "    print('  wrote local base:', base_file_local)\n",
        "\n",
        "    updates_df = generate_updates(base_df, change_fraction, new_fraction, delete_fraction, seed_ds)\n",
        "    updates_file_local = local_dir / 'updates.parquet'\n",
        "    updates_df.to_parquet(str(updates_file_local), engine='pyarrow', index=False, coerce_timestamps='ms')\n",
        "    print('  wrote local updates:', updates_file_local)\n",
        "\n",
        "    # Best-effort: write lakehouse copies (may fail outside Fabric)\n",
        "    try:\n",
        "        Path(base_output_folder).mkdir(parents=True, exist_ok=True)\n",
        "        (Path(base_output_folder) / 'base.parquet').write_bytes(base_file_local.read_bytes())\n",
        "        Path(updates_output_folder).mkdir(parents=True, exist_ok=True)\n",
        "        (Path(updates_output_folder) / 'updates.parquet').write_bytes(updates_file_local.read_bytes())\n",
        "        print('  wrote lakehouse copies ->', base_output_folder, updates_output_folder)\n",
        "    except Exception:\n",
        "        print('  could not write to lakehouse path (running outside Fabric or no permission); skipped lakehouse writes')\n",
        "\n",
        "    # Controller copies (local controller area)\n",
        "    ctrl_dir = data_root / 'controller' / name\n",
        "    ctrl_dir.mkdir(parents=True, exist_ok=True)\n",
        "    (ctrl_dir / 'base.parquet').write_bytes(base_file_local.read_bytes())\n",
        "    (ctrl_dir / 'updates.parquet').write_bytes(updates_file_local.read_bytes())\n",
        "    print('  wrote controller copies ->', ctrl_dir)\n",
        "\n",
        "print('\\nGeneration loop complete.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- Metadata Logging\n",
        "Log simple metadata about the files created."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "source": [
        "def log_metadata(file_path, row_count_val, fmt, distribution_val, update_type):\n",
        "    import json\n",
        "    meta = {\n",
        "        \"file\": file_path,\n",
        "        \"rows\": row_count_val,\n",
        "        \"format\": fmt,\n",
        "        \"distribution\": distribution_val,\n",
        "        \"update_type\": update_type,\n",
        "        \"timestamp\": pd.Timestamp.now().isoformat()\n",
        "    }\n",
        "    meta_file = str(file_path).replace('.parquet', '.meta.json')\n",
        "    with open(meta_file, 'w') as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "    print(f\"Metadata logged: {meta_file}\")\n",
        "\n",
        "# Log metadata for each dataset's local files\n",
        "for ds in DATASETS_PARAM:\n",
        "    name = ds['name']\n",
        "    row_count_ds = int(ds['row_count'])\n",
        "    local_dir = data_root / name\n",
        "    base_file_local = local_dir / 'base.parquet'\n",
        "    updates_file_local = local_dir / 'updates.parquet'\n",
        "    if base_file_local.exists():\n",
        "        log_metadata(str(base_file_local), row_count_ds, 'parquet', distribution, 'base')\n",
        "    if updates_file_local.exists():\n",
        "        log_metadata(str(updates_file_local), len(pd.read_parquet(str(updates_file_local), engine='pyarrow')), 'parquet', distribution, 'batch_updates')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}

{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "# Notebook parameters (Papermill / Fabric friendly)\n",
        "# All values that may be overridden by an orchestrator or by interactive runs\n",
        "# are declared here in a single top-level parameters cell.  This makes it\n",
        "# easy to validate what the notebook actually used when invoked programmatically.\n",
        "\n",
        "# -- Datasets array (interactive defaults). Use Option A naming as requested.\n",
        "DATASETS_PARAM = [\n",
        "    {\n",
        "        \"name\": \"1k\",\n",
        "        \"row_count\": 1000,\n",
        "        \"change_fraction\": 0.01,\n",
        "        \"new_fraction\": 0.005,\n",
        "        \"delete_fraction\": 0.001,\n",
        "        \"seed\": 42,\n",
        "        \"description\": \"Interactive small dataset (1k rows)\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"100k\",\n",
        "        \"row_count\": 100000,\n",
        "        \"change_fraction\": 0.01,\n",
        "        \"new_fraction\": 0.005,\n",
        "        \"delete_fraction\": 0.001,\n",
        "        \"seed\": 42,\n",
        "        \"description\": \"Interactive medium dataset (100k rows)\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Where generated files will be written locally (or in the notebook filesystem)\n",
        "OUTPUT_DIR = \"data\"\n",
        "\n",
        "# Path to repository parameter_sets config (used when orchestrator doesn't pass datasets)\n",
        "CONFIG_PATH = \"config/parameter_sets.yml\"\n",
        "\n",
        "# When orchestrators call the notebook they should pass the datasets array under this input name\n",
        "# (mssparkutils.notebook.getContext().getInput('datasets') or papermill 'datasets')\n",
        "NOTEBOOK_PARAM_INPUT_NAME = \"datasets\"\n",
        "\n",
        "# Controller copy behavior: write controller copies under OUTPUT_DIR/controller/{dataset}/\n",
        "GENERATE_CONTROLLER_COPY = True\n",
        "\n",
        "# Fabric Files destinations (templates). The notebook will attempt to write to these\n",
        "# paths when running inside Fabric with mssparkutils available. Use {dataset} as placeholder.\n",
        "FILES_BASE_TEMPLATE = \"/Files/{dataset}base/\"\n",
        "FILES_UPDATES_TEMPLATE = \"/Files/{dataset}updates/\"\n",
        "\n",
        "# Azure SQL / ODBC parameters (interactive defaults). Orchestrators should pass secure\n",
        "# connection strings via secrets and override SQL_PUSH=True and SQL_ODBC_CONNECTION.\n",
        "SQL_PUSH = False\n",
        "SQL_ODBC_CONNECTION = \"\"   # Example: \"Driver={ODBC Driver 17 for SQL Server};Server=tcp:<server>.database.windows.net,1433;Database=<db>;UID=<user>;PWD=<password>;Encrypt=yes;\"\n",
        "SQL_SCHEMA = \"dbo\"\n",
        "SQL_CHUNKSIZE = 10000\n",
        "SQL_IF_EXISTS = \"replace\"   # options: 'replace','append','fail'\n",
        "\n",
        "# CSV / write options\n",
        "CSV_DELIMITER = \",\"\n",
        "\n",
        "# Generator defaults (can be overridden per-dataset in the datasets array)\n",
        "GENERATOR_DEFAULT_SEED = 42\n",
        "\n",
        "# Safety / behavior toggles\n",
        "SKIP_FILES_COPY_ON_ERROR = True   # don't hard-fail if mssparkutils copy fails; log and continue\n",
        "\n",
        "# Expose a variable that downstream cells will use (may be replaced at runtime)\n",
        "SELECTED_DATASETS = None\n",
        "\n",
        "# End of parameters cell. Orchestrators/papermill can overwrite any of the above by injecting\n",
        "# values into the notebook (e.g. pass 'datasets' JSON into the notebook). At startup the\n",
        "# notebook will (in order) try: incoming param -> config file -> DATASETS_PARAM defaults.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%configure -f\n",
        "{\n",
        "  \"conf\": {\n",
        "    \"spark.driver.memory\": \"4g\",\n",
        "    \"spark.executor.memory\": \"4g\",\n",
        "    \"spark.executor.cores\": \"2\",\n",
        "    \"spark.dynamicAllocation.enabled\": \"false\"\n",
        "  }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Data â€” embedded generator + Azure SQL push\n",
        "\n",
        "This notebook generates base and updates datasets for each dataset entry and then (optionally) writes them into an Azure SQL database.\n",
        "It will also write controller copies and attempt to copy into Fabric Files (/Files/{dataset}base/ and /Files/{dataset}updates/) if running inside Fabric with mssparkutils."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n",
        "    import yaml\n",
        "except Exception:\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install pyyaml\n",
        "    import yaml\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, Iterable, List, Tuple\n",
        "\n",
        "print('yaml:', getattr(yaml, '__version__', 'unknown'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Determine datasets to use (priority: passed-in params -> config file -> embedded defaults)\n",
        "datasets_cfg = None\n",
        "incoming = None\n",
        "try:\n",
        "    import mssparkutils\n",
        "    try:\n",
        "        incoming = mssparkutils.notebook.getContext().getInput(NOTEBOOK_PARAM_INPUT_NAME)\n",
        "    except Exception:\n",
        "        # Some runtimes may expose inputs differently; try 'params' as a fallback\n",
        "        try:\n",
        "            incoming = mssparkutils.notebook.getContext().getInput('params')\n",
        "        except Exception:\n",
        "            incoming = None\n",
        "except Exception:\n",
        "    incoming = None\n",
        "\n",
        "if incoming:\n",
        "    print('Using datasets supplied to the notebook (incoming).')\n",
        "    # incoming may be JSON text or already a Python object\n",
        "    if isinstance(incoming, str):\n",
        "        try:\n",
        "            datasets_cfg = yaml.safe_load(incoming)\n",
        "        except Exception:\n",
        "            import json\n",
        "            datasets_cfg = json.loads(incoming)\n",
        "    else:\n",
        "        datasets_cfg = incoming\n",
        "else:\n",
        "    # No incoming params; try config file\n",
        "    if os.path.exists(CONFIG_PATH):\n",
        "        print(f'Reading datasets from {CONFIG_PATH}')\n",
        "        cfg = yaml.safe_load(open(CONFIG_PATH, 'r', encoding='utf-8'))\n",
        "        datasets_cfg = cfg.get('datasets', [])\n",
        "    else:\n",
        "        print('No config found; using embedded notebook defaults (DATASETS_PARAM).')\n",
        "        datasets_cfg = DATASETS_PARAM\n",
        "\n",
        "print('\\nDatasets to be generated:')\n",
        "for d in datasets_cfg:\n",
        "    print(' -', d.get('name'), 'rows=', d.get('row_count'))\n",
        "\n",
        "# expose for downstream cells\n",
        "SELECTED_DATASETS = datasets_cfg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embedded generator functions\n",
        "\n",
        "Functions to generate base rows and an updates file (op: update/insert/delete)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate_base_rows(n: int, seed: int = None, start_ts: datetime | None = None) -> Iterable[Dict[str, Any]]:\n",
        "    rnd = random.Random(seed) if seed is not None else random.Random()\n",
        "    categories = ['A','B','C']\n",
        "    start = start_ts or datetime.utcnow()\n",
        "    for i in range(1, n+1):\n",
        "        ts = (start + timedelta(seconds=i)).isoformat() + 'Z'\n",
        "        yield {\n",
        "            'id': i,\n",
        "            'value': round(rnd.uniform(0, 100), 4),\n",
        "            'timestamp': ts,\n",
        "            'category': rnd.choice(categories)\n",
        "        }\n",
        "\n",
        "def generate_updates_for_base(base_count: int, change_fraction: float, new_fraction: float, delete_fraction: float, seed: int | None = None, start_ts: datetime | None = None) -> Iterable[Dict[str, Any]]:\n",
        "    rnd = random.Random(seed+1 if seed is not None else None)\n",
        "    n_change = max(1, int(round(base_count * change_fraction))) if change_fraction > 0 else 0\n",
        "    n_new = max(0, int(round(base_count * new_fraction)))\n",
        "    n_delete = max(0, int(round(base_count * delete_fraction)))\n",
        "\n",
        "    ids = list(range(1, base_count+1))\n",
        "    rnd.shuffle(ids)\n",
        "    change_ids = ids[:n_change]\n",
        "    delete_ids = ids[n_change:n_change+n_delete] if n_delete>0 else []\n",
        "\n",
        "    categories = ['A','B','C']\n",
        "    start = start_ts or datetime.utcnow()\n",
        "    # updates: updates for change_ids\n",
        "    idx = 0\n",
        "    for idv in change_ids:\n",
        "        idx += 1\n",
        "        ts = (start + timedelta(seconds=base_count + idx)).isoformat() + 'Z'\n",
        "        yield {\n",
        "            'op': 'update',\n",
        "            'id': idv,\n",
        "            'value': round(rnd.uniform(0, 100), 4),\n",
        "            'timestamp': ts,\n",
        "            'category': rnd.choice(categories)\n",
        "        }\n",
        "    # inserts\n",
        "    for i in range(1, n_new+1):\n",
        "        new_id = base_count + i\n",
        "        ts = (start + timedelta(seconds=base_count + n_change + i)).isoformat() + 'Z'\n",
        "        yield {'op':'insert','id': new_id, 'value': round(rnd.uniform(0,100),4), 'timestamp': ts, 'category': rnd.choice(categories)}\n",
        "    # deletes\n",
        "    for idv in delete_ids:\n",
        "        yield {'op':'delete', 'id': idv, 'value':'', 'timestamp':'', 'category':''}\n",
        "\n",
        "def write_csv(path: str, rows: Iterable[Dict[str, Any]], header: List[str]) -> None:\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path, 'w', newline='', encoding='utf-8') as fh:\n",
        "        writer = csv.writer(fh)\n",
        "        writer.writerow(header)\n",
        "        for r in rows:\n",
        "            writer.writerow([r.get(c, '') for c in header])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate datasets and write local and controller copies; then attempt Fabric Files copy\n",
        "import shutil\n",
        "from pprint import pprint\n",
        "\n",
        "data_root = Path(OUTPUT_DIR)\n",
        "for ds in SELECTED_DATASETS:\n",
        "    name = ds.get('name')\n",
        "    row_count = int(ds.get('row_count', 10000))\n",
        "    change_fraction = float(ds.get('change_fraction', 0.01))\n",
        "    new_fraction = float(ds.get('new_fraction', 0.0))\n",
        "    delete_fraction = float(ds.get('delete_fraction', 0.0))\n",
        "    seed = int(ds.get('seed')) if ds.get('seed') is not None else GENERATOR_DEFAULT_SEED\n",
        "\n",
        "    print(f\"\\n=== Generating dataset {name}: rows={row_count}\")\n",
        "    # base\n",
        "    base_rows = list(generate_base_rows(row_count, seed=seed))\n",
        "    base_path = data_root / name / 'base.csv'\n",
        "    write_csv(str(base_path), base_rows, ['id','value','timestamp','category'])\n",
        "    print(f\"Wrote local base: {base_path}\")\n",
        "    # updates\n",
        "    updates_rows = list(generate_updates_for_base(row_count, change_fraction, new_fraction, delete_fraction, seed=seed))\n",
        "    updates_path = data_root / name / 'updates.csv'\n",
        "    write_csv(str(updates_path), updates_rows, ['op','id','value','timestamp','category'])\n",
        "    print(f\"Wrote local updates: {updates_path}\")\n",
        "    # controller copies\n",
        "    if GENERATE_CONTROLLER_COPY:\n",
        "        ctrl_dir = data_root / 'controller' / name\n",
        "        os.makedirs(ctrl_dir, exist_ok=True)\n",
        "        shutil.copyfile(str(base_path), str(ctrl_dir / 'base.csv'))\n",
        "        shutil.copyfile(str(updates_path), str(ctrl_dir / 'updates.csv'))\n",
        "        print(f\"Wrote controller copies -> {ctrl_dir}\")\n",
        "\n",
        "    # Attempt to write into Fabric Files if available\n",
        "    try:\n",
        "        import mssparkutils\n",
        "        files_base_dir = FILES_BASE_TEMPLATE.format(dataset=name)\n",
        "        files_updates_dir = FILES_UPDATES_TEMPLATE.format(dataset=name)\n",
        "        # Read binary and put\n",
        "        with open(str(base_path), 'rb') as fh:\n",
        "            mssparkutils.fs.put(files_base_dir + 'base.csv', fh.read(), overwrite=True)\n",
        "        with open(str(updates_path), 'rb') as fh:\n",
        "            mssparkutils.fs.put(files_updates_dir + 'updates.csv', fh.read(), overwrite=True)\n",
        "        print(f\"Copied to Fabric Files: {files_base_dir} and {files_updates_dir}\")\n",
        "    except Exception as e:\n",
        "        print('mssparkutils not available or copy failed (running outside Fabric?). Skipping Files copy. Error:', e)\n",
        "\n",
        "print('\\nAll dataset generation done.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Push generated CSVs into Azure SQL\n",
        "\n",
        "Provide an ODBC connection string when prompted. Example:\n",
        "\"Driver={ODBC Driver 17 for SQL Server};Server=tcp:<yourserver>.database.windows.net,1433;Database=<db>;UID=<user>;PWD=<password>;Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\"\n",
        "The notebook converts this into a SQLAlchemy engine using the pyodbc driver. Tables written:\n",
        "- dbo.base_{dataset_name}\n",
        "- dbo.updates_{dataset_name}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "push_sql = SQL_PUSH\n",
        "if not push_sql:\n",
        "    # If SQL_PUSH default is False, prompt interactively in notebook runs\n",
        "    user = input('Push generated data to Azure SQL? (yes/no) > ').strip().lower()\n",
        "    if user in ('y','yes'):\n",
        "        push_sql = True\n",
        "        conn_str = input('Provide ODBC connection string (Driver=...;Server=...;Database=...;UID=...;PWD=...) : ').strip()\n",
        "    else:\n",
        "        push_sql = False\n",
        "        conn_str = SQL_ODBC_CONNECTION\n",
        "else:\n",
        "    conn_str = SQL_ODBC_CONNECTION\n",
        "\n",
        "if push_sql:\n",
        "    if not conn_str:\n",
        "        print('No connection string provided; skipping SQL upload.')\n",
        "    else:\n",
        "        # Install sqlalchemy + pyodbc + pandas if missing\n",
        "        import sys\n",
        "        try:\n",
        "            import sqlalchemy\n",
        "            import pandas as pd\n",
        "        except Exception:\n",
        "            !{sys.executable} -m pip install sqlalchemy pyodbc pandas\n",
        "            import sqlalchemy\n",
        "            import pandas as pd\n",
        "        from sqlalchemy import create_engine\n",
        "        from urllib.parse import quote_plus\n",
        "\n",
        "        odbc_conn_str = quote_plus(conn_str)\n",
        "        engine_url = f'mssql+pyodbc:///?odbc_connect={odbc_conn_str}'\n",
        "        print('Creating SQLAlchemy engine...')\n",
        "        engine = create_engine(engine_url)\n",
        "\n",
        "        for ds in SELECTED_DATASETS:\n",
        "            name = ds.get('name')\n",
        "            base_path = data_root / name / 'base.csv'\n",
        "            updates_path = data_root / name / 'updates.csv'\n",
        "            if base_path.exists():\n",
        "                print(f\"Uploading base -> {SQL_SCHEMA}.base_{name} (from {base_path})\")\n",
        "                df_base = pd.read_csv(str(base_path))\n",
        "                # write in chunks\n",
        "                df_base.to_sql(f'base_{name}', con=engine, schema=SQL_SCHEMA, if_exists=SQL_IF_EXISTS, index=False, chunksize=SQL_CHUNKSIZE)\n",
        "                print('  Uploaded base')\n",
        "            else:\n",
        "                print(f'  Base file missing: {base_path}; skipping')\n",
        "            if updates_path.exists():\n",
        "                print(f\"Uploading updates -> {SQL_SCHEMA}.updates_{name} (from {updates_path})\")\n",
        "                df_upd = pd.read_csv(str(updates_path))\n",
        "                df_upd.to_sql(f'updates_{name}', con=engine, schema=SQL_SCHEMA, if_exists=SQL_IF_EXISTS, index=False, chunksize=SQL_CHUNKSIZE)\n",
        "                print('  Uploaded updates')\n",
        "            else:\n",
        "                print(f'  Updates file missing: {updates_path}; skipping')\n",
        "        print('All SQL uploads attempted.')\n",
        "else:\n",
        "    print('Skipping SQL upload.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('\\nGeneration & optional SQL push complete. Preview generated files:')\n",
        "for ds in SELECTED_DATASETS:\n",
        "    name = ds.get('name')\n",
        "    base_path = data_root / name / 'base.csv'\n",
        "    updates_path = data_root / name / 'updates.csv'\n",
        "    print(' -', name)\n",
        "    if base_path.exists():\n",
        "        print('    base:', base_path, 'size=', base_path.stat().st_size)\n",
        "    if updates_path.exists():\n",
        "        print('    updates:', updates_path, 'size=', updates_path.stat().st_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}

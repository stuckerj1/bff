{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "%%configure\n",
        "# All values that may be overridden by an orchestrator are declared here.\n",
        "# This makes it # easy to validate when invoked programmatically with different numbers.\n",
        "\n",
        "# -- Datasets array (interactive defaults are different than automation defaults).\n",
        "DATASETS_PARAM = [\n",
        "    {\n",
        "        \"name\": \"1k\",\n",
        "        \"row_count\": 1000,\n",
        "        \"change_fraction\": 0.01,\n",
        "        \"new_fraction\": 0.005,\n",
        "        \"delete_fraction\": 0.001,\n",
        "        \"seed\": 42,\n",
        "        \"description\": \"Interactive small dataset (1k rows)\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"100k\",\n",
        "        \"row_count\": 100000,\n",
        "        \"change_fraction\": 0.01,\n",
        "        \"new_fraction\": 0.005,\n",
        "        \"delete_fraction\": 0.001,\n",
        "        \"seed\": 42,\n",
        "        \"description\": \"Interactive medium dataset (100k rows)\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Do the test cases (datasets and parameter sets) include Azure SQL as a data source?\n",
        "PUSH_TO_AZURE_SQL = True\n",
        "\n",
        "# Azure SQL / ODBC parameters (interactive defaults). \n",
        "# These should be passed in from the automation handler.\n",
        "AZURE_SQL_SERVER = \"benchmarking-bff\"\n",
        "AZURE_SQL_DB = \"benchmarking\"\n",
        "\n",
        "# These connection details do not need to be passed in but can be optionally changed\n",
        "AZURE_SQL_CONNECTION = f\"Driver={{ODBC Driver 18 for SQL Server}};Server=tcp:{AZURE_SQL_SERVER}.database.windows.net,1433;Database={AZURE_SQL_DB};Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30\"\n",
        "AZURE_SQL_SCHEMA = \"dbo\"\n",
        "\n",
        "distribution = \"uniform\"  # Options: \"uniform\", \"skewed\", \"null_injection\"\n",
        "seed = 42  # For deterministic output\n",
        "\n",
        "# Lakehouse / local output folders -- these will be dynamically created per-dataset in the loop\n",
        "# Orchestrators can overwrite any of the above by injecting values into the notebook \n",
        "# (e.g. pass 'DATASETS_PARAM' JSON, AZURE_SQL_SERVER, AZURE_SQL_DB into the notebook). \n",
        "# End of parameters cell. \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Minimal schema_config used by the generator (kept outside the parameters cell so the first cell remains exact)\n",
        "schema_config = {\n",
        "    \"categorical_fields\": 3,\n",
        "    \"numeric_fields\": 5,\n",
        "    \"timestamp_fields\": 2\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Data — minimal parquet writer (per-dataset paths) + optional SQL copy\n",
        "\n",
        "This notebook is intentionally minimal and focused. It loops DATASETS_PARAM, generates a base and updates parquet per dataset, writes per-dataset lakehouse paths (e.g. /lakehouse/default/Files/{name}base/), and also writes controller/local copies under data/{name}/. Filenames are base.parquet and updates.parquet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import pytz\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "print('Imports OK — proceeding with generation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- Schema Definition\n",
        "Create a small synthetic schema generator (categorical, numeric, timestamps)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_synthetic_schema(config):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    schema = {}\n",
        "    # Categorical fields\n",
        "    for i in range(config[\"categorical_fields\"]):\n",
        "        schema[f\"cat_{i+1}\"] = lambda n, _choices=['A','B','C','D']: np.random.choice(_choices, size=n)\n",
        "    # Numeric fields\n",
        "    for i in range(config[\"numeric_fields\"]):\n",
        "        schema[f\"num_{i+1}\"] = lambda n: np.random.uniform(0, 1000, size=n)\n",
        "    # Timestamp fields\n",
        "    for i in range(config[\"timestamp_fields\"]):\n",
        "        start = pd.Timestamp(\"2023-01-01\")\n",
        "        schema[f\"ts_{i+1}\"] = lambda n: pd.date_range(start, periods=n, freq=\"ms\")\n",
        "    return schema\n",
        "\n",
        "schema = create_synthetic_schema(schema_config)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- Synthetic Data Generation\n",
        "Generate the base dataframe from the schema and the selected distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate_base_dataframe(row_count, schema, distribution, seed):\n",
        "    \"\"\"Generate base dataframe. Parameters are required (no defaults).\"\"\"\n",
        "    # Print input parameters for visibility\n",
        "    print(f\"generate_base_dataframe called with row_count={row_count}, distribution={distribution}, seed={seed}\")\n",
        "    np.random.seed(seed)\n",
        "    data = {}\n",
        "    for col, gen in schema.items():\n",
        "        if \"num\" in col and distribution == \"skewed\":\n",
        "            data[col] = np.random.exponential(500, size=row_count)\n",
        "        elif \"num\" in col and distribution == \"null_injection\":\n",
        "            col_data = gen(row_count)\n",
        "            null_mask = np.random.rand(row_count) < 0.05\n",
        "            col_data[null_mask] = np.nan\n",
        "            data[col] = col_data\n",
        "        else:\n",
        "            data[col] = gen(row_count)\n",
        "    df = pd.DataFrame(data)\n",
        "    df[\"id\"] = np.arange(1, row_count + 1)\n",
        "    # Ensure timestamp columns are pandas datetime64[ms] and timezone-aware (UTC)\n",
        "    for col in df.columns:\n",
        "        if col.startswith(\"ts_\"):\n",
        "            df[col] = pd.to_datetime(df[col]).dt.tz_localize(pytz.UTC)\n",
        "    # Add update_type column, always 'insert' for base\n",
        "    df[\"update_type\"] = \"insert\"\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- Generate Updates (function)\n",
        "Create updates (updates, inserts, deletes) and shuffle them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate_updates(df, change_pct, new_pct, delete_pct, seed):\n",
        "    \"\"\"Generate updates dataframe. Parameters are required (no defaults).\"\"\"\n",
        "    # Print input parameters for visibility\n",
        "    print(f\"generate_updates called with rows={len(df)}, change_pct={change_pct}, new_pct={new_pct}, delete_pct={delete_pct}, seed={seed}\")\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    n = len(df)\n",
        "    # Changed rows\n",
        "    changed_count = int(n * change_pct)\n",
        "    change_idx = np.random.choice(df.index, changed_count, replace=False)\n",
        "    changed_df = df.loc[change_idx].copy()\n",
        "    # Example modification of one numeric column if present\n",
        "    if \"num_1\" in changed_df.columns:\n",
        "        changed_df[\"num_1\"] = changed_df[\"num_1\"] + np.random.uniform(1, 10, changed_count)\n",
        "    changed_df[\"update_type\"] = \"update\"\n",
        "    # New rows\n",
        "    new_count = int(n * new_pct)\n",
        "    new_df = df.sample(new_count).copy()\n",
        "    new_df[\"id\"] = np.arange(n + 1, n + new_count + 1)\n",
        "    new_df[\"update_type\"] = \"insert\"\n",
        "    # Deleted rows\n",
        "    delete_count = int(n * delete_pct)\n",
        "    delete_idx = np.random.choice(df.index, delete_count, replace=False)\n",
        "    deleted_df = df.loc[delete_idx][[\"id\"]].copy()\n",
        "    deleted_df[\"update_type\"] = \"delete\"\n",
        "    # Combine and shuffle updates\n",
        "    updates = pd.concat([changed_df, new_df, deleted_df], ignore_index=True)\n",
        "    updates = updates.sample(frac=1, random_state=seed).reset_index(drop=True)  # Shuffle rows\n",
        "    # No need to process timestamps again—they are already UTC tz-aware\n",
        "    return updates\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- Save Loop (per-dataset paths and filenames) — strict, no fallbacks or extra validation\n",
        "This loop uses DATASETS_PARAM directly and lets errors surface if keys are missing or malformed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "data_root = Path('data')\n",
        "\n",
        "for ds in DATASETS_PARAM:\n",
        "    # Use the dataset fields directly; let errors surface if keys are missing or invalid\n",
        "    name = ds['name']\n",
        "    row_count_ds = int(ds['row_count'])\n",
        "    change_fraction = float(ds['change_fraction'])\n",
        "    new_fraction = float(ds['new_fraction'])\n",
        "    delete_fraction = float(ds['delete_fraction'])\n",
        "    seed_ds = int(ds['seed'])\n",
        "\n",
        "    print(f\"\\n=== Generating dataset {name}: rows={row_count_ds}\")\n",
        "\n",
        "    # Per-dataset lakehouse paths (as requested)\n",
        "    base_output_folder = f\"/lakehouse/default/Files/{name}base/\"\n",
        "    updates_output_folder = f\"/lakehouse/default/Files/{name}updates/\"\n",
        "\n",
        "    # Local controller area (always write a local copy under data/{name}/)\n",
        "    local_dir = data_root / name\n",
        "    local_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Generate base and updates\n",
        "    base_df = generate_base_dataframe(row_count_ds, schema, distribution, seed_ds)\n",
        "    base_file_local = local_dir / 'base.parquet'\n",
        "    base_df.to_parquet(str(base_file_local), engine='pyarrow', index=False, coerce_timestamps='ms')\n",
        "    print('  wrote local base:', base_file_local)\n",
        "\n",
        "    updates_df = generate_updates(base_df, change_fraction, new_fraction, delete_fraction, seed_ds)\n",
        "    updates_file_local = local_dir / 'updates.parquet'\n",
        "    updates_df.to_parquet(str(updates_file_local), engine='pyarrow', index=False, coerce_timestamps='ms')\n",
        "    print('  wrote local updates:', updates_file_local)\n",
        "\n",
        "    # Best-effort: write lakehouse copies (may fail outside Fabric)\n",
        "    try:\n",
        "        Path(base_output_folder).mkdir(parents=True, exist_ok=True)\n",
        "        (Path(base_output_folder) / 'base.parquet').write_bytes(base_file_local.read_bytes())\n",
        "        Path(updates_output_folder).mkdir(parents=True, exist_ok=True)\n",
        "        (Path(updates_output_folder) / 'updates.parquet').write_bytes(updates_file_local.read_bytes())\n",
        "        print('  wrote lakehouse copies ->', base_output_folder, updates_output_folder)\n",
        "    except Exception:\n",
        "        print('  could not write to lakehouse path (running outside Fabric or no permission); skipped lakehouse writes')\n",
        "\n",
        "    # Controller copies (local controller area)\n",
        "    ctrl_dir = data_root / 'controller' / name\n",
        "    ctrl_dir.mkdir(parents=True, exist_ok=True)\n",
        "    (ctrl_dir / 'base.parquet').write_bytes(base_file_local.read_bytes())\n",
        "    (ctrl_dir / 'updates.parquet').write_bytes(updates_file_local.read_bytes())\n",
        "    print('  wrote controller copies ->', ctrl_dir)\n",
        "\n",
        "print('\\nGeneration loop complete.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## --- Metadata Logging\n",
        "Log simple metadata about the files created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def log_metadata(file_path, row_count_val, fmt, distribution_val, update_type):\n",
        "    import json\n",
        "    meta = {\n",
        "        \"file\": file_path,\n",
        "        \"rows\": row_count_val,\n",
        "        \"format\": fmt,\n",
        "        \"distribution\": distribution_val,\n",
        "        \"update_type\": update_type,\n",
        "        \"timestamp\": pd.Timestamp.now().isoformat()\n",
        "    }\n",
        "    meta_file = str(file_path).replace('.parquet', '.meta.json')\n",
        "    with open(meta_file, 'w') as f:\n",
        "        json.dump(meta, f, indent=2)\n",
        "    print(f\"Metadata logged: {meta_file}\")\n",
        "\n",
        "# Log metadata for each dataset's local files\n",
        "for ds in DATASETS_PARAM:\n",
        "    name = ds['name']\n",
        "    row_count_ds = int(ds['row_count'])\n",
        "    local_dir = data_root / name\n",
        "    base_file_local = local_dir / 'base.parquet'\n",
        "    updates_file_local = local_dir / 'updates.parquet'\n",
        "    if base_file_local.exists():\n",
        "        log_metadata(str(base_file_local), row_count_ds, 'parquet', distribution, 'base')\n",
        "    if updates_file_local.exists():\n",
        "        log_metadata(str(updates_file_local), len(pd.read_parquet(str(updates_file_local), engine='pyarrow')), 'parquet', distribution, 'batch_updates')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}

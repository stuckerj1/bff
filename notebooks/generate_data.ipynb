{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6630581",
   "metadata": {},
   "source": [
    "# ðŸ““ 1. Generate Data\n",
    "## Synthetic Data Generator\n",
    "### Ensure `DataSourceLakehouse` is connected as a data source before running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c099ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Data Generation for Fabric Benchmarking\n",
    "# Purpose: Generate base datasets and incremental update slices for capacity and performance testing\n",
    "\n",
    "# ---\n",
    "# 1. Parameter Setup\n",
    "\n",
    "# Parameters (can be set via notebook widgets or workflow inputs)\n",
    "row_count = 10000  # Options: 10000, 1000000\n",
    "format = \"parquet\" # Options: \"parquet\", \"delta\"\n",
    "schema_config = {\n",
    "    \"categorical_fields\": 3,\n",
    "    \"numeric_fields\": 5,\n",
    "    \"timestamp_fields\": 2\n",
    "}\n",
    "distribution = \"uniform\"  # Options: \"uniform\", \"skewed\", \"null_injection\"\n",
    "seed = 42  # For deterministic output\n",
    "\n",
    "# Lakehouse output folders (Fabric)\n",
    "lakehouse_name = \"DataSourceLakehouse\"\n",
    "base_output_folder = f\"/lakehouse/{lakehouse_name}/Files/base/\"\n",
    "updates_output_folder = f\"/lakehouse/{lakehouse_name}/Files/updates/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_schema(config):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    schema = {}\n",
    "    # Categorical fields\n",
    "    for i in range(config[\"categorical_fields\"]):\n",
    "        schema[f\"cat_{i+1}\"] = lambda n: np.random.choice(['A', 'B', 'C', 'D'], size=n)\n",
    "    # Numeric fields\n",
    "    for i in range(config[\"numeric_fields\"]):\n",
    "        schema[f\"num_{i+1}\"] = lambda n: np.random.uniform(0, 1000, size=n)\n",
    "    # Timestamp fields (guaranteed compatible)\n",
    "    for i in range(config[\"timestamp_fields\"]):\n",
    "        start = pd.Timestamp(\"2023-01-01\")\n",
    "        # Generate with millisecond freq to avoid nanosecond precision\n",
    "        schema[f\"ts_{i+1}\"] = lambda n: pd.date_range(start, periods=n, freq=\"ms\")\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gen_base",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_base_dataframe(row_count, schema, distribution, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    data = {}\n",
    "    for col, gen in schema.items():\n",
    "        if \"num\" in col and distribution == \"skewed\":\n",
    "            data[col] = np.random.exponential(500, size=row_count)\n",
    "        elif \"num\" in col and distribution == \"null_injection\":\n",
    "            col_data = gen(row_count)\n",
    "            null_mask = np.random.rand(row_count) < 0.05\n",
    "            col_data[null_mask] = np.nan\n",
    "            data[col] = col_data\n",
    "        else:\n",
    "            data[col] = gen(row_count)\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"id\"] = np.arange(1, row_count + 1)\n",
    "    # Ensure timestamp columns are pandas datetime64[ms] (no nanosecond precision)\n",
    "    for col in df.columns:\n",
    "        if col.startswith(\"ts_\"):\n",
    "            df[col] = pd.to_datetime(df[col]).astype('datetime64[ms]')\n",
    "    return df\n",
    "\n",
    "schema = create_synthetic_schema(schema_config)\n",
    "base_df = generate_base_dataframe(row_count, schema, distribution, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark_write",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Convert pandas DataFrame to Spark DataFrame for Parquet/Delta compatibility\n",
    "spark_base_df = spark.createDataFrame(base_df)\n",
    "\n",
    "base_file = f\"{base_output_folder}base_{row_count}_parquet.parquet\"\n",
    "# Write as Parquet with Spark (guarantees TIMESTAMP(MILLIS))\n",
    "spark_base_df.write.mode(\"overwrite\").parquet(base_file)\n",
    "print(f\"Base dataset saved: {base_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gen_updates",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_updates(df, change_pct=0.01, new_pct=0.005, delete_pct=0.001, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    n = len(df)\n",
    "    # Changed rows\n",
    "    changed_count = int(n * change_pct)\n",
    "    change_idx = np.random.choice(df.index, changed_count, replace=False)\n",
    "    changed_df = df.loc[change_idx].copy()\n",
    "    changed_df[\"num_1\"] += np.random.uniform(1, 10, changed_count)\n",
    "    changed_df[\"update_type\"] = \"update\"\n",
    "    # New rows\n",
    "    new_count = int(n * new_pct)\n",
    "    new_df = df.sample(new_count).copy()\n",
    "    new_df[\"id\"] = np.arange(n + 1, n + new_count + 1)\n",
    "    new_df[\"update_type\"] = \"insert\"\n",
    "    # Deleted rows\n",
    "    delete_count = int(n * delete_pct)\n",
    "    delete_idx = np.random.choice(df.index, delete_count, replace=False)\n",
    "    deleted_df = df.loc[delete_idx][[\"id\"]].copy()\n",
    "    deleted_df[\"update_type\"] = \"delete\"\n",
    "    # Combine and shuffle updates\n",
    "    updates = pd.concat([changed_df, new_df, deleted_df], ignore_index=True)\n",
    "    updates = updates.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    return updates\n",
    "\n",
    "updates_df = generate_updates(base_df)\n",
    "# Ensure timestamp columns are pandas datetime64[ms]\n",
    "for col in updates_df.columns:\n",
    "    if col.startswith(\"ts_\"):\n",
    "        updates_df[col] = pd.to_datetime(updates_df[col]).astype('datetime64[ms]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark_write_updates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updates as Parquet via Spark\n",
    "spark_updates_df = spark.createDataFrame(updates_df)\n",
    "updates_file = f\"{updates_output_folder}updates_{row_count}_parquet.parquet\"\n",
    "spark_updates_df.write.mode(\"overwrite\").parquet(updates_file)\n",
    "print(f\"Updates saved: {updates_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meta_logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Metadata Logging ---\n",
    "def log_metadata(file_path, row_count, format, distribution, update_type):\n",
    "    import json\n",
    "    meta = {\n",
    "        \"file\": file_path,\n",
    "        \"rows\": row_count,\n",
    "        \"format\": format,\n",
    "        \"distribution\": distribution,\n",
    "        \"update_type\": update_type,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    meta_file = file_path.replace(\".parquet\", \".meta.json\")\n",
    "    with open(meta_file, \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    print(f\"Metadata logged: {meta_file}\")\n",
    "\n",
    "log_metadata(base_file, row_count, format, distribution, \"base\")\n",
    "log_metadata(updates_file, len(updates_df), format, distribution, \"batch_updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Diagnostics: List Lakehouse Files ---\n",
    "try:\n",
    "    import msparkutils\n",
    "    print(\"Lakehouses:\", msparkutils.lakehouse.listLakehouses())\n",
    "    print(\"Base folder files:\", msparkutils.fs.ls(base_output_folder))\n",
    "    print(\"Updates folder files:\", msparkutils.fs.ls(updates_output_folder))\n",
    "except ImportError:\n",
    "    print(\"msparkutils not available. Skipping Lakehouse diagnostics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "Synthetic data generation complete. Proceed to ingestion workflows and benchmarking."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# ðŸ““ 4. Run Queries\n",
    "## Query Performance Benchmarking for TC.11.xâ€“TC.13.x\n",
    "\n",
    "### ðŸ”— Ensure `BenchmarkLakehouse` is connected as a data source before running.\n",
    "\n",
    "Assumes synthetic data, initial load, and updates have been completed.\n",
    "All queries are run against the target table in BenchmarkLakehouse and BenchmarkWarehouse.\n",
    "Join queries are excluded (single table per location).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "setup",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "id": "params",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Paths and table names\n",
    "target_lakehouse = \"BenchmarkLakehouse\"\n",
    "target_warehouse = \"BenchmarkWarehouse\"\n",
    "row_count = 10000  # Update as appropriate\n",
    "\n",
    "delta_tables = {\n",
    "    \"refresh\": \"delta_refresh_load\",\n",
    "    \"compare\": \"delta_compare_load\",\n",
    "    \"increment\": \"delta_increment_load\"\n",
    "}\n",
    "warehouse_tables = {\n",
    "    \"refresh\": \"wh_table_refresh_load\",\n",
    "    \"compare\": \"wh_table_compare_load\",\n",
    "    \"increment\": \"wh_table_increment_load\"\n",
    "}\n",
    "\n",
    "# Choose table to query (usually 'increment' for event log)\n",
    "lakehouse_table = f\"{target_lakehouse}.{delta_tables['increment']}\"\n",
    "warehouse_table = f\"{target_warehouse}.dbo.{warehouse_tables['increment']}\""
   ]
  },
  {
   "cell_type": "code",
   "id": "read_lakehouse",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read Delta table from Lakehouse\n",
    "df_lakehouse = spark.read.table(lakehouse_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query_intro",
   "metadata": {},
   "source": [
    "## Query Types\n",
    "- **Filter Query**: Select rows with a specific category value.\n",
    "- **Aggregate Query**: Group by category, aggregate numeric columns.\n",
    "- **Batch Query**: Select update events in a specific time window.\n",
    "- **Top-N Query**: Retrieve top N rows by a numeric column.\n",
    "\n",
    "*(Join queries are excludedâ€”only one table in each target location.)*"
   ]
  },
  {
   "cell_type": "code",
   "id": "query_funcs",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define query functions\n",
    "def run_filter_query(df):\n",
    "    # Example: Filter by category\n",
    "    return df.filter(col(\"cat_1\") == \"A\").count()\n",
    "\n",
    "def run_aggregate_query(df):\n",
    "    # Example: Group by category, aggregate numerics\n",
    "    return df.groupBy(\"cat_1\").agg({\"num_1\": \"avg\", \"num_2\": \"max\"}).count()\n",
    "\n",
    "def run_batch_query(df):\n",
    "    # Example: Select events by update_type and time window\n",
    "    return df.filter((col(\"update_type\") == \"update\") & (col(\"ts_1\") > \"2025-01-01\")).count()\n",
    "\n",
    "def run_topn_query(df, n=10):\n",
    "    # Example: Top-N by numeric value\n",
    "    return df.orderBy(col(\"num_1\").desc()).limit(n).count()"
   ]
  },
  {
   "cell_type": "code",
   "id": "logging_util",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Performance logging utility\n",
    "def log_query_perf(query_func, df, description):\n",
    "    start = time.time()\n",
    "    result = query_func(df)\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"{description}: {elapsed:.3f}s (Rows: {result})\")\n",
    "    return {\"query\": description, \"rows\": result, \"time_s\": elapsed}"
   ]
  },
  {
   "cell_type": "code",
   "id": "lakehouse_queries",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run queries on Lakehouse Delta table\n",
    "lakehouse_metrics = []\n",
    "lakehouse_metrics.append(log_query_perf(run_filter_query, df_lakehouse, \"Lakehouse Filter cat_1 == 'A'\"))\n",
    "lakehouse_metrics.append(log_query_perf(run_aggregate_query, df_lakehouse, \"Lakehouse Aggregate by cat_1\"))\n",
    "lakehouse_metrics.append(log_query_perf(run_batch_query, df_lakehouse, \"Lakehouse Batch update_type == update, ts_1 > '2025-01-01'\"))\n",
    "lakehouse_metrics.append(log_query_perf(run_topn_query, df_lakehouse, \"Lakehouse Top 10 num_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "id": "read_warehouse",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read Warehouse table (for Spark SQL endpoint, not as DataFrame)\n",
    "from com.microsoft.spark.fabric import Constants\n",
    "df_warehouse = spark.read.synapsesql(warehouse_table)"
   ]
  },
  {
   "cell_type": "code",
   "id": "warehouse_queries",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run queries on Warehouse table\n",
    "warehouse_metrics = []\n",
    "warehouse_metrics.append(log_query_perf(run_filter_query, df_warehouse, \"Warehouse Filter cat_1 == 'A'\"))\n",
    "warehouse_metrics.append(log_query_perf(run_aggregate_query, df_warehouse, \"Warehouse Aggregate by cat_1\"))\n",
    "warehouse_metrics.append(log_query_perf(run_batch_query, df_warehouse, \"Warehouse Batch update_type == update, ts_1 > '2025-01-01'\"))\n",
    "warehouse_metrics.append(log_query_perf(run_topn_query, df_warehouse, \"Warehouse Top 10 num_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "id": "visualize",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display metrics as table\n",
    "import pandas as pd\n",
    "\n",
    "all_metrics = lakehouse_metrics + warehouse_metrics\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "display(metrics_df)\n",
    "\n",
    "# Print completion message\n",
    "print(\"Query performance benchmarking complete. Metrics above can be visualized in the next step.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "synapse_pyspark",
   "display_name": "Synapse PySpark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

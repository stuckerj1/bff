{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# ðŸ““ 4. Run Queries\n",
    "## Query Performance Benchmarking for TC.11.xâ€“TC.13.x\n",
    "\n",
    "### ðŸ”— Ensure `BenchmarkLakehouse` is connected as a data source before running.\n",
    "\n",
    "Assumes synthetic data, initial load, and updates have been completed.\n",
    "All queries are run against the target table in BenchmarkLakehouse and BenchmarkWarehouse.\n",
    "Join queries are excluded (single table per location).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "setup",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "id": "params",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Paths and table names\n",
    "target_lakehouse = \"BenchmarkLakehouse\"\n",
    "target_warehouse = \"BenchmarkWarehouse\"\n",
    "row_count = 10000  # Update as appropriate\n",
    "\n",
    "delta_tables = {\n",
    "    \"refresh\": \"delta_refresh_load\",\n",
    "    \"compare\": \"delta_compare_load\",\n",
    "    \"increment\": \"delta_increment_load\"\n",
    "}\n",
    "warehouse_tables = {\n",
    "    \"refresh\": \"wh_table_refresh_load\",\n",
    "    \"compare\": \"wh_table_compare_load\",\n",
    "    \"increment\": \"wh_table_increment_load\"\n",
    "}\n",
    "\n",
    "# Choose table to query (usually 'increment' for event log)\n",
    "lakehouse_table = f\"{target_lakehouse}.{delta_tables['increment']}\"\n",
    "warehouse_table = f\"{target_warehouse}.dbo.{warehouse_tables['increment']}\""
   ]
  },
  {
   "cell_type": "code",
   "id": "metrics_schema",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
    "from datetime import datetime\n",
    "\n",
    "# Keep storage_size_mb and cu_used as FloatType to match ingest/apply updates notebooks.\n",
    "metrics_schema = StructType([\n",
    "    StructField(\"test_case_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"format\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"rows\", IntegerType(), True),\n",
    "    StructField(\"update_strategy\", StringType(), True),\n",
    "    StructField(\"ingest_time_s\", FloatType(), True),\n",
    "    StructField(\"spinup_time_s\", FloatType(), True),\n",
    "    StructField(\"storage_size_mb\", FloatType(), True),\n",
    "    StructField(\"query_type\", StringType(), True),\n",
    "    StructField(\"query_time_s\", FloatType(), True),\n",
    "    StructField(\"cu_used\", FloatType(), True),\n",
    "    StructField(\"notes\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "id": "read_lakehouse",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read Delta table from Lakehouse\n",
    "df_lakehouse = spark.read.table(lakehouse_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query_intro",
   "metadata": {},
   "source": [
    "## Query Types\n",
    "- **Filter Query**: Select rows with a specific category value.\n",
    "- **Aggregate Query**: Group by category, aggregate numeric columns.\n",
    "- **Batch Query**: Select update events in a specific time window.\n",
    "- **Top-N Query**: Retrieve top N rows by a numeric column.\n",
    "\n",
    "*(Join queries are excludedâ€”only one table in each target location.)*"
   ]
  },
  {
   "cell_type": "code",
   "id": "query_funcs",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define query functions\n",
    "def run_filter_query(df):\n",
    "    # Example: Filter by category\n",
    "    return df.filter(col(\"cat_1\") == \"A\").count()\n",
    "\n",
    "def run_aggregate_query(df):\n",
    "    # Example: Group by category, aggregate numerics\n",
    "    return df.groupBy(\"cat_1\").agg({\"num_1\": \"avg\", \"num_2\": \"max\"}).count()\n",
    "\n",
    "def run_batch_query(df):\n",
    "    # Example: Select events by update_type and time window\n",
    "    return df.filter((col(\"update_type\") == \"update\") & (col(\"ts_1\") > \"2025-01-01\")).count()\n",
    "\n",
    "def run_topn_query(df, n=10):\n",
    "    # Example: Top-N by numeric value\n",
    "    return df.orderBy(col(\"num_1\").desc()).limit(n).count()"
   ]
  },
  {
   "cell_type": "code",
   "id": "logging_util",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Performance logging utility\n",
    "def log_query_perf(query_func, df, description):\n",
    "    start = time.time()\n",
    "    result = query_func(df)\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"{description}: {elapsed:.3f}s (Rows: {result})\")\n",
    "    return {\"query\": description, \"rows\": result, \"time_s\": elapsed}"
   ]
  },
  {
   "cell_type": "code",
   "id": "lakehouse_queries",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run queries on Lakehouse Delta table\n",
    "lakehouse_metrics = []\n",
    "lakehouse_metrics.append(log_query_perf(run_filter_query, df_lakehouse, \"Lakehouse Filter cat_1 == 'A'\"))\n",
    "lakehouse_metrics.append(log_query_perf(run_aggregate_query, df_lakehouse, \"Lakehouse Aggregate by cat_1\"))\n",
    "lakehouse_metrics.append(log_query_perf(run_batch_query, df_lakehouse, \"Lakehouse Batch update_type == update, ts_1 > '2025-01-01'\"))\n",
    "lakehouse_metrics.append(log_query_perf(run_topn_query, df_lakehouse, \"Lakehouse Top 10 num_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "id": "read_warehouse",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read Warehouse table (for Spark SQL endpoint, not as DataFrame)\n",
    "from com.microsoft.spark.fabric import Constants\n",
    "df_warehouse = spark.read.synapsesql(warehouse_table)"
   ]
  },
  {
   "cell_type": "code",
   "id": "warehouse_queries",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run queries on Warehouse table\n",
    "warehouse_metrics = []\n",
    "warehouse_metrics.append(log_query_perf(run_filter_query, df_warehouse, \"Warehouse Filter cat_1 == 'A'\"))\n",
    "warehouse_metrics.append(log_query_perf(run_aggregate_query, df_warehouse, \"Warehouse Aggregate by cat_1\"))\n",
    "warehouse_metrics.append(log_query_perf(run_batch_query, df_warehouse, \"Warehouse Batch update_type == update, ts_1 > '2025-01-01'\"))\n",
    "warehouse_metrics.append(log_query_perf(run_topn_query, df_warehouse, \"Warehouse Top 10 num_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "id": "visualize",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display metrics as table\n",
    "import pandas as pd\n",
    "\n",
    "all_metrics = lakehouse_metrics + warehouse_metrics\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "display(metrics_df)\n",
    "\n",
    "# Print completion message\n",
    "print(\"Query performance benchmarking complete. Metrics above can be visualized in the next step.\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "log_to_metrics_table",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Log metrics to metrics table in BenchmarkLakehouse\n",
    "def log_query_to_metrics(test_case_id, format, location, rows, query_type, query_time_s, notes=\"\"):\n",
    "    # Use float('nan') for non-applicable numeric fields so they match FloatType schema\n",
    "    metrics_row = [(\n",
    "        test_case_id,\n",
    "        datetime.now(),\n",
    "        format,\n",
    "        location,\n",
    "        rows,\n",
    "        \"\",                  # update_strategy (N/A for queries)\n",
    "        float('nan'),        # ingest_time_s (N/A for queries)\n",
    "        float('nan'),        # spinup_time_s (N/A)\n",
    "        float('nan'),        # storage_size_mb (N/A for queries) cast to float to match FloatType\n",
    "        query_type,\n",
    "        float(query_time_s),\n",
    "        float('nan'),        # cu_used (N/A for queries) cast to float to match FloatType\n",
    "        notes\n",
    "    )]\n",
    "    spark.createDataFrame(metrics_row, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')"
   ]
  },
  {
   "cell_type": "code",
   "id": "log_all_metrics",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Log all Lakehouse query metrics\n",
    "for metric in lakehouse_metrics:\n",
    "    log_query_to_metrics(\n",
    "        test_case_id=\"TC.11.x\",\n",
    "        format=\"Delta\",\n",
    "        location=\"Tables\",\n",
    "        rows=metric['rows'],\n",
    "        query_type=metric['query'],\n",
    "        query_time_s=metric['time_s'],\n",
    "        notes=\"Lakehouse query performance\"\n",
    "    )\n",
    "\n",
    "# Log all Warehouse query metrics\n",
    "for metric in warehouse_metrics:\n",
    "    log_query_to_metrics(\n",
    "        test_case_id=\"TC.12.x\",\n",
    "        format=\"Warehouse\",\n",
    "        location=\"Tables\",\n",
    "        rows=metric['rows'],\n",
    "        query_type=metric['query'],\n",
    "        query_time_s=metric['time_s'],\n",
    "        notes=\"Warehouse query performance\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "synapse_pyspark",
   "display_name": "Synapse PySpark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

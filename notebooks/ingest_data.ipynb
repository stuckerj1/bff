{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# ðŸ““ 2. Ingest Data\n",
    "## Ingestion Module: Sequential Benchmark for TC01, TC07, TC13\n",
    "\n",
    "- Ensure `DataSourceLakehouse` & `BenchmarkLakehouse` are connected as data sources before running.\n",
    "- For TC13, data is loaded into BenchmarkWarehouse using T-SQL from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "id": "setup",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "id": "params",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lakehouse/warehouse names\n",
    "source_lakehouse = \"DataSourceLakehouse\"\n",
    "target_lakehouse = \"BenchmarkLakehouse\"\n",
    "target_warehouse = \"BenchmarkWarehouse\"\n",
    "\n",
    "row_count = 10000  # Change as needed\n",
    "\n",
    "# Explicit paths\n",
    "input_path_parquet = f\"/lakehouse/{source_lakehouse}/Files/base/base_{row_count}_parquet.parquet\"\n",
    "delta_table_name = \"target_table_delta\"\n",
    "delta_table_path = f\"/Tables/{target_lakehouse}/{delta_table_name}\"\n",
    "warehouse_table_name = \"target_table_sql\""
   ]
  },
  {
   "cell_type": "code",
   "id": "metrics_schema",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics_schema = StructType([\n",
    "    StructField(\"test_case_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"format\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"rows\", IntegerType(), True),\n",
    "    StructField(\"update_strategy\", StringType(), True),\n",
    "    StructField(\"ingest_time_s\", FloatType(), True),\n",
    "    StructField(\"spinup_time_s\", FloatType(), True),\n",
    "    StructField(\"storage_size_mb\", FloatType(), True),\n",
    "    StructField(\"query_type\", StringType(), True),\n",
    "    StructField(\"query_time_s\", FloatType(), True),\n",
    "    StructField(\"cu_used\", FloatType(), True),\n",
    "    StructField(\"notes\", StringType(), True)\n",
    "])\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {target_lakehouse}.metrics (\n",
    "    test_case_id STRING,\n",
    "    timestamp TIMESTAMP,\n",
    "    format STRING,\n",
    "    location STRING,\n",
    "    rows INT,\n",
    "    update_strategy STRING,\n",
    "    ingest_time_s FLOAT,\n",
    "    spinup_time_s FLOAT,\n",
    "    storage_size_mb FLOAT,\n",
    "    query_type STRING,\n",
    "    query_time_s FLOAT,\n",
    "    cu_used FLOAT,\n",
    "    notes STRING\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc01_title",
   "metadata": {},
   "source": [
    "## TC01: Lakehouse Parquet File Ingest (from DataSourceLakehouse to BenchmarkLakehouse)"
   ]
  },
  {
   "cell_type": "code",
   "id": "tc01",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "tc01_spinup_start = time.time()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "tc01_spinup_end = time.time()\n",
    "tc01_spinup_duration = tc01_spinup_end - tc01_spinup_start\n",
    "\n",
    "tc01_ingest_start = time.time()\n",
    "df = spark.read.format(\"parquet\").load(input_path_parquet)\n",
    "df.write.mode(\"overwrite\").saveAsTable(f\"{target_lakehouse}.target_table_parquet\")\n",
    "tc01_ingest_end = time.time()\n",
    "tc01_ingest_duration = tc01_ingest_end - tc01_ingest_start\n",
    "\n",
    "# Storage size for TC01\n",
    "try:\n",
    "    import msparkutils\n",
    "    tc01_table_path = f\"/lakehouse/{target_lakehouse}/Tables/target_table_parquet\"\n",
    "    storage_files = msparkutils.fs.ls(tc01_table_path)\n",
    "    tc01_storage_size_mb = sum(f.size for f in storage_files) / (1024 * 1024)\n",
    "except Exception:\n",
    "    tc01_storage_size_mb = float('nan')\n",
    "\n",
    "metrics_tc01 = [\n",
    "    (\n",
    "        \"TC01\",\n",
    "        datetime.now(),\n",
    "        \"Parquet\",\n",
    "        \"Files\",\n",
    "        row_count,\n",
    "        \"Full Refresh\",\n",
    "        tc01_ingest_duration,\n",
    "        tc01_spinup_duration,\n",
    "        tc01_storage_size_mb,\n",
    "        \"N/A\",\n",
    "        float('nan'),\n",
    "        float('nan'),\n",
    "        \"No tabular access\"\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_tc01, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc07_title",
   "metadata": {},
   "source": [
    "## TC07: Shortcut to Delta Table Ingest (from BenchmarkLakehouse table, shortcut created in BenchmarkWarehouse)"
   ]
  },
  {
   "cell_type": "code",
   "id": "tc07",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "tc07_spinup_start = time.time()\n",
    "# Ensure Delta table exists in BenchmarkLakehouse\n",
    "df = spark.read.format(\"parquet\").load(input_path_parquet)\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{target_lakehouse}.{delta_table_name}\")\n",
    "tc07_spinup_end = time.time()\n",
    "tc07_spinup_duration = tc07_spinup_end - tc07_spinup_start\n",
    "\n",
    "tc07_ingest_start = time.time()\n",
    "# Shortcut creation logic\n",
    "try:\n",
    "    import msparkutils\n",
    "    shortcut_name = \"shortcut_to_delta\"\n",
    "    msparkutils.warehouse.createShortcut(\n",
    "        shortcutName=shortcut_name,\n",
    "        sourceLakehouse=target_lakehouse,\n",
    "        sourcePath=delta_table_path,\n",
    "        targetWarehouse=target_warehouse\n",
    "    )\n",
    "    shortcut_sync_start = time.time()\n",
    "    msparkutils.warehouse.refreshShortcuts(target_warehouse)\n",
    "    shortcut_sync_end = time.time()\n",
    "    tc07_notes = f\"Shortcut created; Metadata sync delay: {shortcut_sync_end-shortcut_sync_start:.2f}s\"\n",
    "except Exception as e:\n",
    "    tc07_notes = f\"Shortcut creation failed: {str(e)}\"\n",
    "tc07_ingest_end = time.time()\n",
    "tc07_ingest_duration = tc07_ingest_end - tc07_ingest_start\n",
    "\n",
    "tc07_total_ingest_duration = tc01_ingest_duration + tc07_ingest_duration\n",
    "tc07_total_spinup_duration = tc01_spinup_duration + tc07_spinup_duration\n",
    "\n",
    "metrics_tc07 = [\n",
    "    (\n",
    "        \"TC07\",\n",
    "        datetime.now(),\n",
    "        \"Shortcut to Delta\",\n",
    "        \"Tables\",\n",
    "        row_count,\n",
    "        \"Full Refresh\",\n",
    "        tc07_total_ingest_duration,\n",
    "        tc07_total_spinup_duration,\n",
    "        float('nan'),\n",
    "        \"N/A\",\n",
    "        float('nan'),\n",
    "        float('nan'),\n",
    "        tc07_notes\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_tc07, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc13_title",
   "metadata": {},
   "source": [
    "## TC13: Warehouse Table Ingest (physical copy from BenchmarkLakehouse Delta to BenchmarkWarehouse via T-SQL)\n",
    "\n",
    "This cell uses T-SQL magic to ingest data into the Warehouse."
   ]
  },
  {
   "cell_type": "code",
   "id": "tc13_tsql",
   "metadata": {
     "language": "sql"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "-- Attach to the warehouse via T-SQL magic\n",
    "-- This cell assumes you have attached the 'BenchmarkWarehouse' as a SQL endpoint in the notebook.\n",
    "-- You may need to set the context to BenchmarkWarehouse in your environment.\n",
    "\n",
    "-- Drop the table if it exists\n",
    "DROP TABLE IF EXISTS dbo.target_table_sql;\n",
    "\n",
    "-- Create the table by selecting from the lakehouse delta table\n",
    "CREATE TABLE dbo.target_table_sql AS\n",
    "SELECT * FROM OPENROWSET(\n",
    "    BULK 'Tables/target_table_delta',\n",
    "    DATA_SOURCE = 'BenchmarkLakehouse',\n",
    "    FORMAT = 'DELTA'\n",
    ") AS [result];\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "tc13_metric",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# After running the T-SQL cell, log metrics for TC13\n",
    "tc13_spinup_start = time.time()  # Optionally, set this before running T-SQL if timing is needed\n",
    "tc13_spinup_end = time.time()\n",
    "tc13_spinup_duration = tc13_spinup_end - tc13_spinup_start\n",
    "tc13_ingest_duration = tc13_spinup_end - tc13_spinup_start\n",
    "\n",
    "tc13_total_ingest_duration = tc01_ingest_duration + tc13_ingest_duration\n",
    "tc13_total_spinup_duration = tc01_spinup_duration + tc13_spinup_duration\n",
    "\n",
    "metrics_tc13 = [\n",
    "    (\n",
    "        \"TC13\",\n",
    "        datetime.now(),\n",
    "        \"Warehouse\",\n",
    "        \"Tables\",\n",
    "        row_count,\n",
    "        \"Full Refresh\",\n",
    "        tc13_total_ingest_duration,\n",
    "        tc13_total_spinup_duration,\n",
    "        float('nan'),\n",
    "        \"N/A\",\n",
    "        float('nan'),\n",
    "        float('nan'),\n",
    "        \"Physical copy to SQL table via T-SQL\"\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_tc13, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "done",
   "metadata": {},
   "source": [
    "## Completion\n",
    "\n",
    "All three scenarios (TC01, TC07, TC13) completed and metrics logged."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

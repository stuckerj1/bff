{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b62c91",
   "metadata": {},
   "source": [
    "# ðŸ““ 2.Ingest Data\n",
    "## Ingestion Module: Sequential Benchmark for TC01, TC07, TC13\n",
    "### Ensure `DataSourceLakehouse`, `BenchmarkLakehouse` & `BenchmarkWarehouse` are connected as data sources before running.\n",
    "\n",
    "This notebook runs all three initial ingest scenarios in sequence:\n",
    "- TC01: Lakehouse Parquet file ingest\n",
    "- TC07: Shortcut to Delta table in BenchmarkLakehouse (created in BenchmarkWarehouse)\n",
    "- TC13: Warehouse table ingest (physical copy)\n",
    "It accumulates metrics so that TC07 and TC13 include the cost/time of TC01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d3ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and schema\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
    "\n",
    "metrics_schema = StructType([\n",
    "    StructField(\"test_case_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"format\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"rows\", IntegerType(), True),\n",
    "    StructField(\"update_strategy\", StringType(), True),\n",
    "    StructField(\"ingest_time_s\", FloatType(), True),\n",
    "    StructField(\"spinup_time_s\", FloatType(), True),\n",
    "    StructField(\"storage_size_mb\", FloatType(), True),\n",
    "    StructField(\"query_type\", StringType(), True),\n",
    "    StructField(\"query_time_s\", FloatType(), True),\n",
    "    StructField(\"cu_used\", FloatType(), True),\n",
    "    StructField(\"notes\", StringType(), True),\n",
    "])\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS benchmarklakehouse.metrics (\n",
    "    test_case_id STRING,\n",
    "    timestamp TIMESTAMP,\n",
    "    format STRING,\n",
    "    location STRING,\n",
    "    rows INT,\n",
    "    update_strategy STRING,\n",
    "    ingest_time_s FLOAT,\n",
    "    spinup_time_s FLOAT,\n",
    "    storage_size_mb FLOAT,\n",
    "    query_type STRING,\n",
    "    query_time_s FLOAT,\n",
    "    cu_used FLOAT,\n",
    "    notes STRING\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e9e9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "row_count = 10000  # Change as needed\n",
    "input_path_parquet = f\"/lakehouse/default/Files/base/base_{row_count}_parquet.parquet\"\n",
    "delta_table_name = \"target_table_delta\"\n",
    "delta_table_path = f\"/Tables/benchmarklakehouse/{delta_table_name}\"\n",
    "warehouse_table_name = \"target_table_sql\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e3c9b7",
   "metadata": {},
   "source": [
    "## TC01: Lakehouse Parquet File Ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9e4d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc01_spinup_start = time.time()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "tc01_spinup_end = time.time()\n",
    "tc01_spinup_duration = tc01_spinup_end - tc01_spinup_start\n",
    "\n",
    "tc01_ingest_start = time.time()\n",
    "df = spark.read.format(\"parquet\").load(input_path_parquet)\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"benchmarklakehouse.target_table_parquet\")\n",
    "tc01_ingest_end = time.time()\n",
    "tc01_ingest_duration = tc01_ingest_end - tc01_ingest_start\n",
    "\n",
    "# Storage size for TC01\n",
    "try:\n",
    "    import msparkutils\n",
    "    tc01_table_path = \"/lakehouse/default/Tables/benchmarklakehouse/target_table_parquet\"\n",
    "    storage_files = msparkutils.fs.ls(tc01_table_path)\n",
    "    tc01_storage_size_mb = sum(f.size for f in storage_files) / (1024 * 1024)\n",
    "except Exception:\n",
    "    tc01_storage_size_mb = float('nan')\n",
    "\n",
    "metrics_tc01 = [\n",
    "    (\n",
    "        \"TC01\",\n",
    "        datetime.now(),\n",
    "        \"Parquet\",\n",
    "        \"Files\",\n",
    "        row_count,\n",
    "        \"Full Refresh\",\n",
    "        tc01_ingest_duration,\n",
    "        tc01_spinup_duration,\n",
    "        tc01_storage_size_mb,\n",
    "        \"N/A\",  # query_type\n",
    "        float('nan'),  # query_time_s\n",
    "        float('nan'),  # cu_used\n",
    "        \"No tabular access\"\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_tc01, schema=metrics_schema).write.mode('append').saveAsTable('benchmarklakehouse.metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e0e4e7",
   "metadata": {},
   "source": [
    "## TC07: Shortcut to Delta Table Ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8b1c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc07_spinup_start = time.time()\n",
    "# Ensure Delta table exists in BenchmarkLakehouse\n",
    "df = spark.read.format(\"parquet\").load(input_path_parquet)\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"benchmarklakehouse.{delta_table_name}\")\n",
    "tc07_spinup_end = time.time()\n",
    "tc07_spinup_duration = tc07_spinup_end - tc07_spinup_start\n",
    "\n",
    "tc07_ingest_start = time.time()\n",
    "# Shortcut creation logic\n",
    "try:\n",
    "    import msparkutils\n",
    "    shortcut_name = \"shortcut_to_delta\"\n",
    "    msparkutils.warehouse.createShortcut(\n",
    "        shortcutName=shortcut_name,\n",
    "        sourceLakehouse=\"benchmarklakehouse\",\n",
    "        sourcePath=delta_table_path,\n",
    "        targetWarehouse=\"benchmarkwarehouse\"\n",
    "    )\n",
    "    shortcut_sync_start = time.time()\n",
    "    msparkutils.warehouse.refreshShortcuts(\"benchmarkwarehouse\")\n",
    "    shortcut_sync_end = time.time()\n",
    "    tc07_notes = f\"Shortcut created; Metadata sync delay: {shortcut_sync_end-shortcut_sync_start:.2f}s\"\n",
    "except Exception as e:\n",
    "    tc07_notes = f\"Shortcut creation failed: {str(e)}\"\n",
    "tc07_ingest_end = time.time()\n",
    "tc07_ingest_duration = tc07_ingest_end - tc07_ingest_start\n",
    "\n",
    "# TC07 cumulative durations\n",
    "tc07_total_ingest_duration = tc01_ingest_duration + tc07_ingest_duration\n",
    "tc07_total_spinup_duration = tc01_spinup_duration + tc07_spinup_duration\n",
    "\n",
    "metrics_tc07 = [\n",
    "    (\n",
    "        \"TC07\",\n",
    "        datetime.now(),\n",
    "        \"Shortcut to Delta\",\n",
    "        \"Tables\",\n",
    "        row_count,\n",
    "        \"Full Refresh\",\n",
    "        tc07_total_ingest_duration,\n",
    "        tc07_total_spinup_duration,\n",
    "        float('nan'),  # no physical storage for shortcut\n",
    "        \"N/A\",\n",
    "        float('nan'),  # query_time_s\n",
    "        float('nan'),  # cu_used\n",
    "        tc07_notes\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_tc07, schema=metrics_schema).write.mode('append').saveAsTable('benchmarklakehouse.metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c14f0",
   "metadata": {},
   "source": [
    "## TC13: Warehouse Table Ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b1d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc13_spinup_start = time.time()\n",
    "# Data movement: Parquet to Warehouse (physical copy)\n",
    "df = spark.read.format(\"parquet\").load(input_path_parquet)\n",
    "df.write.mode(\"overwrite\").saveAsTable(f\"benchmarkwarehouse.{warehouse_table_name}\")\n",
    "tc13_spinup_end = time.time()\n",
    "tc13_spinup_duration = tc13_spinup_end - tc13_spinup_start\n",
    "\n",
    "tc13_ingest_duration = tc13_spinup_end - tc13_spinup_start  # Since copy happens during spinup in this workflow\n",
    "\n",
    "# TC13 cumulative durations\n",
    "tc13_total_ingest_duration = tc01_ingest_duration + tc13_ingest_duration\n",
    "tc13_total_spinup_duration = tc01_spinup_duration + tc13_spinup_duration\n",
    "\n",
    "metrics_tc13 = [\n",
    "    (\n",
    "        \"TC13\",\n",
    "        datetime.now(),\n",
    "        \"Warehouse\",\n",
    "        \"Tables\",\n",
    "        row_count,\n",
    "        \"Full Refresh\",\n",
    "        tc13_total_ingest_duration,\n",
    "        tc13_total_spinup_duration,\n",
    "        float('nan'),  # warehouse storage measured elsewhere\n",
    "        \"N/A\",\n",
    "        float('nan'),  # query_time_s\n",
    "        float('nan'),  # cu_used\n",
    "        \"Physical copy to SQL table\"\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_tc13, schema=metrics_schema).write.mode('append').saveAsTable('benchmarklakehouse.metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e6e8b4",
   "metadata": {},
   "source": [
    "## Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b3e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All three scenarios (TC01, TC07, TC13) completed and metrics logged.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "codemirror_mode": {\n    "name": "ipython", "version": 3\n  }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "synapse_pyspark",
   "display_name": "Synapse PySpark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [
     "parameters"
    ],
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "%%configure -f\n",
    "{\n",
    "  \"conf\": {\n",
    "    \"spark.notebook.parameters\": \"{\\\"name\\\": \\\"BFF-1M-LH-to-WH-Increment\\\", \\\"dataset_name\\\": \\\"1m\\\", \\\"source\\\": \\\"lakehouse\\\", \\\"format\\\": \\\"warehouse\\\", \\\"update_strategy\\\": \\\"Increment\\\", \\\"AZURE_SQL_SERVER\\\": \\\"benchmarking-bff\\\", \\\"AZURE_SQL_DB\\\": \\\"benchmarking\\\", \\\"AZURE_SQL_SCHEMA\\\": \\\"dbo\\\"}\"\n",
    "  },\n",
    "  \"defaultLakehouse\": {\n",
    "    \"name\": \"BenchmarkLakehouse\"\n",
    "  }\n",
    "}\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ 1. Ingest Data\n",
    "## Ingestion Module â€” single parameter-set run\n",
    "\n",
    "This notebook ingests one parameter set and supports lakehouse/sql sources and delta/warehouse targets.\n",
    "Behavior: strict parameter expectations (no silent defaults). Parameter-set keys expected in spark.notebook.parameters: `name`, `dataset_name`, `source`, `format`, `update_strategy`. If `source == 'sql'`, `AZURE_SQL_SERVER` and `AZURE_SQL_DB` are also required."
   ],
   "id": "title"
  },
  {
   "cell_type": "code",
   "id": "setup",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "print('Setup imports done')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "params",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# Params cell â€” ABFSS URIs using workspace/display-name containers (apply_updates style)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "conf_key = 'spark.notebook.parameters'\n",
    "conf_str = None\n",
    "try:\n",
    "    conf_str = spark.conf.get(conf_key, None)\n",
    "except Exception:\n",
    "    conf_str = None\n",
    "if not conf_str:\n",
    "    try:\n",
    "        conf_str = spark.sparkContext.getConf().get(conf_key, None)\n",
    "    except Exception:\n",
    "        conf_str = None\n",
    "if not conf_str:\n",
    "    raise SystemExit('Missing required spark.notebook.parameters. Provide parameter-set keys in the %%configure cell.')\n",
    "\n",
    "raw_params = json.loads(conf_str)\n",
    "\n",
    "# Required keys\n",
    "required = [\"name\", \"dataset_name\", \"source\", \"format\", \"update_strategy\"]\n",
    "missing = [k for k in required if k not in raw_params]\n",
    "if missing:\n",
    "    raise SystemExit(f\"Missing required parameters in spark.notebook.parameters: {', '.join(missing)}\")\n",
    "\n",
    "# Core params\n",
    "test_case_name = raw_params[\"name\"]\n",
    "dataset_name = raw_params[\"dataset_name\"]\n",
    "source = str(raw_params[\"source\"]).lower()\n",
    "fmt = str(raw_params[\"format\"]).lower()\n",
    "update_strategy = raw_params[\"update_strategy\"]\n",
    "\n",
    "# derive sanitized target table name from the parameter-set display name\n",
    "# produce safe SQL identifier: lowercase letters, numbers and underscores\n",
    "# convert spaces and hyphens to underscores, then remove any other invalid chars\n",
    "sanitized_name = re.sub(r\"[^a-z0-9_]\", \"\", re.sub(r\"[\\s-]+\", \"_\", test_case_name.strip().lower()))\n",
    "target_table = sanitized_name\n",
    "\n",
    "# Display-name anchors\n",
    "controller_workspace_name = \"BFF-Controller\"           # ABFSS container for DataSourceLakehouse & MetricsLakehouse\n",
    "controller_lakehouse_name = \"DataSourceLakehouse\"     # folder under workspace lakehouse area\n",
    "\n",
    "# Benchmark workspace/container uses the test_case display name\n",
    "benchmark_workspace_container = test_case_name\n",
    "\n",
    "# ABFSS account/host used in this workspace\n",
    "abfss_account = \"onelake.dfs.fabric.microsoft.com\"\n",
    "\n",
    "# Helper to convert display-name -> container-like string used elsewhere in repo (apply_updates uses container names without spaces)\n",
    "def _container_from_display(name):\n",
    "    return name.replace(\" \", \"\")\n",
    "\n",
    "# Build ABFSS path for data source (generator writes to e.g. 10kbase)\n",
    "src_container = _container_from_display(controller_workspace_name)\n",
    "data_source_lakehouse_path = (\n",
    "    f\"abfss://{src_container}@{abfss_account}/{controller_lakehouse_name}.Lakehouse/Files/{dataset_name}base\"\n",
    ")\n",
    "\n",
    "# Targets (display-name based) â€” keep same simple names as apply_updates\n",
    "target_lakehouse = raw_params.get(\"target_lakehouse\", \"BenchmarkLakehouse\")\n",
    "target_warehouse = raw_params.get(\"target_warehouse\", \"BenchmarkWarehouse\")\n",
    "\n",
    "# Build target ABFSS URIs for diagnostics / fallback (use benchmark workspace container)\n",
    "tgt_container = _container_from_display(benchmark_workspace_container)\n",
    "target_lakehouse_abfss_tables = (\n",
    "    f\"abfss://{tgt_container}@{abfss_account}/{target_lakehouse}.Lakehouse/Tables/{target_table}\"\n",
    ")\n",
    "target_lakehouse_abfss_files = (\n",
    "    f\"abfss://{tgt_container}@{abfss_account}/{target_lakehouse}.Lakehouse/Files/{dataset_name}/{target_table}\"\n",
    ")\n",
    "# SQL params (only required for source == 'sql')\n",
    "AZURE_SQL_SERVER = raw_params.get(\"AZURE_SQL_SERVER\")\n",
    "AZURE_SQL_DB = raw_params.get(\"AZURE_SQL_DB\")\n",
    "AZURE_SQL_SCHEMA = raw_params.get(\"AZURE_SQL_SCHEMA\", \"dbo\")\n",
    "if source == \"sql\" and (not AZURE_SQL_SERVER or not AZURE_SQL_DB):\n",
    "    raise SystemExit(\"SOURCE=sql requires AZURE_SQL_SERVER and AZURE_SQL_DB in spark.notebook.parameters\")\n",
    "\n",
    "# Expose globals for downstream cells\n",
    "globals().update({\n",
    "    \"raw_params\": raw_params,\n",
    "    \"test_case_name\": test_case_name,\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"source\": source,\n",
    "    \"fmt\": fmt,\n",
    "    \"update_strategy\": update_strategy,\n",
    "    \"sanitized_name\": sanitized_name,\n",
    "    \"target_table\": target_table,\n",
    "    \"data_source_lakehouse_path\": data_source_lakehouse_path,\n",
    "    \"target_lakehouse\": target_lakehouse,\n",
    "    \"target_warehouse\": target_warehouse,\n",
    "    \"target_lakehouse_abfss_tables\": target_lakehouse_abfss_tables,\n",
    "    \"target_lakehouse_abfss_files\": target_lakehouse_abfss_files,\n",
    "    \"AZURE_SQL_SERVER\": AZURE_SQL_SERVER,\n",
    "    \"AZURE_SQL_DB\": AZURE_SQL_DB,\n",
    "    \"AZURE_SQL_SCHEMA\": AZURE_SQL_SCHEMA,\n",
    "})\n",
    "\n",
    "print(f\"Loaded parameter set: name={test_case_name} sanitized_name={sanitized_name}\")\n",
    "print(f\" dataset_name={dataset_name} source={source} format={fmt} update_strategy={update_strategy}\")\n",
    "print('Using data_source_lakehouse_path =', data_source_lakehouse_path)\n",
    "print('Target lakehouse (display) =', target_lakehouse, '| target_warehouse =', target_warehouse)\n",
    "print('Target tables ABFSS (diagnostic) =', target_lakehouse_abfss_tables)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "metrics_schema",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "metrics_schema = StructType([\n",
    "    StructField(\"test_case_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"format\", StringType(), True),\n",
    "    StructField(\"rows\", IntegerType(), True),\n",
    "    StructField(\"update_strategy\", StringType(), True),\n",
    "    StructField(\"action\", StringType(), True),\n",
    "    StructField(\"ingest_time_s\", FloatType(), True),\n",
    "    StructField(\"spinup_time_s\", FloatType(), True),\n",
    "    StructField(\"query_type\", StringType(), True),\n",
    "    StructField(\"query_time_s\", FloatType(), True),\n",
    "    StructField(\"notes\", StringType(), True)\n",
    "])\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {target_lakehouse}.metrics (\n",
    "    test_case_id STRING,\n",
    "    timestamp TIMESTAMP,\n",
    "    source STRING,\n",
    "    format STRING,\n",
    "    rows INT,\n",
    "    update_strategy STRING,\n",
    "    action STRING,\n",
    "    ingest_time_s FLOAT,\n",
    "    spinup_time_s FLOAT,\n",
    "    query_type STRING,\n",
    "    query_time_s FLOAT,\n",
    "    notes STRING\n",
    ")\n",
    "\"\"\")\n",
    "print('Ensured metrics table exists in', target_lakehouse)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "helpers_sql",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# SQL helpers (only used when source == 'sql')\n",
    "try:\n",
    "    from notebookutils import mssparkutils\n",
    "except Exception:\n",
    "    mssparkutils = None\n",
    "\n",
    "SQL_COPT_SS_ACCESS_TOKEN = 1256\n",
    "\n",
    "def _token_struct():\n",
    "    if not mssparkutils:\n",
    "        raise RuntimeError('mssparkutils not available to get token')\n",
    "    t = mssparkutils.credentials.getToken('https://database.windows.net/')\n",
    "    exptoken = b''.join(bytes([c]) + b'\\x00' for c in t.encode('utf-8'))\n",
    "    return __import__('struct').pack('=i', len(exptoken)) + exptoken\n",
    "\n",
    "def _pyodbc_conn_with_retry(server=None, database=None, timeout=120, retries=2, backoff=2):\n",
    "    import pyodbc\n",
    "    server = server or AZURE_SQL_SERVER\n",
    "    database = database or AZURE_SQL_DB\n",
    "    if not server or not database:\n",
    "        raise RuntimeError('AZURE_SQL_SERVER and AZURE_SQL_DB must be set (or passed in)')\n",
    "    if not server.lower().endswith('.database.windows.net'):\n",
    "        server = server.rstrip('.') + '.database.windows.net'\n",
    "    conn_str = (\n",
    "        'Driver={ODBC Driver 18 for SQL Server};'\n",
    "        f'Server=tcp:{server},1433;'\n",
    "        f'Database={database};'\n",
    "        'Encrypt=yes;TrustServerCertificate=no;'\n",
    "    )\n",
    "    last_exc = None\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            return pyodbc.connect(conn_str, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: _token_struct()}, timeout=timeout)\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            if attempt < retries:\n",
    "                time.sleep(backoff * attempt)\n",
    "            else:\n",
    "                raise\n",
    "    raise last_exc\n",
    "\n",
    "print('SQL helper functions ready')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "read_source",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# --- Read source (updated) â€” single-pass timestamp canonicalization in Spark\n",
    "spinup_start = time.time()\n",
    "base_folder = data_source_lakehouse_path.rstrip('/')\n",
    "print('Reading parquet from ABFSS folder:', base_folder)\n",
    "\n",
    "# Optional listing for debugging\n",
    "try:\n",
    "    from notebookutils import mssparkutils\n",
    "    try:\n",
    "        entries = mssparkutils.fs.ls(base_folder)\n",
    "        print('Listed entries:', [getattr(e, 'path', e) for e in entries[:20]])\n",
    "    except Exception as _e:\n",
    "        print('mssparkutils.fs.ls failed:', _e)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "if source == 'lakehouse':\n",
    "    try:\n",
    "        df = spark.read.parquet(base_folder)\n",
    "        print('Read rows (spark):', df.count())\n",
    "    except Exception as e:\n",
    "        raise SystemExit(f\"Spark failed to read parquet from {base_folder}. Underlying error: {e}\")\n",
    "\n",
    "elif source == 'sql':\n",
    "    # Read from Azure SQL (token-based, using mssparkutils if available)\n",
    "    table_name_sql = f\"{AZURE_SQL_SCHEMA}.base_{dataset_name}\"\n",
    "    print(f'Reading Azure SQL table {table_name_sql} from {AZURE_SQL_SERVER}/{AZURE_SQL_DB} (token-based)')\n",
    "    conn = _pyodbc_conn_with_retry(server=AZURE_SQL_SERVER, database=AZURE_SQL_DB)\n",
    "    try:\n",
    "        pdf = pd.read_sql(f'SELECT * FROM {table_name_sql}', conn)\n",
    "        print('Pandas rows read from SQL:', len(pdf))\n",
    "        # create Spark DataFrame from pandas; canonicalize timestamps in Spark below\n",
    "        df = spark.createDataFrame(pdf)\n",
    "        print('Converted pandas -> spark rows:', df.count())\n",
    "    finally:\n",
    "        conn.close()\n",
    "else:\n",
    "    raise SystemExit(f'Unsupported source: {source}')\n",
    "\n",
    "# Single pass: canonicalize any timestamp columns named ts_* into legacy Spark TimestampType\n",
    "# (cast in Spark to avoid creating TimestampNTZType). Do NOT loop twice over all columns.\n",
    "ts_cols = [c for c in df.columns if c.startswith('ts_')]\n",
    "if ts_cols:\n",
    "    print(\"Canonicalizing timestamp columns to legacy Spark TimestampType:\", ts_cols)\n",
    "    for c in ts_cols:\n",
    "        df = df.withColumn(c, col(c).cast('timestamp'))\n",
    "\n",
    "# Repartition by ts_1 to reduce skew (single-line, tuned partition count)\n",
    "df = df.repartition(200, \"ts_1\")\n",
    "\n",
    "spinup_end = time.time()\n",
    "spinup_duration = spinup_end - spinup_start\n",
    "print('Spinup duration (s):', spinup_duration)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "write_target",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# synapsesql needs this import to work\n",
    "from com.microsoft.spark.fabric import Constants\n",
    "\n",
    "import time\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "ingest_start = time.time()\n",
    "\n",
    "if fmt == 'delta':\n",
    "    table_full = f\"{target_lakehouse}.{target_table}\"\n",
    "    print('Writing Delta table ->', table_full)\n",
    "    # Direct saveAsTable â€” will raise on errors\n",
    "    df.write.mode('overwrite').saveAsTable(table_full)\n",
    "    ingest_end = time.time()\n",
    "    ingest_duration = ingest_end - ingest_start\n",
    "\n",
    "elif fmt == 'warehouse':\n",
    "    table_full = f\"{target_warehouse}.dbo.{target_table}\"\n",
    "    print('Writing Warehouse table ->', table_full)\n",
    "\n",
    "    # Fabric writer currently fails on Spark TimestampNTZType -> map it to legacy timestamp\n",
    "    # Detect fields whose Spark type is timestamp_ntz and cast them to legacy timestamp before write.\n",
    "    # This produces a Spark TimestampType the connector knows how to map to SQL DATETIME2.\n",
    "    ntz_cols = [f.name for f in df.schema.fields if getattr(f.dataType, \"simpleString\", lambda: \"\")().lower() == \"timestamp_ntz\"]\n",
    "    if ntz_cols:\n",
    "        print(\"Casting TimestampNTZ columns to legacy timestamp for synapsesql:\", ntz_cols)\n",
    "        for c in ntz_cols:\n",
    "            df = df.withColumn(c, col(c).cast(\"timestamp\"))\n",
    "\n",
    "    # Direct synapsesql call â€” no try/except, will raise AttributeError or Java error if connector can't handle schema.\n",
    "    df.write.mode('overwrite').synapsesql(table_full)\n",
    "\n",
    "    ingest_end = time.time()\n",
    "    ingest_duration = ingest_end - ingest_start\n",
    "\n",
    "else:\n",
    "    raise SystemExit(f'Unsupported format: {fmt}')\n",
    "\n",
    "print('Ingest duration (s):', ingest_duration)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "log_metrics",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "metrics_row = [\n",
    "    (\n",
    "        test_case_name,\n",
    "        datetime.now(),\n",
    "        source,\n",
    "        fmt.upper(),\n",
    "        int(df.count()),\n",
    "        update_strategy,\n",
    "        'initial_load',\n",
    "        float(ingest_duration),\n",
    "        float(spinup_duration),\n",
    "        'N/A',\n",
    "        float('nan'),\n",
    "        f'Ingest from {source} into {fmt} target table {target_table}'\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_row, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')\n",
    "print('Metrics appended to', target_lakehouse + '.metrics')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "completion",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "print('Ingest step completed successfully for single parameter set.')\n",
    "print('Summary:')\n",
    "print(f\" test_case: {test_case_name} | dataset: {dataset_name} | source: {source} | format: {fmt} | target_table: {target_table}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}

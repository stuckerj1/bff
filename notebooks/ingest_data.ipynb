{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "synapse_pyspark",
   "display_name": "Synapse PySpark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [
     "parameters"
    ],
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "%%configure -f\n",
    "{\n",
    "  \"conf\": {\n",
    "    \"spark.notebook.parameters\": \"{\\\"SOURCE\\\": \\\"lakehouse\\\", \\\"FORMAT\\\": \\\"warehouse\\\", \\\"AZURE_SQL_SERVER\\\": \\\"benchmarking-bff\\\", \\\"AZURE_SQL_DB\\\": \\\"benchmarking\\\", \\\"AZURE_SQL_SCHEMA\\\": \\\"dbo\\\"}\"\n",
    "  },\n",
    "  \"defaultLakehouse\": {\n",
    "    \"name\": \"BenchmarkLakehouse\"\n",
    "  }\n",
    "}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# ðŸ““ 1. Ingest Data\n",
    "## Ingestion Module â€” single parameter-set run\n",
    "\n",
    "This notebook ingests one parameter set (decided at runtime) and supports four ingestion paths:\n",
    "- lakehouse -> delta\n",
    "- lakehouse -> warehouse\n",
    "- sql -> delta\n",
    "- sql -> warehouse\n",
    "\n",
    "Behavior and style:\n",
    "- Linear, happy-path code with debug prints.\n",
    "- Parameters are read from SparkConf (spark.notebook.parameters). Keep the notebook simple: it runs exactly one path per execution.\n",
    "- SQL connectivity reuses the token-based pyodbc pattern from generate_data.ipynb to read/write to Azure SQL.\n",
    "- Metrics for the single ingest are appended to BenchmarkLakehouse.metrics (same schema as before).\n",
    "\n",
    "Required params (via spark.notebook.parameters JSON string):\n",
    "{ \"dataset_name\": \"10k\", \"row_count\": 10000, \"source\": \"lakehouse|sql\", \"format\": \"delta|warehouse\", \"update_strategy\": \"Full Refresh|Full Compare|Incremental\", \"target_lakehouse\": \"BenchmarkLakehouse\", \"target_warehouse\": \"BenchmarkWarehouse\", \"AZURE_SQL_SERVER\": \"...\", \"AZURE_SQL_DB\": \"...\" }\n",
    "\n",
    "This notebook is case-insensitive for parameter keys (it accepts SOURCE or source, FORMAT or format, etc.).\n",
    "If spark.notebook.parameters is not provided, the notebook will fall back to the original hard-coded sample variables for local/debug runs."
   ]
  },
  {
   "cell_type": "code",
   "id": "setup",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "print('Setup imports done')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "params",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "conf_key = 'spark.notebook.parameters'\n",
    "conf_str = None\n",
    "try:\n",
    "    conf_str = spark.conf.get(conf_key, None)\n",
    "except Exception:\n",
    "    conf_str = None\n",
    "if not conf_str:\n",
    "    try:\n",
    "        conf_str = spark.sparkContext.getConf().get(conf_key, None)\n",
    "    except Exception:\n",
    "        conf_str = None\n",
    "\n",
    "if conf_str:\n",
    "    print('Loaded parameters from spark.notebook.parameters')\n",
    "    raw_params = json.loads(conf_str)\n",
    "else:\n",
    "    print('spark.notebook.parameters not found - falling back to embedded defaults (for debug)')\n",
    "    raw_params = {\n",
    "        'dataset_name': '10k',\n",
    "        'row_count': 10000,\n",
    "        'source': 'lakehouse',      # lakehouse | sql\n",
    "        'format': 'delta',         # delta | warehouse\n",
    "        'update_strategy': 'Full Refresh',\n",
    "        'target_lakehouse': 'BenchmarkLakehouse',\n",
    "        'target_warehouse': 'BenchmarkWarehouse',\n",
    "    }\n",
    "\n",
    "# Normalize keys to be case-insensitive (accept SOURCE or source etc.)\n",
    "params = {k.lower(): v for k, v in raw_params.items()}\n",
    "\n",
    "print('Params (normalized keys):')\n",
    "print(json.dumps(params, indent=2))\n",
    "\n",
    "# Convenience locals (use lowercase keys)\n",
    "dataset_name = params.get('dataset_name')\n",
    "row_count = int(params.get('row_count', 10000))\n",
    "source = str(params.get('source', params.get('SOURCE', 'lakehouse'))).lower()\n",
    "fmt = str(params.get('format', params.get('FORMAT', 'delta'))).lower()\n",
    "update_strategy = params.get('update_strategy', params.get('UPDATE_STRATEGY', 'Full Refresh'))\n",
    "target_lakehouse = params.get('target_lakehouse', params.get('TARGET_LAKEHOUSE', 'BenchmarkLakehouse'))\n",
    "target_warehouse = params.get('target_warehouse', params.get('TARGET_WAREHOUSE', 'BenchmarkWarehouse'))\n",
    "AZURE_SQL_SERVER = params.get('azure_sql_server', params.get('AZURE_SQL_SERVER'))\n",
    "AZURE_SQL_DB = params.get('azure_sql_db', params.get('AZURE_SQL_DB'))\n",
    "AZURE_SQL_SCHEMA = params.get('azure_sql_schema', params.get('AZURE_SQL_SCHEMA', 'dbo'))\n",
    "data_source_lakehouse_path = params.get('data_source_lakehouse_path', params.get('data_source_path', f\"/Files/{dataset_name}base/base.parquet\"))\n",
    "\n",
    "print(f\"Effective run: dataset={dataset_name} rows={row_count} source={source} format={fmt} strategy={update_strategy}\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "metrics_schema",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "source": [
    "metrics_schema = StructType([\n",
    "    StructField(\"test_case_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"format\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"rows\", IntegerType(), True),\n",
    "    StructField(\"update_strategy\", StringType(), True),\n",
    "    StructField(\"ingest_time_s\", FloatType(), True),\n",
    "    StructField(\"spinup_time_s\", FloatType(), True),\n",
    "    StructField(\"storage_size_mb\", FloatType(), True),\n",
    "    StructField(\"query_type\", StringType(), True),\n",
    "    StructField(\"query_time_s\", FloatType(), True),\n",
    "    StructField(\"cu_used\", FloatType(), True),\n",
    "    StructField(\"notes\", StringType(), True)\n",
    "])\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {target_lakehouse}.metrics (\n",
    "    test_case_id STRING,\n",
    "    timestamp TIMESTAMP,\n",
    "    format STRING,\n",
    "    location STRING,\n",
    "    rows INT,\n",
    "    update_strategy STRING,\n",
    "    ingest_time_s FLOAT,\n",
    "    spinup_time_s FLOAT,\n",
    "    storage_size_mb FLOAT,\n",
    "    query_type STRING,\n",
    "    query_time_s FLOAT,\n",
    "    cu_used FLOAT,\n",
    "    notes STRING\n",
    ")\n",
    "\"\"\")\n",
    "print('Ensured metrics table exists in', target_lakehouse)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ingest_logic_md",
   "metadata": {},
   "source": [
    "## Ingest logic\n",
    "Pick the proper source read path and the proper destination write path based on parameters. Only a single destination table is written per run (named by strategy + format mapping)."
   ]
  },
  {
   "cell_type": "code",
   "id": "helpers_sql",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "source": [
    "# --- SQL token helpers (copied from generate_data.ipynb - concise happy-path) ---\n",
    "try:\n",
    "    from notebookutils import mssparkutils\n",
    "except Exception:\n",
    "    # in environments without notebookutils available, set a stub; SQL paths will fail later with clear message\n",
    "    mssparkutils = None\n",
    "\n",
    "SQL_COPT_SS_ACCESS_TOKEN = 1256\n",
    "\n",
    "def _token_struct():\n",
    "    if not mssparkutils:\n",
    "        raise RuntimeError('mssparkutils not available to get token')\n",
    "    t = mssparkutils.credentials.getToken('https://database.windows.net/')\n",
    "    exptoken = b''.join(bytes([c]) + b'\\x00' for c in t.encode('utf-8'))\n",
    "    return __import__('struct').pack('=i', len(exptoken)) + exptoken\n",
    "\n",
    "def _pyodbc_conn_with_retry(server=None, database=None, timeout=120, retries=2, backoff=2):\n",
    "    import pyodbc\n",
    "    server = server or AZURE_SQL_SERVER\n",
    "    database = database or AZURE_SQL_DB\n",
    "    if not server or not database:\n",
    "        raise RuntimeError('AZURE_SQL_SERVER and AZURE_SQL_DB must be set (or passed in)')\n",
    "    # ensure fully-qualified server name\n",
    "    if not server.lower().endswith('.database.windows.net'):\n",
    "        server = server.rstrip('.') + '.database.windows.net'\n",
    "    conn_str = (\n",
    "        'Driver={ODBC Driver 18 for SQL Server};'\n",
    "        f'Server=tcp:{server},1433;'\n",
    "        f'Database={database};'\n",
    "        'Encrypt=yes;TrustServerCertificate=no;'\n",
    "    )\n",
    "    last_exc = None\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            return pyodbc.connect(conn_str, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: _token_struct()}, timeout=timeout)\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            if attempt < retries:\n",
    "                time.sleep(backoff * attempt)\n",
    "            else:\n",
    "                raise\n",
    "    raise last_exc\n",
    "\n",
    "def push_df_concise(df_pd, table_name, schema_name='dbo', server=None, database=None):\n",
    "    # minimal schema mapping; used for writing to Warehouse via pyodbc when synapsesql isn't available\n",
    "    import pyodbc\n",
    "    conn = _pyodbc_conn_with_retry(server=server, database=database)\n",
    "    cur = conn.cursor()\n",
    "    def _col_type_from_name(col):\n",
    "        if col == 'id':\n",
    "            return 'BIGINT'\n",
    "        if col.startswith('num_'):\n",
    "            return 'FLOAT'\n",
    "        if col.startswith('cat_'):\n",
    "            return 'NVARCHAR(100)'\n",
    "        if col.startswith('ts_'):\n",
    "            return 'DATETIMEOFFSET'\n",
    "        if col == 'update_type':\n",
    "            return 'NVARCHAR(32)'\n",
    "        return 'NVARCHAR(MAX)'\n",
    "    # sanitize df\n",
    "    df2 = df_pd.copy()\n",
    "    for c in df2.columns:\n",
    "        if df2[c].dtype == object:\n",
    "            df2[c] = df2[c].replace('', pd.NA)\n",
    "    cols_ddl = [f'[{c}] {_col_type_from_name(c)} NULL' for c in df2.columns]\n",
    "    full_table = f'{schema_name}.{table_name}'\n",
    "    create_sql = f\"IF OBJECT_ID(N'{full_table}', 'U') IS NOT NULL DROP TABLE {full_table}; CREATE TABLE {full_table} ({', '.join(cols_ddl)});\"\n",
    "    cur.execute(create_sql)\n",
    "    conn.commit()\n",
    "    col_names = ['[' + c.replace('\"','') + ']' for c in df2.columns]\n",
    "    placeholders = ', '.join('?' for _ in col_names)\n",
    "    insert_sql = f'INSERT INTO {full_table} ({', '.join(col_names)}) VALUES ({placeholders})'\n",
    "    records = [tuple(None if pd.isna(v) else (v.to_pydatetime() if isinstance(v, pd.Timestamp) else v) for v in row) for row in df2.itertuples(index=False, name=None)]\n",
    "    cur.fast_executemany = True\n",
    "    cur.executemany(insert_sql, records)\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return len(records)\n",
    "\n",
    "print('SQL helper functions ready (pyodbc token approach)')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "choose_table_name",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "source": [
    "# Map format + update_strategy -> target table name (consistent with prior naming)\n",
    "def choose_table(format_lower, strategy):\n",
    "    strat_normal = strategy.lower()\n",
    "    if format_lower == 'delta':\n",
    "        if 'refresh' in strat_normal:\n",
    "            return 'delta_refresh_load'\n",
    "        if 'compare' in strat_normal:\n",
    "            return 'delta_compare_load'\n",
    "        return 'delta_increment_load'\n",
    "    else:\n",
    "        # warehouse mapping\n",
    "        if 'refresh' in strat_normal:\n",
    "            return 'wh_table_refresh_load'\n",
    "        if 'compare' in strat_normal:\n",
    "            return 'wh_table_compare_load'\n",
    "        return 'wh_table_increment_load'\n",
    "\n",
    "target_table = choose_table(fmt, update_strategy)\n",
    "print('Target table chosen:', target_table)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "read_source",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "source": [
    "# Read source data depending on 'source' param\n",
    "spinup_start = time.time()\n",
    "if source == 'lakehouse':\n",
    "    # default parquet path pattern; controllers/orchestrator should set explicit path if needed\n",
    "    base_file = data_source_lakehouse_path\n",
    "    print('Reading parquet from lakehouse path:', base_file)\n",
    "    df = spark.read.parquet(base_file)\n",
    "    print('Read lakehouse parquet rows:', df.count())\n",
    "elif source == 'sql':\n",
    "    # Use pyodbc + token to pull base_{dataset_name} into pandas, then to Spark (happy-path)\n",
    "    if not (AZURE_SQL_SERVER and AZURE_SQL_DB):\n",
    "        raise SystemExit('AZURE_SQL_SERVER and AZURE_SQL_DB must be set in parameters for source=sql')\n",
    "    table_name_sql = f\"{AZURE_SQL_SCHEMA}.base_{dataset_name}\"\n",
    "    print(f'Reading Azure SQL table {table_name_sql} from {AZURE_SQL_SERVER}/{AZURE_SQL_DB} (token-based)')\n",
    "    conn = _pyodbc_conn_with_retry(server=AZURE_SQL_SERVER, database=AZURE_SQL_DB)\n",
    "    try:\n",
    "        # pandas read_sql works with pyodbc connection\n",
    "        pdf = pd.read_sql(f'SELECT * FROM {table_name_sql}', conn)\n",
    "        print('Pandas rows read from SQL:', len(pdf))\n",
    "        # Ensure timestamps are timezone-aware if present: convert naive to UTC\n",
    "        for c in pdf.columns:\n",
    "            if str(pdf[c].dtype).startswith('datetime'):\n",
    "                pdf[c] = pd.to_datetime(pdf[c], errors='coerce')\n",
    "                if getattr(pdf[c].dt, 'tz', None) is None:\n",
    "                    pdf[c] = pdf[c].dt.tz_localize('UTC')\n",
    "        # Convert to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)\n",
    "        print('Converted pandas -> spark rows:', df.count())\n",
    "    finally:\n",
    "        conn.close()\n",
    "else:\n",
    "    raise SystemExit(f'Unsupported source: {source}')\n",
    "spinup_end = time.time()\n",
    "spinup_duration = spinup_end - spinup_start\n",
    "print('Spinup duration (s):', spinup_duration)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "write_target",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "source": [
    "# Write the dataframe to the chosen target table according to format\n",
    "ingest_start = time.time()\n",
    "storage_size_mb = float('nan')\n",
    "if fmt == 'delta':\n",
    "    # Write to delta table in the target lakehouse\n",
    "    table_full = f\"{target_lakehouse}.{target_table}\"\n",
    "    print('Writing Delta table ->', table_full)\n",
    "    df.write.mode('overwrite').saveAsTable(table_full)\n",
    "    ingest_end = time.time()\n",
    "    ingest_duration = ingest_end - ingest_start\n",
    "    # Attempt to compute storage size using mssparkutils if available\n",
    "    try:\n",
    "        if mssparkutils:\n",
    "            table_path = f\"/lakehouse/{target_lakehouse}/Tables/{target_table}\"\n",
    "            storage_files = mssparkutils.fs.ls(table_path)\n",
    "            storage_size_mb = sum(f.size for f in storage_files) / (1024 * 1024)\n",
    "    except Exception:\n",
    "        storage_size_mb = float('nan')\n",
    "    print(f'Delta write complete. Ingest time: {ingest_duration:.2f}s | storage_size_mb: {storage_size_mb}')\n",
    "elif fmt == 'warehouse':\n",
    "    # Try synapsesql write first (Fabric runtime). If not available, fallback to pyodbc push via pandas.\n",
    "    table_full = f\"{target_warehouse}.dbo.{target_table}\"\n",
    "    print('Writing Warehouse table ->', table_full)\n",
    "    try:\n",
    "        # attempt the synapsesql API (available in Fabric runtimes)\n",
    "        df.write.mode('overwrite').synapsesql(table_full)\n",
    "        ingest_end = time.time()\n",
    "        ingest_duration = ingest_end - ingest_start\n",
    "        print('synapsesql write succeeded')\n",
    "    except Exception as _e:\n",
    "        # fallback: convert to pandas and push via pyodbc token helper\n",
    "        print('synapsesql not available or failed; falling back to pyodbc push via pandas:', _e)\n",
    "        pdf = df.toPandas()\n",
    "        pushed = push_df_concise(pdf, target_table, schema_name='dbo', server=AZURE_SQL_SERVER, database=AZURE_SQL_DB)\n",
    "        ingest_end = time.time()\n",
    "        ingest_duration = ingest_end - ingest_start\n",
    "        print(f'pyodbc fallback inserted rows: {pushed}')\n",
    "    # Warehouse storage size is not easy to get; leave as NaN\n",
    "    storage_size_mb = float('nan')\n",
    "else:\n",
    "    raise SystemExit(f'Unsupported format: {fmt}')\n",
    "\n",
    "print('Ingest duration (s):', ingest_duration)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "log_metrics",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "source": [
    "# Log a single metrics row for this ingest run\n",
    "metrics_row = [\n",
    "    (\n",
    "        f\"TC.{dataset_name}.{fmt}.{update_strategy}\",\n",
    "        datetime.now(),\n",
    "        fmt.upper(),\n",
    "        'Delta' if fmt == 'delta' else 'Warehouse',\n",
    "        int(row_count),\n",
    "        update_strategy,\n",
    "        float(ingest_duration),\n",
    "        float(spinup_duration),\n",
    "        float(storage_size_mb),\n",
    "        'N/A',\n",
    "        float('nan'),\n",
    "        float(row_count),\n",
    "        f'Ingest from {source} into {fmt} target table {target_table}'\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_row, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')\n",
    "print('Metrics appended to', target_lakehouse + '.metrics')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "completion",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "source": [
    "print('Ingest step completed successfully for single parameter set.')\n",
    "print('Summary:')\n",
    "print(f\" dataset: {dataset_name} | rows: {row_count} | source: {source} | format: {fmt} | target_table: {target_table}\")\n"
   ],
   "outputs": []
  }
 ]
}

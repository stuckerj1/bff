{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "synapse_pyspark",
   "display_name": "Synapse PySpark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [
     "parameters"
    ],
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "%%configure -f\n",
    "{\n",
    "  \"conf\": {\n",
    "    \"spark.notebook.parameters\": \"{\\\"name\\\": \\\"BFF 10k LH to Delta Full Refresh\\\", \\\"dataset_name\\\": \\\"10k\\\", \\\"source\\\": \\\"lakehouse\\\", \\\"format\\\": \\\"delta\\\", \\\"update_strategy\\\": \\\"Full Refresh\\\", \\\"AZURE_SQL_SERVER\\\": \\\"benchmarking-bff\\\", \\\"AZURE_SQL_DB\\\": \\\"benchmarking\\\", \\\"AZURE_SQL_SCHEMA\\\": \\\"dbo\\\"}\"\n",
    "  },\n",
    "  \"defaultLakehouse\": {\n",
    "    \"name\": \"BenchmarkLakehouse\"\n",
    "  }\n",
    "}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ 1. Ingest Data\n",
    "## Ingestion Module â€” single parameter-set run\n",
    "\n",
    "This notebook ingests one parameter set (decided at runtime) and supports four ingestion paths:\n",
    "- lakehouse -> delta\n",
    "- lakehouse -> warehouse\n",
    "- sql -> delta\n",
    "- sql -> warehouse\n",
    "\n",
    "Style: strict parameter expectations (no silent defaults). The notebook expects the parameter-set style keys in spark.notebook.parameters: name, dataset_name, source, format, update_strategy. If source == \"sql\", AZURE_SQL_SERVER and AZURE_SQL_DB are also required.\n",
    "\n",
    "The notebook uses sanitized_name derived from the parameter-set display name as the target table name and uses the original name as the test_case_id in the metrics table."
   ],
   "id": "title"
  },
  {
   "cell_type": "code",
   "id": "setup",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Controller workspace and lakehouse names (hard-coded for simplicity)\n",
    "controller_workspace_name = \"BFF Controller\"\n",
    "controller_lakehouse_name = \"DataSourceLakehouse\"\n",
    "\n",
    "print('Setup imports done')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "params",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "conf_key = 'spark.notebook.parameters'\n",
    "conf_str = None\n",
    "try:\n",
    "    conf_str = spark.conf.get(conf_key, None)\n",
    "except Exception:\n",
    "    conf_str = None\n",
    "if not conf_str:\n",
    "    try:\n",
    "        conf_str = spark.sparkContext.getConf().get(conf_key, None)\n",
    "    except Exception:\n",
    "        conf_str = None\n",
    "\n",
    "if not conf_str:\n",
    "    raise SystemExit(\n",
    "        \"Missing required spark.notebook.parameters. Provide the parameter-set keys in the %%configure cell: 'name','dataset_name','source','format','update_strategy'.\"\n",
    "    )\n",
    "\n",
    "raw_params = json.loads(conf_str)\n",
    "\n",
    "# Required keys (parameter-set style)\n",
    "required = [\"name\", \"dataset_name\", \"source\", \"format\", \"update_strategy\"]\n",
    "missing = [k for k in required if k not in raw_params]\n",
    "if missing:\n",
    "    raise SystemExit(f\"Missing required parameters in spark.notebook.parameters: {', '.join(missing)}\")\n",
    "\n",
    "# Assign directly from the parameter set (no silent defaults)\n",
    "test_case_name = raw_params[\"name\"]\n",
    "dataset_name = raw_params[\"dataset_name\"]\n",
    "source = str(raw_params[\"source\"]).lower()   # 'lakehouse' or 'sql'\n",
    "fmt = str(raw_params[\"format\"]).lower()      # 'delta' or 'warehouse'\n",
    "update_strategy = raw_params[\"update_strategy\"]\n",
    "\n",
    "# derive sanitized_name from the display name (lower, spaces->hyphen, remove non a-z0-9-)\n",
    "sanitized_name = re.sub(r\"[^a-z0-9\\-]\", \"\", re.sub(r\"\\s+\", \"-\", test_case_name.strip().lower()))\n",
    "\n",
    "# fixed targets\n",
    "target_lakehouse = \"BenchmarkLakehouse\"\n",
    "target_warehouse = \"BenchmarkWarehouse\"\n",
    "\n",
    "# SQL params (only required for source == 'sql')\n",
    "AZURE_SQL_SERVER = raw_params.get(\"AZURE_SQL_SERVER\")\n",
    "AZURE_SQL_DB = raw_params.get(\"AZURE_SQL_DB\")\n",
    "AZURE_SQL_SCHEMA = raw_params.get(\"AZURE_SQL_SCHEMA\", \"dbo\")\n",
    "if source == \"sql\" and (not AZURE_SQL_SERVER or not AZURE_SQL_DB):\n",
    "    raise SystemExit(\"SOURCE=sql requires AZURE_SQL_SERVER and AZURE_SQL_DB in spark.notebook.parameters\")\n",
    "\n",
    "# table name for target writes = sanitized_name\n",
    "target_table = sanitized_name\n",
    "\n",
    "# Resolve DataSourceLakehouse Files path by looking up the lakehouse id (not displayName)\n",
    "_override = raw_params.get(\"data_source_lakehouse_path\")\n",
    "if _override:\n",
    "    data_source_lakehouse_path = _override\n",
    "else:\n",
    "    try:\n",
    "        from notebookutils import mssparkutils\n",
    "    except Exception:\n",
    "        mssparkutils = None\n",
    "\n",
    "    if not mssparkutils:\n",
    "        raise SystemExit(\n",
    "            \"data_source_lakehouse_path not provided and mssparkutils is not available to resolve lakehouse id. \"\n",
    "            \"Either pass data_source_lakehouse_path in spark.notebook.parameters or run in a notebook runtime with mssparkutils.\"\n",
    "        )\n",
    "\n",
    "    API_BASE = \"https://api.fabric.microsoft.com/v1\"\n",
    "    try:\n",
    "        token = mssparkutils.credentials.getToken('https://api.fabric.microsoft.com/')\n",
    "        headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}\n",
    "\n",
    "        # find controller workspace id\n",
    "        wr = requests.get(f\"{API_BASE}/workspaces\", headers=headers, timeout=30)\n",
    "        wr.raise_for_status()\n",
    "        ws_list = wr.json().get('value', [])\n",
    "        ws_id = next((w.get('id') for w in ws_list if w.get('displayName') == controller_workspace_name), None)\n",
    "        if not ws_id:\n",
    "            raise SystemExit(f\"Workspace named '{controller_workspace_name}' not found when listing /workspaces\")\n",
    "\n",
    "        # list lakehouses in controller workspace and find lakehouse id by displayName\n",
    "        lr = requests.get(f\"{API_BASE}/workspaces/{ws_id}/lakehouses\", headers=headers, timeout=30)\n",
    "        lr.raise_for_status()\n",
    "        lh_list = lr.json().get('value', [])\n",
    "        lh_id = next((l.get('id') for l in lh_list if l.get('displayName') == controller_lakehouse_name), None)\n",
    "        if not lh_id:\n",
    "            raise SystemExit(f\"Lakehouse named '{controller_lakehouse_name}' not found in workspace '{controller_workspace_name}'\")\n",
    "\n",
    "        data_source_lakehouse_path = f\"/lakehouse/{lh_id}/Files/{dataset_name}/base/base.parquet\"\n",
    "    except Exception as e:\n",
    "        raise SystemExit(f\"Failed to resolve DataSourceLakehouse path via Fabric REST: {e}\")\n",
    "\n",
    "# Expose to globals for downstream cells\n",
    "globals().update({\n",
    "    \"raw_params\": raw_params,\n",
    "    \"test_case_name\": test_case_name,\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"source\": source,\n",
    "    \"fmt\": fmt,\n",
    "    \"update_strategy\": update_strategy,\n",
    "    \"sanitized_name\": sanitized_name,\n",
    "    \"target_lakehouse\": target_lakehouse,\n",
    "    \"target_warehouse\": target_warehouse,\n",
    "    \"target_table\": target_table,\n",
    "    \"data_source_lakehouse_path\": data_source_lakehouse_path,\n",
    "    \"AZURE_SQL_SERVER\": AZURE_SQL_SERVER,\n",
    "    \"AZURE_SQL_DB\": AZURE_SQL_DB,\n",
    "    \"AZURE_SQL_SCHEMA\": AZURE_SQL_SCHEMA,\n",
    "})\n",
    "\n",
    "print(f\"Loaded parameter set: name={test_case_name} sanitized_name={sanitized_name}\")\n",
    "print(f\" dataset_name={dataset_name} source={source} format={fmt} update_strategy={update_strategy}\")\n",
    "print('Resolved data_source_lakehouse_path =', data_source_lakehouse_path)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "metrics_schema",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "metrics_schema = StructType([\n",
    "    StructField(\"test_case_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"format\", StringType(), True),\n",
    "    StructField(\"rows\", IntegerType(), True),\n",
    "    StructField(\"update_strategy\", StringType(), True),\n",
    "    StructField(\"action\", StringType(), True),\n",
    "    StructField(\"ingest_time_s\", FloatType(), True),\n",
    "    StructField(\"spinup_time_s\", FloatType(), True),\n",
    "    StructField(\"query_type\", StringType(), True),\n",
    "    StructField(\"query_time_s\", FloatType(), True),\n",
    "    StructField(\"notes\", StringType(), True)\n",
    "])\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {target_lakehouse}.metrics (\n",
    "    test_case_id STRING,\n",
    "    timestamp TIMESTAMP,\n",
    "    source STRING,\n",
    "    format STRING,\n",
    "    rows INT,\n",
    "    update_strategy STRING,\n",
    "    action STRING,\n",
    "    ingest_time_s FLOAT,\n",
    "    spinup_time_s FLOAT,\n",
    "    query_type STRING,\n",
    "    query_time_s FLOAT,\n",
    "    notes STRING\n",
    ")\n",
    "\"\"\")\n",
    "print('Ensured metrics table exists in', target_lakehouse)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "helpers_sql",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# SQL helpers (only used when source == 'sql')\n",
    "try:\n",
    "    from notebookutils import mssparkutils\n",
    "except Exception:\n",
    "    mssparkutils = None\n",
    "\n",
    "SQL_COPT_SS_ACCESS_TOKEN = 1256\n",
    "\n",
    "def _token_struct():\n",
    "    if not mssparkutils:\n",
    "        raise RuntimeError('mssparkutils not available to get token')\n",
    "    t = mssparkutils.credentials.getToken('https://database.windows.net/')\n",
    "    exptoken = b''.join(bytes([c]) + b'\\x00' for c in t.encode('utf-8'))\n",
    "    return __import__('struct').pack('=i', len(exptoken)) + exptoken\n",
    "\n",
    "def _pyodbc_conn_with_retry(server=None, database=None, timeout=120, retries=2, backoff=2):\n",
    "    import pyodbc\n",
    "    server = server or AZURE_SQL_SERVER\n",
    "    database = database or AZURE_SQL_DB\n",
    "    if not server or not database:\n",
    "        raise RuntimeError('AZURE_SQL_SERVER and AZURE_SQL_DB must be set (or passed in)')\n",
    "    if not server.lower().endswith('.database.windows.net'):\n",
    "        server = server.rstrip('.') + '.database.windows.net'\n",
    "    conn_str = (\n",
    "        'Driver={ODBC Driver 18 for SQL Server};'\n",
    "        f'Server=tcp:{server},1433;'\n",
    "        f'Database={database};'\n",
    "        'Encrypt=yes;TrustServerCertificate=no;'\n",
    "    )\n",
    "    last_exc = None\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            return pyodbc.connect(conn_str, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: _token_struct()}, timeout=timeout)\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            if attempt < retries:\n",
    "                time.sleep(backoff * attempt)\n",
    "            else:\n",
    "                raise\n",
    "    raise last_exc\n",
    "\n",
    "print('SQL helper functions ready')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "read_source",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "spinup_start = time.time()\n",
    "if source == 'lakehouse':\n",
    "    base_file = data_source_lakehouse_path\n",
    "    print('Reading parquet from lakehouse path:', base_file)\n",
    "    df = spark.read.parquet(base_file)\n",
    "    print('Read lakehouse parquet rows:', df.count())\n",
    "elif source == 'sql':\n",
    "    table_name_sql = f\"{AZURE_SQL_SCHEMA}.base_{dataset_name}\"\n",
    "    print(f'Reading Azure SQL table {table_name_sql} from {AZURE_SQL_SERVER}/{AZURE_SQL_DB} (token-based)')\n",
    "    conn = _pyodbc_conn_with_retry(server=AZURE_SQL_SERVER, database=AZURE_SQL_DB)\n",
    "    try:\n",
    "        pdf = pd.read_sql(f'SELECT * FROM {table_name_sql}', conn)\n",
    "        print('Pandas rows read from SQL:', len(pdf))\n",
    "        for c in pdf.columns:\n",
    "            if str(pdf[c].dtype).startswith('datetime'):\n",
    "                pdf[c] = pd.to_datetime(pdf[c], errors='coerce')\n",
    "                if getattr(pdf[c].dt, 'tz', None) is None:\n",
    "                    pdf[c] = pdf[c].dt.tz_localize('UTC')\n",
    "        df = spark.createDataFrame(pdf)\n",
    "        print('Converted pandas -> spark rows:', df.count())\n",
    "    finally:\n",
    "        conn.close()\n",
    "else:\n",
    "    raise SystemExit(f'Unsupported source: {source}')\n",
    "spinup_end = time.time()\n",
    "spinup_duration = spinup_end - spinup_start\n",
    "print('Spinup duration (s):', spinup_duration)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "write_target",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "ingest_start = time.time()\n",
    "storage_size_mb = float('nan')\n",
    "if fmt == 'delta':\n",
    "    table_full = f\"{target_lakehouse}.{target_table}\"\n",
    "    print('Writing Delta table ->', table_full)\n",
    "    df.write.mode('overwrite').saveAsTable(table_full)\n",
    "    ingest_end = time.time()\n",
    "    ingest_duration = ingest_end - ingest_start\n",
    "    try:\n",
    "        if mssparkutils:\n",
    "            table_path = f\"/lakehouse/{target_lakehouse}/Tables/{target_table}\"\n",
    "            storage_files = mssparkutils.fs.ls(table_path)\n",
    "            storage_size_mb = sum(f.size for f in storage_files) / (1024 * 1024)\n",
    "    except Exception:\n",
    "        storage_size_mb = float('nan')\n",
    "    print(f'Delta write complete. Ingest time: {ingest_duration:.2f}s | storage_size_mb: {storage_size_mb}')\n",
    "elif fmt == 'warehouse':\n",
    "    table_full = f\"{target_warehouse}.dbo.{target_table}\"\n",
    "    print('Writing Warehouse table ->', table_full)\n",
    "    try:\n",
    "        df.write.mode('overwrite').synapsesql(table_full)\n",
    "        ingest_end = time.time()\n",
    "        ingest_duration = ingest_end - ingest_start\n",
    "        print('synapsesql write succeeded')\n",
    "    except Exception as _e:\n",
    "        print('synapsesql not available or failed; falling back to parquet write:', _e)\n",
    "        fallback_path = f\"/lakehouse/{target_lakehouse}/Files/{dataset_name}/{target_table}/\"\n",
    "        print('Falling back to parquet write at:', fallback_path)\n",
    "        try:\n",
    "            df.write.mode('overwrite').parquet(fallback_path)\n",
    "            print('Parquet fallback write complete at', fallback_path)\n",
    "        except Exception as e2:\n",
    "            print('Parquet fallback failed:', e2)\n",
    "        ingest_end = time.time()\n",
    "        ingest_duration = ingest_end - ingest_start\n",
    "    storage_size_mb = float('nan')\n",
    "else:\n",
    "    raise SystemExit(f'Unsupported format: {fmt}')\n",
    "\n",
    "print('Ingest duration (s):', ingest_duration)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "log_metrics",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "metrics_row = [\n",
    "    (\n",
    "        test_case_name,\n",
    "        datetime.now(),\n",
    "        source,\n",
    "        fmt.upper(),\n",
    "        int(df.count()),\n",
    "        update_strategy,\n",
    "        'initial_load',\n",
    "        float(ingest_duration),\n",
    "        float(spinup_duration),\n",
    "        'N/A',\n",
    "        float('nan'),\n",
    "        f'Ingest from {source} into {fmt} target table {target_table}'\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_row, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')\n",
    "print('Metrics appended to', target_lakehouse + '.metrics')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "completion",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "print('Ingest step completed successfully for single parameter set.')\n",
    "print('Summary:')\n",
    "print(f\" test_case: {test_case_name} | dataset: {dataset_name} | source: {source} | format: {fmt} | target_table: {target_table}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# ðŸ““ 2. Ingest Data\n",
    "## Ingestion Module: Sequential Benchmark for TC01, TC07, TC13\n",
    "\n",
    "### Ensure `DataSourceLakehouse` & `BenchmarkLakehouse` are connected as data sources before running.\n",
    "- For TC13, data is loaded into a Warehouse using T-SQL from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "id": "setup",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "id": "params",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lakehouse/warehouse names\n",
    "source_lakehouse = \"DataSourceLakehouse\"\n",
    "target_lakehouse = \"BenchmarkLakehouse\"\n",
    "target_warehouse = \"BenchmarkWarehouse\"\n",
    "\n",
    "row_count = 10000  # Change as needed\n",
    "\n",
    "# Explicit paths\n",
    "input_path_parquet = f\"abfss://FabricBenchmarking@onelake.dfs.fabric.microsoft.com/{source_lakehouse}.Lakehouse/Files/base/base_{row_count}_parquet.parquet\"\n",
    "delta_table_name = \"target_table_delta\"\n",
    "delta_table_path = f\"/Tables/{target_lakehouse}/{delta_table_name}\"\n",
    "warehouse_table_name = f\"data_{row_count}\""
   ]
  },
  {
   "cell_type": "code",
   "id": "metrics_schema",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics_schema = StructType([\n",
    "    StructField(\"test_case_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"format\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"rows\", IntegerType(), True),\n",
    "    StructField(\"update_strategy\", StringType(), True),\n",
    "    StructField(\"ingest_time_s\", FloatType(), True),\n",
    "    StructField(\"spinup_time_s\", FloatType(), True),\n",
    "    StructField(\"storage_size_mb\", FloatType(), True),\n",
    "    StructField(\"query_type\", StringType(), True),\n",
    "    StructField(\"query_time_s\", FloatType(), True),\n",
    "    StructField(\"cu_used\", FloatType(), True),\n",
    "    StructField(\"notes\", StringType(), True)\n",
    "])\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {target_lakehouse}.metrics (\n",
    "    test_case_id STRING,\n",
    "    timestamp TIMESTAMP,\n",
    "    format STRING,\n",
    "    location STRING,\n",
    "    rows INT,\n",
    "    update_strategy STRING,\n",
    "    ingest_time_s FLOAT,\n",
    "    spinup_time_s FLOAT,\n",
    "    storage_size_mb FLOAT,\n",
    "    query_type STRING,\n",
    "    query_time_s FLOAT,\n",
    "    cu_used FLOAT,\n",
    "    notes STRING\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc01_title",
   "metadata": {},
   "source": [
    "## TC01: Lakehouse Parquet File Ingest (from DataSourceLakehouse to BenchmarkLakehouse)"
   ]
  },
  {
   "cell_type": "code",
   "id": "tc01",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "tc01_spinup_start = time.time()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "tc01_spinup_end = time.time()\n",
    "tc01_spinup_duration = tc01_spinup_end - tc01_spinup_start\n",
    "\n",
    "tc01_ingest_start = time.time()\n",
    "try:\n",
    "    df = spark.read.format(\"parquet\").load(input_path_parquet)\n",
    "except Exception as e:\n",
    "    if (\"Path does not exist\" in str(e)) or (\"not found\" in str(e).lower()) or (\"Operation failed\" in str(e) and \"Bad Request\" in str(e)):\n",
    "        print(f\"Error: {input_path_parquet} not found. Did you run 1.GenerateData first?\")\n",
    "        raise\n",
    "    else:\n",
    "        raise\n",
    "else:\n",
    "    df.write.mode(\"overwrite\").saveAsTable(f\"{target_lakehouse}.target_table_parquet\")\n",
    "    tc01_ingest_end = time.time()\n",
    "    tc01_ingest_duration = tc01_ingest_end - tc01_ingest_start\n",
    "\n",
    "    # Storage size for TC01\n",
    "    try:\n",
    "        import msparkutils\n",
    "        tc01_table_path = f\"/lakehouse/{target_lakehouse}/Tables/target_table_parquet\"\n",
    "        storage_files = msparkutils.fs.ls(tc01_table_path)\n",
    "        tc01_storage_size_mb = sum(f.size for f in storage_files) / (1024 * 1024)\n",
    "    except Exception:\n",
    "        tc01_storage_size_mb = float('nan')\n",
    "\n",
    "    metrics_tc01 = [\n",
    "        (\n",
    "            \"TC01\",\n",
    "            datetime.now(),\n",
    "            \"Parquet\",\n",
    "            \"Files\",\n",
    "            row_count,\n",
    "            \"Full Refresh\",\n",
    "            tc01_ingest_duration,\n",
    "            tc01_spinup_duration,\n",
    "            tc01_storage_size_mb,\n",
    "            \"N/A\",\n",
    "            float('nan'),\n",
    "            float('nan'),\n",
    "            \"No tabular access\"\n",
    "        )\n",
    "    ]\n",
    "    spark.createDataFrame(metrics_tc01, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc07_title",
   "metadata": {},
   "source": [
    "## TC07: Shortcut to Delta Table Ingest (from BenchmarkLakehouse table, shortcut created in BenchmarkWarehouse)"
   ]
  },
  {
   "cell_type": "code",
   "id": "tc07",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "tc07_spinup_start = time.time()\n",
    "# Ensure Delta table exists in BenchmarkLakehouse\n",
    "df = spark.read.format(\"parquet\").load(input_path_parquet)\n",
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{target_lakehouse}.{delta_table_name}\")\n",
    "tc07_spinup_end = time.time()\n",
    "tc07_spinup_duration = tc07_spinup_end - tc07_spinup_start\n",
    "\n",
    "tc07_ingest_start = time.time()\n",
    "# Shortcut creation logic\n",
    "try:\n",
    "    import msparkutils\n",
    "    shortcut_name = \"shortcut_to_delta\"\n",
    "    msparkutils.warehouse.createShortcut(\n",
    "        shortcutName=shortcut_name,\n",
    "        sourceLakehouse=target_lakehouse,\n",
    "        sourcePath=delta_table_path,\n",
    "        targetWarehouse=target_warehouse\n",
    "    )\n",
    "    shortcut_sync_start = time.time()\n",
    "    msparkutils.warehouse.refreshShortcuts(target_warehouse)\n",
    "    shortcut_sync_end = time.time()\n",
    "    tc07_notes = f\"Shortcut created; Metadata sync delay: {shortcut_sync_end-shortcut_sync_start:.2f}s\"\n",
    "except Exception as e:\n",
    "    tc07_notes = f\"Shortcut creation failed: {str(e)}\"\n",
    "tc07_ingest_end = time.time()\n",
    "tc07_ingest_duration = tc07_ingest_end - tc07_ingest_start\n",
    "\n",
    "tc07_total_ingest_duration = tc01_ingest_duration + tc07_ingest_duration\n",
    "tc07_total_spinup_duration = tc01_spinup_duration + tc07_spinup_duration\n",
    "\n",
    "metrics_tc07 = [\n",
    "    (\n",
    "        \"TC07\",\n",
    "        datetime.now(),\n",
    "        \"Shortcut to Delta\",\n",
    "        \"Tables\",\n",
    "        row_count,\n",
    "        \"Full Refresh\",\n",
    "        tc07_total_ingest_duration,\n",
    "        tc07_total_spinup_duration,\n",
    "        float('nan'),\n",
    "        \"N/A\",\n",
    "        float('nan'),\n",
    "        float('nan'),\n",
    "        tc07_notes\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_tc07, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc13_title",
   "metadata": {},
   "source": [
    "## TC13: Warehouse Table Ingest (physical copy from BenchmarkLakehouse Delta to BenchmarkWarehouse via T-SQL)\n",
    "\n",
    "This cell uses T-SQL magic to ingest data into the Warehouse."
   ]
  },
  {
   "cell_type": "code",
   "id": "tc13_tsql",
   "metadata": {
    "language": "sql",
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "collapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using spark connector to warehouse\n",
    "from com.microsoft.spark.fabric import Constants\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Make sure timestamps have time zone\n",
    "for c in df.columns:\n",
    "    if dict(df.dtypes)[c] == \"timestamp_ntz\":\n",
    "        df = df.withColumn(c, col(c).cast(\"timestamp\"))\n",
    "\n",
    "# Write data from a DataFrame to the warehouse\n",
    "df.write.mode(\"overwrite\").synapsesql(f\"{target_warehouse}.dbo.{warehouse_table_name}\")\n",
    "\n",
    "## Verify the data written to the warehouse\n",
    "# df_verify = spark.read.synapsesql(f\"{target_warehouse}.dbo.{warehouse_table_name}\")\n",
    "#display(df_verify.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "id": "tc13_metric",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# After running the T-SQL cell, log metrics for TC13\n",
    "tc13_spinup_start = time.time()  # Optionally, set this before running T-SQL if timing is needed\n",
    "tc13_spinup_end = time.time()\n",
    "tc13_spinup_duration = tc13_spinup_end - tc13_spinup_start\n",
    "tc13_ingest_duration = tc13_spinup_end - tc13_spinup_start\n",
    "\n",
    "tc13_total_ingest_duration = tc01_ingest_duration + tc13_ingest_duration\n",
    "tc13_total_spinup_duration = tc01_spinup_duration + tc13_spinup_duration\n",
    "\n",
    "metrics_tc13 = [\n",
    "    (\n",
    "        \"TC13\",\n",
    "        datetime.now(),\n",
    "        \"Warehouse\",\n",
    "        \"Tables\",\n",
    "        row_count,\n",
    "        \"Full Refresh\",\n",
    "        tc13_total_ingest_duration,\n",
    "        tc13_total_spinup_duration,\n",
    "        float('nan'),\n",
    "        \"N/A\",\n",
    "        float('nan'),\n",
    "        float('nan'),\n",
    "        \"Physical copy to SQL table via T-SQL\"\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_tc13, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')\n",
    "\n",
    "print(\"Completion\")\n",
    "print(\"All three scenarios (TC01, TC07, TC13) completed and metrics logged.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "synapse_pyspark",
   "display_name": "Synapse PySpark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

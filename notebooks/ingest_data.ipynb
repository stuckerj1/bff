{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# ðŸ““ 2. Ingest Data\n",
    "## Ingestion Module: Initial Load Benchmarking for TC.01.x (Delta) & TC.02.x (Warehouse)\n",
    "\n",
    "### Ensure `DataSourceLakehouse` & `BenchmarkLakehouse` are connected as data sources before running.\n",
    "- `BenchmarkWarehouse` is connected automatically via code below.\n",
    "- This notebook performs initial loads for each update strategy target table in both Delta and Warehouse.\n",
    "- Note that `df = spark.read.parquet(base_file)` preserves timezone information which is necessary for successful SQL endpoints.\n",
    "- Metrics are captured for the first load in each location.\n",
    "- All initial load rows will have `update_type = 'insert'`."
   ]
  },
  {
   "cell_type": "code",
   "id": "setup",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import col, lit"
   ]
  },
  {
   "cell_type": "code",
   "id": "params",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lakehouse/warehouse names\n",
    "source_lakehouse = \"DataSourceLakehouse\"\n",
    "target_lakehouse = \"BenchmarkLakehouse\"\n",
    "target_warehouse = \"BenchmarkWarehouse\"\n",
    "\n",
    "row_count = 10000  # Change as needed\n",
    "\n",
    "# Explicit paths\n",
    "input_path_parquet = f\"abfss://FabricBenchmarking@onelake.dfs.fabric.microsoft.com/{source_lakehouse}.Lakehouse/Files/base/base_{row_count}_parquet.parquet\"\n",
    "base_file = input_path_parquet\n",
    "\n",
    "# Delta and Warehouse target table names for each strategy\n",
    "delta_tables = [\n",
    "    \"delta_refresh_load\",\n",
    "    \"delta_compare_load\",\n",
    "    \"delta_increment_load\"\n",
    "]\n",
    "warehouse_tables = [\n",
    "    \"wh_table_refresh_load\",\n",
    "    \"wh_table_compare_load\",\n",
    "    \"wh_table_increment_load\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "id": "metrics_schema",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics_schema = StructType([\n",
    "    StructField(\"test_case_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"format\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"rows\", IntegerType(), True),\n",
    "    StructField(\"update_strategy\", StringType(), True),\n",
    "    StructField(\"ingest_time_s\", FloatType(), True),\n",
    "    StructField(\"spinup_time_s\", FloatType(), True),\n",
    "    StructField(\"storage_size_mb\", FloatType(), True),\n",
    "    StructField(\"query_type\", StringType(), True),\n",
    "    StructField(\"query_time_s\", FloatType(), True),\n",
    "    StructField(\"cu_used\", FloatType(), True),\n",
    "    StructField(\"notes\", StringType(), True)\n",
    "])\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {target_lakehouse}.metrics (\n",
    "    test_case_id STRING,\n",
    "    timestamp TIMESTAMP,\n",
    "    format STRING,\n",
    "    location STRING,\n",
    "    rows INT,\n",
    "    update_strategy STRING,\n",
    "    ingest_time_s FLOAT,\n",
    "    spinup_time_s FLOAT,\n",
    "    storage_size_mb FLOAT,\n",
    "    query_type STRING,\n",
    "    query_time_s FLOAT,\n",
    "    cu_used FLOAT,\n",
    "    notes STRING\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc01x_title",
   "metadata": {},
   "source": [
    "## TC.01.x: Initial Load to Delta Tables\n",
    "\n",
    "Loads synthetic data into three Delta tables, preparing for future update strategy benchmarks.\n",
    "Metrics are captured for `delta_refresh_load` only.\n",
    "All initial rows will have `update_type = 'insert'`."
   ]
  },
  {
   "cell_type": "code",
   "id": "tc01x",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read source data (base load)\n",
    "df = spark.read.parquet(base_file)\n",
    "\n",
    "# Spinup timing (for symmetry with prior logic)\n",
    "tc01x_spinup_start = time.time()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "tc01x_spinup_end = time.time()\n",
    "tc01x_spinup_duration = tc01x_spinup_end - tc01x_spinup_start\n",
    "\n",
    "# Ingest timing and write to each Delta target table\n",
    "metrics_logged = False\n",
    "for i, table in enumerate(delta_tables):\n",
    "    ingest_start = time.time()\n",
    "    df.write.mode(\"overwrite\").saveAsTable(f\"{target_lakehouse}.{table}\")\n",
    "    ingest_end = time.time()\n",
    "    ingest_duration = ingest_end - ingest_start\n",
    "\n",
    "    if not metrics_logged:\n",
    "        try:\n",
    "            import mssparkutils\n",
    "            table_path = f\"/lakehouse/{target_lakehouse}/Tables/{table}\"\n",
    "            storage_files = mssparkutils.fs.ls(table_path)\n",
    "            storage_size_mb = sum(f.size for f in storage_files) / (1024 * 1024)\n",
    "        except Exception:\n",
    "            storage_size_mb = float('nan')\n",
    "\n",
    "        metrics_tc01x = [\n",
    "            (\n",
    "                \"TC.01.x\",\n",
    "                datetime.now(),\n",
    "                \"Delta\",\n",
    "                \"Tables\",\n",
    "                row_count,\n",
    "                \"Full Refresh\",\n",
    "                ingest_duration,\n",
    "                tc01x_spinup_duration,\n",
    "                storage_size_mb,\n",
    "                \"N/A\",\n",
    "                float('nan'),\n",
    "                float(row_count), # using row_count in lieu of cu_used and match FloatType schema\n",
    "                f\"Initial load to {table} (Delta)\"\n",
    "            )\n",
    "        ]\n",
    "        spark.createDataFrame(metrics_tc01x, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')\n",
    "        print(f\"Delta initial load complete for {table} | Ingest time: {ingest_duration:.2f}s | Storage: {storage_size_mb:.2f} MB\")\n",
    "        metrics_logged = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tc02x_title",
   "metadata": {},
   "source": [
    "## TC.02.x: Initial Load to Warehouse Tables\n",
    "\n",
    "Loads synthetic data into three Warehouse tables, preparing for future update strategy benchmarks.\n",
    "Metrics are captured for `wh_table_refresh_load` only.\n",
    "All initial rows will have `update_type = 'insert'`."
   ]
  },
  {
   "cell_type": "code",
   "id": "tc02x",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "from com.microsoft.spark.fabric import Constants\n",
    "\n",
    "# Ingest timing and write to each Warehouse target table\n",
    "metrics_logged = False\n",
    "for i, table in enumerate(warehouse_tables):\n",
    "    ingest_start = time.time()\n",
    "    df.write.mode(\"overwrite\").synapsesql(f\"{target_warehouse}.dbo.{table}\")\n",
    "    ingest_end = time.time()\n",
    "    ingest_duration = ingest_end - ingest_start\n",
    "\n",
    "    if not metrics_logged:\n",
    "        try:\n",
    "            import mssparkutils\n",
    "            storage_size_mb = float('nan')  # Warehouse storage size usually not directly available\n",
    "        except Exception:\n",
    "            storage_size_mb = float('nan')\n",
    "\n",
    "        metrics_tc02x = [\n",
    "            (\n",
    "                \"TC.02.x\",\n",
    "                datetime.now(),\n",
    "                \"Warehouse\",\n",
    "                \"Tables\",\n",
    "                row_count,\n",
    "                \"Full Refresh\",\n",
    "                ingest_duration,\n",
    "                float('nan'),  # spinup time N/A\n",
    "                storage_size_mb,\n",
    "                \"N/A\",\n",
    "                float('nan'),\n",
    "                float(row_count), # using row_count in lieu of cu_used and match FloatType schema\n",
    "                f\"Initial load to {table} (Warehouse)\"\n",
    "            )\n",
    "        ]\n",
    "        spark.createDataFrame(metrics_tc02x, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')\n",
    "        print(f\"Warehouse initial load complete for {table} | Ingest time: {ingest_duration:.2f}s\")\n",
    "        metrics_logged = True"
   ]
  },
  {
   "cell_type": "code",
   "id": "completion",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Completion\")\n",
    "print(\"Initial loads for Delta and Warehouse tables (TC.01.x & TC.02.x) completed and metrics logged.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "synapse_pyspark",
   "display_name": "Synapse PySpark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

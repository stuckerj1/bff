{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "synapse_pyspark",
   "display_name": "Synapse PySpark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [
     "parameters"
    ],
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "%%configure -f\n",
    "{\n",
    "  \"conf\": {\n",
    "    \"spark.notebook.parameters\": \"{\\\"DATASET_NAME\\\": \\\"10k\\\", \\\"SOURCE\\\": \\\"lakehouse\\\", \\\"FORMAT\\\": \\\"warehouse\\\", \\\"AZURE_SQL_SERVER\\\": \\\"benchmarking-bff\\\", \\\"AZURE_SQL_DB\\\": \\\"benchmarking\\\", \\\"AZURE_SQL_SCHEMA\\\": \\\"dbo\\\"}\"\n",
    "  },\n",
    "  \"defaultLakehouse\": {\n",
    "    \"name\": \"BenchmarkLakehouse\"\n",
    "  }\n",
    "}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# ðŸ““ 1. Ingest Data\n",
    "## Ingestion Module â€” single parameter-set run\n",
    "\n",
    "This notebook ingests one parameter set (decided at runtime) and supports four ingestion paths:\n",
    "- lakehouse -> delta\n",
    "- lakehouse -> warehouse\n",
    "- sql -> delta\n",
    "- sql -> warehouse\n",
    "\n",
    "Style: strict parameter expectations (no silent defaults). Required spark.notebook.parameters keys for this notebook: DATASET_NAME, SOURCE, FORMAT. If SOURCE == \"sql\", AZURE_SQL_SERVER and AZURE_SQL_DB are also required."
   ]
  },
  {
   "cell_type": "code",
   "id": "setup",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Controller workspace and lakehouse names are fixed here for simplicity\n",
    "controller_workspace_name = \"BFF Controller\"\n",
    "controller_lakehouse_name = \"DataSourceLakehouse\"\n",
    "\n",
    "print('Setup imports done')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "params",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "conf_key = 'spark.notebook.parameters'\n",
    "conf_str = None\n",
    "try:\n",
    "    conf_str = spark.conf.get(conf_key, None)\n",
    "except Exception:\n",
    "    conf_str = None\n",
    "if not conf_str:\n",
    "    try:\n",
    "        conf_str = spark.sparkContext.getConf().get(conf_key, None)\n",
    "    except Exception:\n",
    "        conf_str = None\n",
    "\n",
    "if not conf_str:\n",
    "    raise SystemExit('Missing required spark.notebook.parameters. Provide DATASET_NAME, SOURCE, and FORMAT in the %%configure cell.')\n",
    "\n",
    "raw_params = json.loads(conf_str)\n",
    "\n",
    "# Required minimal parameters (no defaults)\n",
    "required = ['DATASET_NAME', 'SOURCE', 'FORMAT']\n",
    "missing = [k for k in required if k not in raw_params]\n",
    "if missing:\n",
    "    raise SystemExit(f\"Missing required parameters in spark.notebook.parameters: {', '.join(missing)}\")\n",
    "\n",
    "dataset_name = raw_params['DATASET_NAME']\n",
    "source = str(raw_params['SOURCE']).lower()\n",
    "fmt = str(raw_params['FORMAT']).lower()\n",
    "\n",
    "# Hard-coded target lakehouse/warehouse\n",
    "target_lakehouse = 'BenchmarkLakehouse'\n",
    "target_warehouse = 'BenchmarkWarehouse'\n",
    "\n",
    "# SQL params (required only when source == 'sql')\n",
    "AZURE_SQL_SERVER = raw_params.get('AZURE_SQL_SERVER')\n",
    "AZURE_SQL_DB = raw_params.get('AZURE_SQL_DB')\n",
    "AZURE_SQL_SCHEMA = raw_params.get('AZURE_SQL_SCHEMA', 'dbo')\n",
    "if source == 'sql' and (not AZURE_SQL_SERVER or not AZURE_SQL_DB):\n",
    "    raise SystemExit('SOURCE=sql requires AZURE_SQL_SERVER and AZURE_SQL_DB in spark.notebook.parameters')\n",
    "\n",
    "# For initial ingest this notebook writes to the refresh target table name\n",
    "if fmt == 'delta':\n",
    "    target_table = f'delta_{dataset_name}'\n",
    "else:\n",
    "    target_table = f'wh_table_{dataset_name}'\n",
    "\n",
    "# Resolve DataSourceLakehouse path using the controller lakehouse name + dataset_name\n",
    "data_source_lakehouse_path = raw_params.get('data_source_lakehouse_path') or f\"/lakehouse/{controller_lakehouse_name}/Files/{dataset_name}/base/base.parquet\"\n",
    "\n",
    "print(f\"Params: DATASET_NAME={dataset_name} SOURCE={source} FORMAT={fmt}\")\n",
    "print('Resolved data_source_lakehouse_path =', data_source_lakehouse_path)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "metrics_schema",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "metrics_schema = StructType([\n",
    "    StructField(\"test_case_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"format\", StringType(), True),\n",
    "    StructField(\"rows\", IntegerType(), True),\n",
    "    StructField(\"update_strategy\", StringType(), True),\n",
    "    StructField(\"action\", StringType(),True),\n",
    "    StructField(\"ingest_time_s\", FloatType(), True),\n",
    "    StructField(\"spinup_time_s\", FloatType(), True),\n",
    "    StructField(\"query_type\", StringType(), True),\n",
    "    StructField(\"query_time_s\", FloatType(), True),\n",
    "    StructField(\"cu_used\", FloatType(), True),\n",
    "    StructField(\"notes\", StringType(), True)\n",
    "])\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {target_lakehouse}.metrics (\n",
    "    test_case_id STRING,\n",
    "    timestamp TIMESTAMP,\n",
    "    source STRING,\n",
    "    format STRING,\n",
    "    rows INT,\n",
    "    update_strategy STRING,\n",
    "    action STRING,\n",
    "    ingest_time_s FLOAT,\n",
    "    spinup_time_s FLOAT,\n",
    "    query_type STRING,\n",
    "    query_time_s FLOAT,\n",
    "    cu_used FLOAT,\n",
    "    notes STRING\n",
    ")\n",
    "\"\"\")\n",
    "print('Ensured metrics table exists in', target_lakehouse)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "helpers_sql",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "# SQL helpers (only used when source == 'sql')\n",
    "try:\n",
    "    from notebookutils import mssparkutils\n",
    "except Exception:\n",
    "    mssparkutils = None\n",
    "\n",
    "SQL_COPT_SS_ACCESS_TOKEN = 1256\n",
    "\n",
    "def _token_struct():\n",
    "    if not mssparkutils:\n",
    "        raise RuntimeError('mssparkutils not available to get token')\n",
    "    t = mssparkutils.credentials.getToken('https://database.windows.net/')\n",
    "    exptoken = b''.join(bytes([c]) + b'\\x00' for c in t.encode('utf-8'))\n",
    "    return __import__('struct').pack('=i', len(exptoken)) + exptoken\n",
    "\n",
    "def _pyodbc_conn_with_retry(server=None, database=None, timeout=120, retries=2, backoff=2):\n",
    "    import pyodbc\n",
    "    server = server or AZURE_SQL_SERVER\n",
    "    database = database or AZURE_SQL_DB\n",
    "    if not server or not database:\n",
    "        raise RuntimeError('AZURE_SQL_SERVER and AZURE_SQL_DB must be set (or passed in)')\n",
    "    if not server.lower().endswith('.database.windows.net'):\n",
    "        server = server.rstrip('.') + '.database.windows.net'\n",
    "    conn_str = (\n",
    "        'Driver={ODBC Driver 18 for SQL Server};'\n",
    "        f'Server=tcp:{server},1433;'\n",
    "        f'Database={database};'\n",
    "        'Encrypt=yes;TrustServerCertificate=no;'\n",
    "    )\n",
    "    last_exc = None\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            return pyodbc.connect(conn_str, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: _token_struct()}, timeout=timeout)\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            if attempt < retries:\n",
    "                time.sleep(backoff * attempt)\n",
    "            else:\n",
    "                raise\n",
    "    raise last_exc\n",
    "\n",
    "print('SQL helper functions ready')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "read_source",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "spinup_start = time.time()\n",
    "if source == 'lakehouse':\n",
    "    base_file = data_source_lakehouse_path\n",
    "    print('Reading parquet from lakehouse path:', base_file)\n",
    "    df = spark.read.parquet(base_file)\n",
    "    print('Read lakehouse parquet rows:', df.count())\n",
    "elif source == 'sql':\n",
    "    table_name_sql = f\"{AZURE_SQL_SCHEMA}.base_{dataset_name}\"\n",
    "    print(f'Reading Azure SQL table {table_name_sql} from {AZURE_SQL_SERVER}/{AZURE_SQL_DB} (token-based)')\n",
    "    conn = _pyodbc_conn_with_retry(server=AZURE_SQL_SERVER, database=AZURE_SQL_DB)\n",
    "    try:\n",
    "        pdf = pd.read_sql(f'SELECT * FROM {table_name_sql}', conn)\n",
    "        print('Pandas rows read from SQL:', len(pdf))\n",
    "        for c in pdf.columns:\n",
    "            if str(pdf[c].dtype).startswith('datetime'):\n",
    "                pdf[c] = pd.to_datetime(pdf[c], errors='coerce')\n",
    "                if getattr(pdf[c].dt, 'tz', None) is None:\n",
    "                    pdf[c] = pdf[c].dt.tz_localize('UTC')\n",
    "        df = spark.createDataFrame(pdf)\n",
    "        print('Converted pandas -> spark rows:', df.count())\n",
    "    finally:\n",
    "        conn.close()\n",
    "else:\n",
    "    raise SystemExit(f'Unsupported source: {source}')\n",
    "spinup_end = time.time()\n",
    "spinup_duration = spinup_end - spinup_start\n",
    "print('Spinup duration (s):', spinup_duration)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "write_target",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "ingest_start = time.time()\n",
    "storage_size_mb = float('nan')\n",
    "if fmt == 'delta':\n",
    "    table_full = f\"{target_lakehouse}.{target_table}\"\n",
    "    print('Writing Delta table ->', table_full)\n",
    "    df.write.mode('overwrite').saveAsTable(table_full)\n",
    "    ingest_end = time.time()\n",
    "    ingest_duration = ingest_end - ingest_start\n",
    "    try:\n",
    "        if mssparkutils:\n",
    "            table_path = f\"/lakehouse/{target_lakehouse}/Tables/{target_table}\"\n",
    "            storage_files = mssparkutils.fs.ls(table_path)\n",
    "            storage_size_mb = sum(f.size for f in storage_files) / (1024 * 1024)\n",
    "    except Exception:\n",
    "        storage_size_mb = float('nan')\n",
    "    print(f'Delta write complete. Ingest time: {ingest_duration:.2f}s | storage_size_mb: {storage_size_mb}')\n",
    "elif fmt == 'warehouse':\n",
    "    table_full = f\"{target_warehouse}.dbo.{target_table}\"\n",
    "    print('Writing Warehouse table ->', table_full)\n",
    "    try:\n",
    "        df.write.mode('overwrite').synapsesql(table_full)\n",
    "        ingest_end = time.time()\n",
    "        ingest_duration = ingest_end - ingest_start\n",
    "        print('synapsesql write succeeded')\n",
    "    except Exception as _e:\n",
    "        print('synapsesql not available or failed; falling back to parquet write:', _e)\n",
    "        fallback_path = f\"/lakehouse/{target_lakehouse}/Files/{dataset_name}/{target_table}/\"\n",
    "        print('Falling back to parquet write at:', fallback_path)\n",
    "        try:\n",
    "            df.write.mode('overwrite').parquet(fallback_path)\n",
    "            print('Parquet fallback write complete at', fallback_path)\n",
    "        except Exception as e2:\n",
    "            print('Parquet fallback failed:', e2)\n",
    "        ingest_end = time.time()\n",
    "        ingest_duration = ingest_end - ingest_start\n",
    "    storage_size_mb = float('nan')\n",
    "else:\n",
    "    raise SystemExit(f'Unsupported format: {fmt}')\n",
    "\n",
    "print('Ingest duration (s):', ingest_duration)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "log_metrics",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "metrics_row = [\n",
    "    (\n",
    "        f\"TC.{dataset_name}.{fmt}.initial_load\",\n",
    "        datetime.now(),\n",
    "        fmt.upper(),\n",
    "        'Delta' if fmt == 'delta' else 'Warehouse',\n",
    "        int(df.count()),\n",
    "        'Initial Load',\n",
    "        float(ingest_duration),\n",
    "        float(spinup_duration),\n",
    "        float(storage_size_mb),\n",
    "        'N/A',\n",
    "        float('nan'),\n",
    "        float(df.count()),\n",
    "        f'Ingest from {source} into {fmt} target table {target_table}'\n",
    "    )\n",
    "]\n",
    "spark.createDataFrame(metrics_row, schema=metrics_schema).write.mode('append').saveAsTable(f'{target_lakehouse}.metrics')\n",
    "print('Metrics appended to', target_lakehouse + '.metrics')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "completion",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "source": [
    "print('Ingest step completed successfully for single parameter set.')\n",
    "print('Summary:')\n",
    "print(f\" dataset: {dataset_name} | source: {source} | format: {fmt} | target_table: {target_table}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}

# Provision BFF Controller and Action Workspaces (test parameter sets)
# - Reads config/test_parameter_sets.yml
# - Builds/validates a matrix and uploads matrix.json artifact
# - Calls scripts/provision_workspace.py to create the BFF Controller workspace (writes .state/controller.json)
# - Calls scripts/provision_workspace.py for each action workspace (writes .state/workspace-<sanitized>.json)
# - Uploads per-workspace artifacts and assembles a summary (.state/bff-workspaces-summary.json)
#
# Notes:
# - Ensures requests is installed before running the script.
# - Uses one-line python -c invocations for small extractions to avoid heredoc/indentation pitfalls that can break YAML parsing.
name: 1. Provision BFF Controller and Action Workspaces (test parameter sets)

on:
  workflow_dispatch:

jobs:
  gen-matrix:
    name: Build matrix from config/test_parameter_sets.yml
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.build-matrix.outputs.matrix }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install PyYAML
        run: python -m pip install pyyaml

      - id: build-matrix
        name: Build matrix JSON and validate config
        shell: bash
        run: |
          python - <<'PY'
          import yaml, json, sys, re, os

          cfg_path = "config/test_parameter_sets.yml"
          if not os.path.exists(cfg_path):
              print("ERROR: config/test_parameter_sets.yml not found", file=sys.stderr)
              sys.exit(2)

          cfg = yaml.safe_load(open(cfg_path, 'r', encoding='utf-8'))
          datasets = {d['name']: d for d in cfg.get('datasets', [])}
          param_sets = cfg.get('parameter_sets', [])

          seen_names = set()
          includes = []
          def sanitize(s):
              s = s.strip().lower()
              s = re.sub(r'\s+', '-', s)
              s = re.sub(r'[^a-z0-9-]', '', s)
              return s[:40]

          for i, ps in enumerate(param_sets):
              name = ps.get('name')
              if not name:
                  raise SystemExit(f"parameter_sets[{i}] missing name")
              if name in seen_names:
                  raise SystemExit(f"duplicate parameter_sets.name: {name}")
              seen_names.add(name)

              ds_name = ps.get('dataset_name')
              if not ds_name:
                  raise SystemExit(f"parameter_sets[{i}] missing dataset_name")
              ds = datasets.get(ds_name)
              if not ds:
                  raise SystemExit(f"parameter_sets[{i}] references unknown dataset_name '{ds_name}'")

              # Basic validation
              fmt = ps.get('format')
              if fmt not in ('delta', 'warehouse'):
                  raise SystemExit(f"parameter_sets[{i}] invalid format '{fmt}' (allowed: delta, warehouse)")
              src = ps.get('source')
              if src not in ('lakehouse', 'sql'):
                  raise SystemExit(f"parameter_sets[{i}] invalid source '{src}' (allowed: lakehouse, sql)")

              upd = ps.get('update_strategy', '')
              if upd not in ('Full Refresh', 'Full Compare', 'Incremental'):
                  print(f"WARNING: parameter_sets[{i}] has update_strategy '{upd}' (recommended: Full Refresh, Full Compare, Incremental)", file=sys.stderr)

              merged = {}
              merged.update(ds)
              merged.update(ps)

              sanitized = sanitize(name)
              includes.append({
                  "workspace_name": merged["name"],
                  "sanitized_name": sanitized,
                  "dataset_name": merged["dataset_name"],
                  "row_count": str(int(merged["row_count"])),
                  "source": merged["source"],
                  "format": merged["format"],
                  "update_strategy": merged.get("update_strategy",""),
                  "change_fraction": merged.get("change_fraction"),
                  "new_fraction": merged.get("new_fraction"),
                  "delete_fraction": merged.get("delete_fraction"),
                  "seed": merged.get("seed"),
                  "notes": merged.get("notes",""),
                  "run_order": merged.get("run_order", [])
              })

          matrix = {"include": includes}
          matrix_json = json.dumps(matrix)

          # write matrix.json for artifact upload
          with open('.state/bff-matrix.json', 'w', encoding='utf-8') as f:
              f.write(matrix_json)

          # Write to GITHUB_OUTPUT for downstream strategy.matrix consumption
          gh_out = os.environ.get('GITHUB_OUTPUT')
          if gh_out:
              with open(gh_out, 'a', encoding='utf-8') as fh:
                  fh.write(f"matrix={matrix_json}\n")
          else:
              print(matrix_json)
          print("Matrix generation complete. entries=", len(includes))
          PY

      - name: Upload matrix artifact
        uses: actions/upload-artifact@v4
        with:
          name: bff-matrix
          path: .state/bff-matrix.json

  provision-controller:
    name: Provision BFF Controller workspace (single)
    runs-on: ubuntu-latest
    needs: gen-matrix
    outputs:
      controller_workspace_id: ${{ steps.save-controller.outputs.controller_id }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Ensure requests installed
        run: python -m pip install requests

      - name: Provision Controller workspace via script
        id: provision-controller
        env:
          CONTROLLER_NAME: "BFF Controller"
          # pass the existing FabricBenchmarkingProvisioner secrets into the script
          TENANT_ID: ${{ secrets.TENANT_ID }}
          CLIENT_ID: ${{ secrets.CLIENT_ID }}
          CLIENT_SECRET: ${{ secrets.CLIENT_SECRET }}
          CAPACITY_ID: ${{ secrets.CAPACITY_ID }}
          ADMIN_OBJECT_ID: ${{ secrets.ADMIN_OBJECT_ID }}
        run: |
          set -euo pipefail
          echo "Calling scripts/provision_workspace.py to provision Controller '${CONTROLLER_NAME}'"
          mkdir -p .state
          # call script; it must write controller JSON to the given --output (no trailing backslashes)
          python ./scripts/provision_workspace.py --workspace-name "${CONTROLLER_NAME}" --sanitized-name "bff-controller" --output .state/controller.json
          echo "Controller provisioning script completed. .state/controller.json:"
          cat .state/controller.json || true
          # extract workspace_id using a one-line python -c to avoid heredoc/indentation issues
          CONTROLLER_ID=$(
            python -c "import json,sys;print(json.load(open('.state/controller.json')).get('workspace_id',''))"
          )
          echo "CONTROLLER_ID=${CONTROLLER_ID}" >> $GITHUB_ENV

      - name: Upload controller artifact
        uses: actions/upload-artifact@v4
        with:
          name: controller
          path: .state/controller.json

      - name: Set controller id output
        id: save-controller
        run: |
          echo "controller_id=${{ env.CONTROLLER_ID }}" >> $GITHUB_OUTPUT

  provision-workspaces:
    name: Provision action workspaces per parameter set
    runs-on: ubuntu-latest
    needs: [gen-matrix, provision-controller]
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.gen-matrix.outputs.matrix) }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Ensure requests installed
        run: python -m pip install requests

      - name: Debug matrix entry
        run: |
          echo "---- Matrix entry ----"
          echo "workspace_name: ${{ matrix.workspace_name }}"
          echo "sanitized_name: ${{ matrix.sanitized_name }}"
          echo "dataset_name: ${{ matrix.dataset_name }}"
          echo "row_count: ${{ matrix.row_count }}"
          echo "source: ${{ matrix.source }}"
          echo "format: ${{ matrix.format }}"
          echo "update_strategy: ${{ matrix.update_strategy }}"
          echo "change_fraction: ${{ matrix.change_fraction }}"
          echo "new_fraction: ${{ matrix.new_fraction }}"
          echo "delete_fraction: ${{ matrix.delete_fraction }}"
          echo "seed: ${{ matrix.seed }}"
          echo "notes: ${{ matrix.notes }}"
          echo "run_order: ${{ toJson(matrix.run_order) }}"
          echo "----------------------"
        shell: bash

      - name: Provision action workspace via script
        id: provision-ws
        env:
          WORKSPACE_NAME: ${{ matrix.workspace_name }}
          SANITIZED_NAME: ${{ matrix.sanitized_name }}
          DATASET_NAME: ${{ matrix.dataset_name }}
          ROW_COUNT: ${{ matrix.row_count }}
          SOURCE: ${{ matrix.source }}
          FORMAT: ${{ matrix.format }}
          UPDATE_STRATEGY: ${{ matrix.update_strategy }}
          CHANGE_FRACTION: ${{ matrix.change_fraction }}
          NEW_FRACTION: ${{ matrix.new_fraction }}
          DELETE_FRACTION: ${{ matrix.delete_fraction }}
          SEED: ${{ matrix.seed }}
          NOTES: ${{ matrix.notes }}
          # pass the existing FabricBenchmarkingProvisioner secrets into the script
          TENANT_ID: ${{ secrets.TENANT_ID }}
          CLIENT_ID: ${{ secrets.CLIENT_ID }}
          CLIENT_SECRET: ${{ secrets.CLIENT_SECRET }}
          CAPACITY_ID: ${{ secrets.CAPACITY_ID }}
          ADMIN_OBJECT_ID: ${{ secrets.ADMIN_OBJECT_ID }}
        run: |
          set -euo pipefail
          mkdir -p .state
          echo "Calling scripts/provision_workspace.py to provision action workspace '${WORKSPACE_NAME}' (sanitized: ${SANITIZED_NAME})"
          python ./scripts/provision_workspace.py --workspace-name "${WORKSPACE_NAME}" --sanitized-name "${SANITIZED_NAME}" --dataset-name "${DATASET_NAME}" --row-count "${ROW_COUNT}" --source "${SOURCE}" --format "${FORMAT}" --update-strategy "${UPDATE_STRATEGY}" --output ".state/workspace-${SANITIZED_NAME}.json"
          echo "Provision script completed. .state/workspace-${SANITIZED_NAME}.json:"
          cat ".state/workspace-${SANITIZED_NAME}.json" || true
          # extract workspace_id into GITHUB_ENV using one-line python -c (avoid heredoc)
          WORKSPACE_ID=$(
            python -c "import json,sys;print(json.load(open('.state/workspace-${SANITIZED_NAME}.json')).get('workspace_id',''))"
          )
          echo "WORKSPACE_ID=${WORKSPACE_ID}" >> $GITHUB_ENV
          echo "workspace_id=${WORKSPACE_ID}" >> $GITHUB_OUTPUT
        shell: bash

      - name: Upload per-workspace artifact
        uses: actions/upload-artifact@v4
        with:
          name: workspace-${{ matrix.sanitized_name }}
          path: .state/workspace-${{ matrix.sanitized_name }}.json

  assemble-workspaces:
    name: Assemble workspace ids and create summary
    runs-on: ubuntu-latest
    needs: [provision-controller, provision-workspaces]
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: ./artifacts

      - name: Show downloaded artifact files (debug)
        run: |
          echo "Artifacts directory listing:"
          ls -la ./artifacts || true
          echo "Show controller artifact content (if present):"
          # show any controller.json found anywhere under the extracted artifacts
          find ./artifacts -type f -name "controller.json" -print -exec sh -c 'echo "---- {} ----"; cat "{}"' \; || true
          echo "Show per-workspace artifacts:"
          # show any workspace-*.json found anywhere under the extracted artifacts
          find ./artifacts -type f -name "workspace-*.json" -print -exec sh -c 'echo "---- {} ----"; cat "{}"' \; || true

      - name: Merge artifacts into .state/bff-workspaces-summary.json
        run: |
          python - <<'PY'
          import json, glob, os
          out = {"controller": None, "workspaces": []}

          # helper to iterate candidate patterns and yield files
          def iter_files(patterns):
              seen = set()
              for pat in patterns:
                  for p in glob.glob(pat, recursive=True):
                      if p in seen:
                          continue
                      seen.add(p)
                      yield p

          # Candidate controller locations (prefer .state/controller.json within artifact, but also accept controller.json at artifact root)
          controller_patterns = [
              "artifacts/**/.state/controller.json",
              "artifacts/**/controller.json"
          ]
          for p in iter_files(controller_patterns):
              try:
                  out["controller"] = json.load(open(p, "r", encoding="utf-8"))
                  break
              except Exception as e:
                  print("Failed to read controller candidate", p, e)

          # Candidate workspace locations (both .state/workspace-*.json and workspace-*.json at artifact root)
          workspace_patterns = [
              "artifacts/**/.state/workspace-*.json",
              "artifacts/**/workspace-*.json"
          ]
          seen_keys = set()
          for p in iter_files(workspace_patterns):
              try:
                  j = json.load(open(p, "r", encoding="utf-8"))
                  key = j.get("workspace_id") or j.get("sanitized_name") or json.dumps(j)
                  if key in seen_keys:
                      continue
                  seen_keys.add(key)
                  out["workspaces"].append(j)
              except Exception as e:
                  print("Failed to read workspace candidate", p, e)

          os.makedirs('.state', exist_ok=True)
          with open('.state/bff-workspaces-summary.json','w',encoding='utf-8') as fh:
              json.dump(out, fh, indent=2)
          print("Wrote .state/bff-workspaces-summary.json with", len(out["workspaces"]), "workspaces")
          PY
        shell: bash

      - name: Upload merged summary artifact
        uses: actions/upload-artifact@v4
        with:
          name: bff-workspaces-summary
          path: .state/bff-workspaces-summary.json

      - name: Expose merged summary (small) as job output
        id: set-summary
        run: |
          # guard against controller being present but set to None
          summary=$(
            python -c "import json,sys; s=json.load(open('.state/bff-workspaces-summary.json')); c=(s.get('controller') or {}); print(json.dumps({'controller_id': c.get('workspace_id',''), 'workspace_count': len(s.get('workspaces',[]))}))"
          )
          echo "bff-workspaces-summary=${summary}" >> $GITHUB_OUTPUT
        shell: bash

  orchestration:
    name: "Example: read summary in Python and iterate"
    runs-on: ubuntu-latest
    needs: assemble-workspaces
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download merged summary (artifact)
        uses: actions/download-artifact@v4
        with:
          name: bff-workspaces-summary
          path: ./artifacts

      - name: Show summary
        run: |
          echo "Summary contents:"
          # find and show the merged summary in whichever artifact dir it was placed
          find ./artifacts -type f -name "bff-workspaces-summary.json" -print -exec sh -c 'echo "---- {} ----"; cat "{}"' \; || true

      - name: Example read summary in Python and iterate
        run: |
          python - <<'PY'
          import json, glob
          files = glob.glob('./artifacts/**/bff-workspaces-summary.json', recursive=True)
          if files:
              s = json.load(open(files[0], 'r', encoding='utf-8'))
          else:
              s = {"controller": None, "workspaces": []}
          print("Controller:", s.get("controller"))
          for ws in s.get("workspaces", []):
              print("Workspace:", ws.get("workspace_name"), "id:", ws.get("workspace_id"))
          PY
